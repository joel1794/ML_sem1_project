{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joel1794/ML_sem1_project/blob/master/MLProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIHvJu0z-nAP",
        "colab_type": "code",
        "outputId": "c16a52d5-1537-4178-eaef-b076f17a7920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QbiHZ0Z_1iY",
        "colab_type": "text"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/ml_project_dataset/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4FEorjSjQOY",
        "colab_type": "code",
        "outputId": "f7992786-9bf0-4c70-d4aa-e9d0a7d88e68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd drive/My\\ Drive/ml_project_dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ml_project_dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKaPOeBXB8yQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfFxdyyAB-zW",
        "colab_type": "code",
        "outputId": "3b158ce9-6d8e-4d83-bf83-68ae4d5dd36c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import os\n",
        "fpaths = []\n",
        "labels = []\n",
        "spoken = []\n",
        "for f in os.listdir('audio'):\n",
        "    for w in os.listdir('audio/' + f):\n",
        "        fpaths.append('audio/' + f + '/' + w)\n",
        "        labels.append(f)\n",
        "        if f not in spoken:\n",
        "            spoken.append(f)\n",
        "print('Words spoken:', spoken)\n",
        "print(labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words spoken: ['lime', 'kiwi', 'orange', 'apple', 'banana', 'pineapple', 'peach']\n",
            "['lime', 'lime', 'lime', 'lime', 'lime', 'lime', 'lime', 'lime', 'lime', 'lime', 'lime', 'lime', 'lime', 'lime', 'lime', 'kiwi', 'kiwi', 'kiwi', 'kiwi', 'kiwi', 'kiwi', 'kiwi', 'kiwi', 'kiwi', 'kiwi', 'kiwi', 'kiwi', 'kiwi', 'kiwi', 'kiwi', 'orange', 'orange', 'orange', 'orange', 'orange', 'orange', 'orange', 'orange', 'orange', 'orange', 'orange', 'orange', 'orange', 'orange', 'orange', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'banana', 'pineapple', 'pineapple', 'pineapple', 'pineapple', 'pineapple', 'pineapple', 'pineapple', 'pineapple', 'pineapple', 'pineapple', 'pineapple', 'pineapple', 'pineapple', 'pineapple', 'pineapple', 'peach', 'peach', 'peach', 'peach', 'peach', 'peach', 'peach', 'peach', 'peach', 'peach', 'peach', 'peach', 'peach', 'peach', 'peach']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG-ns3VNCEV_",
        "colab_type": "code",
        "outputId": "1b313ff7-92ce-461f-8cfb-29ef63299694",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "#Files can be heard in Linux using the following commands from the command line\n",
        "#cat kiwi07.wav | aplay -f S16_LE -t wav -r 8000\n",
        "#Files are signed 16 bit raw, sample rate 8000\n",
        "from scipy.io import wavfile\n",
        "\n",
        "data = np.zeros((len(fpaths), 32000))\n",
        "# for image_info in data:\n",
        "#     print(image_info)\n",
        "maxsize = -1\n",
        "for n,file in enumerate(fpaths):\n",
        "    _, d = wavfile.read(file)\n",
        "    # print(data.shape)\n",
        "    # print(d.shape)\n",
        "    data[n, :d.shape[0]] = d\n",
        "    # print(data[n, :d.shape[0]].shape)\n",
        "    if d.shape[0] > maxsize:\n",
        "        maxsize = d.shape[0]\n",
        "    # break\n",
        "data = data[:, :maxsize]\n",
        "print(data.shape)\n",
        "\n",
        "# print(\"the data is \" + str(data))\n",
        "\n",
        "# for image_info in data:\n",
        "#     print(len(image_info))\n",
        "\n",
        "#Each sample file is one row in data, and has one entry in labels\n",
        "print('Number of files total:', data.shape[0])\n",
        "\n",
        "all_labels = np.zeros(data.shape[0])\n",
        "print(set(labels))\n",
        "print([(i, _) for i, _ in enumerate(set(labels))])\n",
        "for n, l in enumerate(set(labels)):\n",
        "    all_labels[np.array([i for i, _ in enumerate(labels) if _ == l])] = n\n",
        "    \n",
        "print('Labels and label indices', all_labels)\n",
        "print('data dimensions are', d.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(105, 6966)\n",
            "Number of files total: 105\n",
            "{'banana', 'kiwi', 'apple', 'lime', 'pineapple', 'orange', 'peach'}\n",
            "[(0, 'banana'), (1, 'kiwi'), (2, 'apple'), (3, 'lime'), (4, 'pineapple'), (5, 'orange'), (6, 'peach')]\n",
            "Labels and label indices [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 6. 6. 6. 6. 6. 6.\n",
            " 6. 6. 6. 6. 6. 6. 6. 6. 6.]\n",
            "data dimensions are (5944,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgUAHBr8CGY9",
        "colab_type": "code",
        "outputId": "9b7002f5-ea76-4fef-81e8-85341c5b2e50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "!pip install python_speech_features"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python_speech_features\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/d1/94c59e20a2631985fbd2124c45177abaa9e0a4eee8ba8a305aa26fc02a8e/python_speech_features-0.6.tar.gz\n",
            "Building wheels for collected packages: python-speech-features\n",
            "  Building wheel for python-speech-features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-speech-features: filename=python_speech_features-0.6-cp36-none-any.whl size=5889 sha256=446d36f16d55fccb610ad3fc26d695ec0551815192abf3fd915a27f9f4056edb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/42/7c/f60e9d1b40015cd69b213ad90f7c18a9264cd745b9888134be\n",
            "Successfully built python-speech-features\n",
            "Installing collected packages: python-speech-features\n",
            "Successfully installed python-speech-features-0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMXpbALuCSRa",
        "colab_type": "code",
        "outputId": "256cbbac-bbbf-4177-dec3-878c95547e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "from python_speech_features import mfcc\n",
        "# print(fpaths)\n",
        "\n",
        "for n,file in enumerate(fpaths):\n",
        "    (rate, sig) = wavfile.read(file)\n",
        "    mfcc_feat = mfcc(sig,rate)\n",
        "    print(mfcc_feat.shape)\n",
        "    print(\"mfcc shape\", mfcc_feat.shape)\n",
        "    if n == 2:\n",
        "      break\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(mfcc_feat, cmap='gray', interpolation=None)\n",
        "plt.xlabel('Freq (bin)')\n",
        "plt.ylabel('Time (overlapped frames)')\n",
        "plt.ylim(mfcc_feat.shape[1])\n",
        "plt.title('PSD of %s example'%labels[50])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47, 13)\n",
            "mfcc shape (47, 13)\n",
            "(45, 13)\n",
            "mfcc shape (45, 13)\n",
            "(47, 13)\n",
            "mfcc shape (47, 13)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'PSD of apple example')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAEWCAYAAACTwaluAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC4aZJfv/xXEzA0w8SNUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAbQkzNCWQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy\n",
        "\n",
        "def stft(x, fftsize=64, overlap_pct=.5):   \n",
        "    #Modified from http://stackoverflow.com/questions/2459295/stft-and-istft-in-python\n",
        "    hop = int(fftsize * (1 - overlap_pct))\n",
        "    w = scipy.hanning(fftsize + 1)[:-1]    \n",
        "    raw = np.array([np.fft.rfft(w * x[i:i + fftsize]) for i in range(0, len(x) - fftsize, hop)])\n",
        "    return raw[:, :(fftsize // 2)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSlwXuOZCb1F",
        "colab_type": "code",
        "outputId": "f5369969-488b-4af8-a8d1-4503f0c2f8cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "source": [
        "print('data dimension', data.shape)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(data[50, :], color='steelblue')\n",
        "plt.title('Timeseries example for %s'%labels[50])\n",
        "plt.xlim(0, 3500)\n",
        "plt.xlabel('Time (samples)')\n",
        "plt.ylabel('Amplitude (signed 16 bit)')\n",
        "plt.figure()\n",
        "\n",
        "# + 1 to avoid log of 0\n",
        "log_freq = 30 * np.log(np.abs(stft(data[50, :])) + 1)\n",
        "print(log_freq.shape)\n",
        "plt.imshow(log_freq, cmap='gray', interpolation=None)\n",
        "plt.xlabel('Freq (bin)')\n",
        "plt.ylabel('Time (overlapped frames)')\n",
        "plt.ylim(log_freq.shape[1])\n",
        "plt.title('PSD of %s example'%labels[50])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data dimension (105, 6966)\n",
            "(216, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'PSD of apple example')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAEWCAYAAAAtuzN2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngBRhAiBFmABIESYAUoQJgJQB/OuCb2C9riMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAEWCAYAAACE4zmnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYwQP0qMCc4_",
        "colab_type": "code",
        "outputId": "f25814af-b29b-409d-f4fb-c3edcde47263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('data dimensions', data.shape)\n",
        "\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "\n",
        "#Peak detection using the technique described here: http://kkjkok.blogspot.com/2013/12/dsp-snippets_9.html \n",
        "def peakfind(x, n_peaks, l_size=3, r_size=3, c_size=3, f=np.mean):\n",
        "    win_size = l_size + r_size + c_size\n",
        "    shape = x.shape[:-1] + (x.shape[-1] - win_size + 1, win_size)\n",
        "    strides = x.strides + (x.strides[-1],)\n",
        "    xs = as_strided(x, shape=shape, strides=strides)\n",
        "    def is_peak(x):\n",
        "        centered = (np.argmax(x) == l_size + int(c_size/2))\n",
        "        l = x[:l_size]\n",
        "        c = x[l_size:l_size + c_size]\n",
        "        r = x[-r_size:]\n",
        "        passes = np.max(c) > np.max([f(l), f(r)])\n",
        "        if centered and passes:\n",
        "            return np.max(c)\n",
        "        else:\n",
        "            return -1\n",
        "    r = np.apply_along_axis(is_peak, 1, xs)\n",
        "    top = np.argsort(r, None)[::-1]\n",
        "    heights = r[top[:n_peaks]]\n",
        "    #Add l_size and half - 1 of center size to get to actual peak location\n",
        "    top[top > -1] = top[top > -1] + l_size + int(c_size / 2.)\n",
        "    return heights, top[:n_peaks]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data dimensions (105, 6966)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4dvHqvkCliN",
        "colab_type": "code",
        "outputId": "39a3cea9-14fb-4916-ef6f-a03baeabf4b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "print('data dimensions', data.shape)\n",
        "plot_data = np.abs(stft(data[20, :]))[50, :]\n",
        "print(stft(data[20, :]).shape)\n",
        "print(\"plot_data dimensions\", plot_data.shape)\n",
        "values, locs = peakfind(plot_data, n_peaks=2)\n",
        "fp = locs[values > -1]\n",
        "fv = values[values > -1]\n",
        "plt.plot(plot_data, color='steelblue')\n",
        "plt.plot(fp, fv, 'x', color='darkred')\n",
        "plt.title('Peak location example')\n",
        "plt.xlabel('Frequency (bins)')\n",
        "plt.ylabel('Amplitude')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data dimensions (105, 6966)\n",
            "(216, 32)\n",
            "plot_data dimensions (32,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Amplitude')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsZtuoBrCnWh",
        "colab_type": "code",
        "outputId": "4e9dae56-45d9-4700-b54d-84550a32cf8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#This processing (top freq peaks) only works for single speaker case... need better features for multispeaker!\n",
        "#MFCC (or deep NN/automatic feature extraction) could be interesting\n",
        "all_obs = []\n",
        "for i in range(data.shape[0]):\n",
        "    d = np.abs(stft(data[i, :]))\n",
        "    mfcc_feat = mfcc(sig,rate)\n",
        "    print(mfcc_feat.shape)\n",
        "    n_dim = 6\n",
        "    obs = np.zeros((n_dim, d.shape[0]))\n",
        "    for r in range(d.shape[0]):\n",
        "#         print(d.shape[1])\n",
        "        _, t = peakfind(d[r, :], n_peaks=n_dim)\n",
        "        obs[:, r] = t.copy()\n",
        "    if i % 10 == 0:\n",
        "        print(\"Processed obs %s\" % i)\n",
        "#     print(\"dimension of obs\", obs.shape)\n",
        "#     print(\"OBS =\", obs)\n",
        "    all_obs.append(obs)\n",
        "    \n",
        "all_obs = np.atleast_3d(all_obs)\n",
        "print(all_obs.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47, 13)\n",
            "Processed obs 0\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "Processed obs 10\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "Processed obs 20\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "Processed obs 30\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "Processed obs 40\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "Processed obs 50\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "Processed obs 60\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "Processed obs 70\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "Processed obs 80\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "Processed obs 90\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "Processed obs 100\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(47, 13)\n",
            "(105, 6, 216)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBcH9farCqPl",
        "colab_type": "code",
        "outputId": "f7bdb852-b1a6-4d0b-ecc3-c52bdef107f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "import scipy.stats as st\n",
        "import numpy as np\n",
        "\n",
        "class gmmhmm:\n",
        "    #This class converted with modifications from https://code.google.com/p/hmm-speech-recognition/source/browse/Word.m\n",
        "    def __init__(self, n_states):\n",
        "        self.n_states = n_states\n",
        "        self.random_state = np.random.RandomState(0)\n",
        "        \n",
        "        #Normalize random initial state\n",
        "        self.prior = self._normalize(self.random_state.rand(self.n_states, 1))\n",
        "        self.A = self._stochasticize(self.random_state.rand(self.n_states, self.n_states))\n",
        "        \n",
        "        self.mu = None\n",
        "        self.covs = None\n",
        "        self.n_dims = None\n",
        "           \n",
        "    def _forward(self, B):\n",
        "        log_likelihood = 0.\n",
        "        T = B.shape[1]\n",
        "        alpha = np.zeros(B.shape)\n",
        "        for t in range(T):\n",
        "            if t == 0:\n",
        "                alpha[:, t] = B[:, t] * self.prior.ravel()\n",
        "            else:\n",
        "                alpha[:, t] = B[:, t] * np.dot(self.A.T, alpha[:, t - 1])\n",
        "         \n",
        "            alpha_sum = np.sum(alpha[:, t])\n",
        "            alpha[:, t] /= alpha_sum\n",
        "            log_likelihood = log_likelihood + np.log(alpha_sum)\n",
        "        return log_likelihood, alpha\n",
        "    \n",
        "    def _backward(self, B):\n",
        "        T = B.shape[1]\n",
        "        beta = np.zeros(B.shape);\n",
        "           \n",
        "        beta[:, -1] = np.ones(B.shape[0])\n",
        "            \n",
        "        for t in range(T - 1)[::-1]:\n",
        "            beta[:, t] = np.dot(self.A, (B[:, t + 1] * beta[:, t + 1]))\n",
        "            beta[:, t] /= np.sum(beta[:, t])\n",
        "        return beta\n",
        "    \n",
        "    def _state_likelihood(self, obs):\n",
        "        obs = np.atleast_2d(obs)\n",
        "        B = np.zeros((self.n_states, obs.shape[1]))\n",
        "        for s in range(self.n_states):\n",
        "            #Needs scipy 0.14\n",
        "            np.random.seed(self.random_state.randint(1))\n",
        "            B[s, :] = st.multivariate_normal.pdf(\n",
        "                obs.T, mean=self.mu[:, s].T, cov=self.covs[:, :, s].T)\n",
        "            #This function can (and will!) return values >> 1\n",
        "            #See the discussion here for the equivalent matlab function\n",
        "            #https://groups.google.com/forum/#!topic/comp.soft-sys.matlab/YksWK0T74Ak\n",
        "            #Key line: \"Probabilities have to be less than 1,\n",
        "            #Densities can be anything, even infinite (at individual points).\"\n",
        "            #This is evaluating the density at individual points...\n",
        "        return B\n",
        "    \n",
        "    def _normalize(self, x):\n",
        "        return (x + (x == 0)) / np.sum(x)\n",
        "    \n",
        "    def _stochasticize(self, x):\n",
        "        return (x + (x == 0)) / np.sum(x, axis=1)\n",
        "    \n",
        "    def _em_init(self, obs):\n",
        "        #Using this _em_init function allows for less required constructor args\n",
        "        if self.n_dims is None:\n",
        "            self.n_dims = obs.shape[0]\n",
        "        if self.mu is None:\n",
        "            subset = self.random_state.choice(np.arange(self.n_dims), size=self.n_states, replace=False)\n",
        "            self.mu = obs[:, subset]\n",
        "        if self.covs is None:\n",
        "            self.covs = np.zeros((self.n_dims, self.n_dims, self.n_states))\n",
        "            self.covs += np.diag(np.diag(np.cov(obs)))[:, :, None]\n",
        "        return self\n",
        "    \n",
        "    def _em_step(self, obs): \n",
        "        obs = np.atleast_2d(obs)\n",
        "        B = self._state_likelihood(obs)\n",
        "        T = obs.shape[1]\n",
        "        \n",
        "        log_likelihood, alpha = self._forward(B)\n",
        "        beta = self._backward(B)\n",
        "        \n",
        "        xi_sum = np.zeros((self.n_states, self.n_states))\n",
        "        gamma = np.zeros((self.n_states, T))\n",
        "        \n",
        "        for t in range(T - 1):\n",
        "            partial_sum = self.A * np.dot(alpha[:, t], (beta[:, t] * B[:, t + 1]).T)\n",
        "            xi_sum += self._normalize(partial_sum)\n",
        "            partial_g = alpha[:, t] * beta[:, t]\n",
        "            gamma[:, t] = self._normalize(partial_g)\n",
        "              \n",
        "        partial_g = alpha[:, -1] * beta[:, -1]\n",
        "        gamma[:, -1] = self._normalize(partial_g)\n",
        "        \n",
        "        expected_prior = gamma[:, 0]\n",
        "        expected_A = self._stochasticize(xi_sum)\n",
        "        \n",
        "        expected_mu = np.zeros((self.n_dims, self.n_states))\n",
        "        expected_covs = np.zeros((self.n_dims, self.n_dims, self.n_states))\n",
        "        \n",
        "        gamma_state_sum = np.sum(gamma, axis=1)\n",
        "        #Set zeros to 1 before dividing\n",
        "        gamma_state_sum = gamma_state_sum + (gamma_state_sum == 0)\n",
        "        \n",
        "        for s in range(self.n_states):\n",
        "            gamma_obs = obs * gamma[s, :]\n",
        "            expected_mu[:, s] = np.sum(gamma_obs, axis=1) / gamma_state_sum[s]\n",
        "            partial_covs = np.dot(gamma_obs, obs.T) / gamma_state_sum[s] - np.dot(expected_mu[:, s], expected_mu[:, s].T)\n",
        "            #Symmetrize\n",
        "            partial_covs = np.triu(partial_covs) + np.triu(partial_covs).T - np.diag(partial_covs)\n",
        "        \n",
        "        #Ensure positive semidefinite by adding diagonal loading\n",
        "        expected_covs += .01 * np.eye(self.n_dims)[:, :, None]\n",
        "        \n",
        "        self.prior = expected_prior\n",
        "        self.mu = expected_mu\n",
        "        self.covs = expected_covs\n",
        "        self.A = expected_A\n",
        "        return log_likelihood\n",
        "    \n",
        "    def fit(self, obs, n_iter=15):\n",
        "        #Support for 2D and 3D arrays\n",
        "        #2D should be n_features, n_dims\n",
        "        #3D should be n_examples, n_features, n_dims\n",
        "        #For example, with 6 features per speech segment, 105 different words\n",
        "        #this array should be size\n",
        "        #(105, 6, X) where X is the number of frames with features extracted\n",
        "        #For a single example file, the array should be size (6, X)\n",
        "        if len(obs.shape) == 2:\n",
        "            for i in range(n_iter):\n",
        "                self._em_init(obs)\n",
        "                log_likelihood = self._em_step(obs)\n",
        "        elif len(obs.shape) == 3:\n",
        "            count = obs.shape[0]\n",
        "            for n in range(count):\n",
        "                for i in range(n_iter):\n",
        "                    self._em_init(obs[n, :, :])\n",
        "                    log_likelihood = self._em_step(obs[n, :, :])\n",
        "        return self\n",
        "    \n",
        "    def transform(self, obs):\n",
        "        #Support for 2D and 3D arrays\n",
        "        #2D should be n_features, n_dims\n",
        "        #3D should be n_examples, n_features, n_dims\n",
        "        #For example, with 6 features per speech segment, 105 different words\n",
        "        #this array should be size\n",
        "        #(105, 6, X) where X is the number of frames with features extracted\n",
        "        #For a single example file, the array should be size (6, X)\n",
        "        if len(obs.shape) == 2:\n",
        "            B = self._state_likelihood(obs)\n",
        "            log_likelihood, _ = self._forward(B)\n",
        "            return log_likelihood\n",
        "        elif len(obs.shape) == 3:\n",
        "            count = obs.shape[0]\n",
        "            out = np.zeros((count,))\n",
        "            for n in range(count):\n",
        "                B = self._state_likelihood(obs[n, :, :])\n",
        "                log_likelihood, _ = self._forward(B)\n",
        "                out[n] = log_likelihood\n",
        "            return out\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rstate = np.random.RandomState(0)\n",
        "    t1 = np.ones((4, 40)) + .001 * rstate.rand(4, 40)\n",
        "    t1 /= t1.sum(axis=0)\n",
        "    t2 = rstate.rand(*t1.shape)\n",
        "    t2 /= t2.sum(axis=0)\n",
        "    \n",
        "    m1 = gmmhmm(2)\n",
        "    m1.fit(t1)\n",
        "    m2 = gmmhmm(2)\n",
        "    m2.fit(t2)\n",
        "    \n",
        "    m1t1 = m1.transform(t1)\n",
        "    m2t1 = m2.transform(t1)\n",
        "    print(\"Likelihoods for test set 1\")\n",
        "    print(\"M1:\", m1t1)\n",
        "    print(\"M2:\", m2t1)\n",
        "    print(\"Prediction for test set 1\")\n",
        "    print(\"Model\", np.argmax([m1t1, m2t1]) + 1)\n",
        "    print()\n",
        "    \n",
        "    m1t2 = m1.transform(t2)\n",
        "    m2t2 = m2.transform(t2)\n",
        "    print(\"Likelihoods for test set 2\")\n",
        "    print(\"M1:\", m1t2)\n",
        "    print(\"M2:\", m2t2)\n",
        "    print(\"Prediction for test set 2\")\n",
        "    print(\"Model\", np.argmax([m1t2, m2t2]) + 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Likelihoods for test set 1\n",
            "M1: 221.38828575112734\n",
            "M2: 165.27280230776495\n",
            "Prediction for test set 1\n",
            "Model 1\n",
            "\n",
            "Likelihoods for test set 2\n",
            "M1: 33.19459421485196\n",
            "M2: 59.15274753052998\n",
            "Prediction for test set 2\n",
            "Model 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y_At9s1Crh5",
        "colab_type": "code",
        "outputId": "39b79dc0-50c1-477b-fb68-fe8a3028c55a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.1, random_state=0)\n",
        "sss.get_n_splits(all_obs, all_labels)\n",
        "\n",
        "for n,i in enumerate(all_obs):\n",
        "    all_obs[n] /= all_obs[n].sum(axis=0)\n",
        "\n",
        "for train_index, test_index in sss.split(all_obs, all_labels):\n",
        "    X_train, X_test = all_obs[train_index, ...], all_obs[test_index, ...]\n",
        "    y_train, y_test = all_labels[train_index], all_labels[test_index]\n",
        "print('Size of training matrix:', X_train.shape)\n",
        "print('Size of testing matrix:', X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training matrix: (94, 6, 216)\n",
            "Size of testing matrix: (11, 6, 216)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRJku-biCtGi",
        "colab_type": "code",
        "outputId": "d4fc2476-81d8-4d13-9ae8-9b81e008eecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        }
      },
      "source": [
        "ys = set(all_labels)\n",
        "ms = [gmmhmm(6) for y in ys]\n",
        "_ = [m.fit(X_train[y_train == y, :, :]) for m, y in zip(ms, ys)]\n",
        "ps = [m.transform(X_test) for m in ms]\n",
        "print(\"ps is\", ps)\n",
        "res = np.vstack(ps)\n",
        "print(\"res is\", res)\n",
        "predicted_labels = np.argmax(res, axis=0)\n",
        "print(\"predicted labels is\", predicted_labels)\n",
        "missed = (predicted_labels != y_test)\n",
        "print(\"missed is\", missed)\n",
        "print('Test accuracy: %.2f percent' % (100 * (1 - np.mean(missed))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ps is [array([1305.69061289, 1239.21793619, 1485.67483275, 1372.82439678,\n",
            "       1311.79523148, 1351.82521936, 1376.85877001, 1352.96220606,\n",
            "       1309.71497396, 1275.84574947, 1430.43191908]), array([1268.83623794, 1071.01655565, 1588.76899475, 1234.60167682,\n",
            "       1149.15957091, 1301.6497115 , 1436.91786077, 1350.46127676,\n",
            "       1326.04669936, 1088.74392387, 1324.13252816]), array([1318.70833165, 1183.08436239, 1559.44546958, 1331.10219726,\n",
            "       1261.69562351, 1353.99308884, 1429.52795373, 1378.94202239,\n",
            "       1348.14797149, 1209.03572245, 1404.32712957]), array([1320.77859071, 1228.69327041, 1534.79072357, 1359.75959106,\n",
            "       1296.70361919, 1362.59291758, 1429.77986002, 1378.92469398,\n",
            "       1339.71827639, 1261.36189839, 1413.86876598]), array([1272.79805326, 1303.28420487, 1390.29832382, 1375.48131708,\n",
            "       1338.36301919, 1318.00403444, 1350.48037951, 1304.67543004,\n",
            "       1259.48463246, 1340.24493638, 1397.26614704]), array([1224.77352057, 1338.46147038, 1333.39823425, 1351.66856748,\n",
            "       1325.83218653, 1278.89662674, 1359.75609713, 1264.06230948,\n",
            "       1210.35949408, 1371.29982498, 1338.52466853]), array([1259.52347569, 1198.57986023, 1524.25835018, 1256.50566174,\n",
            "       1204.70894416, 1289.60326534, 1484.3383007 , 1328.90514672,\n",
            "       1306.98168217, 1208.35670834, 1295.05321724])]\n",
            "res is [[1305.69061289 1239.21793619 1485.67483275 1372.82439678 1311.79523148\n",
            "  1351.82521936 1376.85877001 1352.96220606 1309.71497396 1275.84574947\n",
            "  1430.43191908]\n",
            " [1268.83623794 1071.01655565 1588.76899475 1234.60167682 1149.15957091\n",
            "  1301.6497115  1436.91786077 1350.46127676 1326.04669936 1088.74392387\n",
            "  1324.13252816]\n",
            " [1318.70833165 1183.08436239 1559.44546958 1331.10219726 1261.69562351\n",
            "  1353.99308884 1429.52795373 1378.94202239 1348.14797149 1209.03572245\n",
            "  1404.32712957]\n",
            " [1320.77859071 1228.69327041 1534.79072357 1359.75959106 1296.70361919\n",
            "  1362.59291758 1429.77986002 1378.92469398 1339.71827639 1261.36189839\n",
            "  1413.86876598]\n",
            " [1272.79805326 1303.28420487 1390.29832382 1375.48131708 1338.36301919\n",
            "  1318.00403444 1350.48037951 1304.67543004 1259.48463246 1340.24493638\n",
            "  1397.26614704]\n",
            " [1224.77352057 1338.46147038 1333.39823425 1351.66856748 1325.83218653\n",
            "  1278.89662674 1359.75609713 1264.06230948 1210.35949408 1371.29982498\n",
            "  1338.52466853]\n",
            " [1259.52347569 1198.57986023 1524.25835018 1256.50566174 1204.70894416\n",
            "  1289.60326534 1484.3383007  1328.90514672 1306.98168217 1208.35670834\n",
            "  1295.05321724]]\n",
            "predicted labels is [3 5 1 4 4 3 6 2 2 5 0]\n",
            "missed is [ True False False False False False False  True False False False]\n",
            "Test accuracy: 81.82 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvaHUtD5CuMF",
        "colab_type": "code",
        "outputId": "b1852248-9cf3-49b5-ff0b-96997d537767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "plt.matshow(cm, cmap='gray')\n",
        "ax = plt.gca()\n",
        "_ = ax.set_xticklabels([\" \"] + [l[:2] for l in spoken])\n",
        "_ = ax.set_yticklabels([\" \"] + spoken)\n",
        "plt.title('Confusion matrix, single speaker')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Predicted label')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATMAAAEWCAYAAAAKOXDbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngpOZmZWCk5mZlYKTmZmVgpOZmZXC/weF30MJ2sApoAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zl0FKO6CvKe",
        "colab_type": "code",
        "outputId": "7609fe7e-16e6-4964-841b-24efed60a92a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print('Size of training matrix:', X_train.shape)\n",
        "print('Size of testing matrix:', X_test.shape)\n",
        "print('Size of training matrix:', y_train.shape)\n",
        "print('Size of testing matrix:', y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training matrix: (94, 6, 216)\n",
            "Size of testing matrix: (11, 6, 216)\n",
            "Size of training matrix: (94,)\n",
            "Size of testing matrix: (11,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgR-wa17Cwfl",
        "colab_type": "code",
        "outputId": "101bf52e-6a8b-4bf9-d5f5-f4ed65013a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "np.random.seed(1337)\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Embedding\n",
        "# from keras.layers.recurrent import LSTM\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import CuDNNLSTM "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCimau14FKFI",
        "colab_type": "code",
        "outputId": "7acc9fbc-4bcd-410c-92d3-e21ab9698826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print('Size of training matrix:', X_train.shape)\n",
        "print('Size of testing matrix:', X_test.shape)\n",
        "print('Size of updated training matrix:', X_train[:, :, [i for i in range(99)]].shape)\n",
        "print('Size of updated testing matrix:', X_test[:, :, [i for i in range(99)]].shape)\n",
        "# m.fit(X_train[y_train == y, :, :].mean(axis=2))\n",
        "# X_train_new = X_train[:, :, [i for i in range(20)]]\n",
        "# X_test_new = X_test[:, :, [i for i in range(20)]]\n",
        "# X_train_new = X_train.mean(axis=2)\n",
        "# X_test_new = X_test.mean(axis=2)\n",
        "# print('Size of training matrix:', X_train_new.shape)\n",
        "# print('Size of testing matrix:', X_test_new.shape)\n",
        "X_train_new = X_train.mean(axis=2)\n",
        "X_test_new = X_test.mean(axis=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training matrix: (94, 6, 216)\n",
            "Size of testing matrix: (11, 6, 216)\n",
            "Size of updated training matrix: (94, 6, 99)\n",
            "Size of updated testing matrix: (11, 6, 99)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J2CGEHEIQKq",
        "colab_type": "code",
        "outputId": "0f0b6cad-b946-4942-b3d2-b0a4079142a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihXpTUODjdUi",
        "colab_type": "code",
        "outputId": "c94f88b8-6f24-4d05-88f8-6e58be25b605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "# print(X_train[0, 0, 0])\n",
        "# print(X_train[[3,4], [3,4], :])\n",
        "# with open(\"train_sample.txt\", \"w\") as fp:\n",
        "#   fp.write(str(X_train[[3,4], :, :]))\n",
        "# np.save(\"train_numpy\", X_train[[3,4], :, :])\n",
        "X_train[[5], :, :][0][0]\n",
        "# print(y_train[3:6])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.22368421, 0.17582418, 0.05376344, 0.20618557, 0.19791667,\n",
              "       0.05063291, 0.07692308, 0.07692308, 0.07692308, 0.07692308,\n",
              "       0.05063291, 0.06451613, 0.06451613, 0.05063291, 0.04761905,\n",
              "       0.06451613, 0.06557377, 0.06666667, 0.05063291, 0.06896552,\n",
              "       0.06451613, 0.06779661, 0.05952381, 0.06557377, 0.34177215,\n",
              "       0.06557377, 0.06329114, 0.06896552, 0.0625    , 0.07142857,\n",
              "       0.06097561, 0.06896552, 0.06329114, 0.06896552, 0.06097561,\n",
              "       0.06896552, 0.06896552, 0.06896552, 0.06666667, 0.0617284 ,\n",
              "       0.06349206, 0.06329114, 0.07142857, 0.0617284 , 0.06666667,\n",
              "       0.07142857, 0.06666667, 0.06896552, 0.06896552, 0.06896552,\n",
              "       0.06451613, 0.06557377, 0.06666667, 0.06557377, 0.05970149,\n",
              "       0.06024096, 0.06779661, 0.06557377, 0.06779661, 0.07142857,\n",
              "       0.14457831, 0.06666667, 0.1686747 , 0.06060606, 0.1547619 ,\n",
              "       0.06451613, 0.06666667, 0.06097561, 0.17073171, 0.14457831,\n",
              "       0.12941176, 0.06557377, 0.06557377, 0.05952381, 0.1547619 ,\n",
              "       0.1547619 , 0.06666667, 0.06666667, 0.06666667, 0.14772727,\n",
              "       0.14772727, 0.1547619 , 0.13793103, 0.06557377, 0.06779661,\n",
              "       0.2804878 , 0.34177215, 0.14942529, 0.06666667, 0.1547619 ,\n",
              "       0.1547619 , 0.16666667, 0.32911392, 0.175     , 0.2804878 ,\n",
              "       0.06896552, 0.04395604, 0.24358974, 0.13953488, 0.13414634,\n",
              "       0.18478261, 0.13953488, 0.24358974, 0.04761905, 0.10843373,\n",
              "       0.06451613, 0.28915663, 0.28915663, 0.1686747 , 0.06097561,\n",
              "       0.07407407, 0.17582418, 0.24444444, 0.23529412, 0.24418605,\n",
              "       0.27586207, 0.28915663, 0.4       , 0.28915663, 0.4       ,\n",
              "       0.29069767, 0.25263158, 0.28915663, 0.29761905, 0.29761905,\n",
              "       0.2804878 , 0.2804878 , 0.27160494, 0.38983051, 0.16470588,\n",
              "       0.28915663, 0.29761905, 0.28915663, 0.32911392, 0.28915663,\n",
              "       0.29761905, 0.28915663, 0.34177215, 0.4       , 0.35211268,\n",
              "       0.25806452, 0.29761905, 0.2804878 , 0.2804878 , 0.29761905,\n",
              "       0.28915663, 0.25      , 0.29761905, 0.25      , 0.28915663,\n",
              "       0.29761905, 0.28915663, 0.2967033 , 0.28915663, 0.28915663,\n",
              "       0.29761905, 0.28915663, 0.2804878 , 0.32911392, 0.25806452,\n",
              "       0.2804878 , 0.2804878 , 0.25      , 0.4       , 0.2804878 ,\n",
              "       0.2804878 , 0.2804878 , 0.29761905, 0.29761905, 0.25      ,\n",
              "       0.28915663, 0.34722222, 0.25531915, 0.28915663, 0.2804878 ,\n",
              "       0.25      , 0.28915663, 0.29761905, 0.25806452, 0.25806452,\n",
              "       0.29761905, 0.27380952, 0.2804878 , 0.25531915, 0.32911392,\n",
              "       0.27659574, 0.35616438, 0.28915663, 0.24210526, 0.25531915,\n",
              "       0.29761905, 0.2804878 , 0.25      , 0.28915663, 0.27710843,\n",
              "       0.34177215, 0.34177215, 0.34177215, 0.34177215, 0.34177215,\n",
              "       0.34177215, 0.34177215, 0.34177215, 0.34177215, 0.34177215,\n",
              "       0.34177215, 0.34177215, 0.34177215, 0.34177215, 0.34177215,\n",
              "       0.34177215, 0.34177215, 0.34177215, 0.34177215, 0.34177215,\n",
              "       0.34177215])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E74ulJzoZ30",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1-anAxiCxbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_for_fruits(X_train_new, y_train, X_test_new, y_test):\n",
        "  batch_size = 5\n",
        "  hidden_units = 10\n",
        "  # nb_classes = 7\n",
        "  nb_classes = 1\n",
        "  print('Loading data...')\n",
        "  print(len(X_train_new), 'train sequences')\n",
        "  print(len(X_test_new), 'test sequences')\n",
        "  print('X_train shape:', X_train_new.shape)\n",
        "  print('y_train shape:', y_train.shape)\n",
        "  print('Build model...')\n",
        "\n",
        "  # y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "  # y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "  print(y_train)\n",
        "  print(y_test)\n",
        "  model = Sequential()\n",
        "\n",
        "  # model.add(Embedding(input_dim=(X_train.shape[1], X_train.shape[2]), output_dim=7, input_length=6))\n",
        "  model.add(LSTM(units=hidden_units, input_shape=X_train.shape[1:], activation='tanh', recurrent_activation ='sigmoid', unit_forget_bias=True, kernel_initializer=\"uniform\", recurrent_initializer=\"uniform\"))\n",
        "  # model.add(CuDNNLSTM(units=hidden_units, activation='tanh', recurrent_activation ='sigmoid'))\n",
        "\n",
        "  model.add(Dense(nb_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  # model.compile(loss='categorical_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])\n",
        "\n",
        "  print(\"Train...\")\n",
        "  # with tf.device('/gpu:0'):\n",
        "  model.fit(X_train_new, y_train, batch_size=batch_size, epochs=3, validation_data=(X_test_new, y_test))\n",
        "\n",
        "  print('X_test shape:', X_test_new.shape)\n",
        "  print('y_test shape:', y_test.shape)\n",
        "  print(\"evaluation metrics\\n\", model.evaluate(X_test_new, y_test, batch_size=batch_size))\n",
        "  score, acc = model.evaluate(X_test_new, y_test, batch_size=batch_size)\n",
        "  print('Test score:', score)\n",
        "  print('Test accuracy:', acc)\n",
        "  # print(\"prediction for test set is\", model.predict(X_test_new))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4crkq-u_JO7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def test_for_fruits(model, X_test_new, y_test):\n",
        "    batch_size = 5\n",
        "    print('X_test shape:', X_test_new.shape)\n",
        "    print('y_test shape:', y_test.shape)\n",
        "    print(\"evaluation metrics\\n\", model.evaluate(X_test_new, y_test, batch_size=batch_size))\n",
        "    score, acc = model.evaluate(X_test_new, y_test, batch_size=batch_size)\n",
        "    print('Test score:', score)\n",
        "    print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MxDAJZKDPiD",
        "colab_type": "code",
        "outputId": "61ce4406-ce9d-41b2-d15c-b3dd95dd59d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        }
      },
      "source": [
        "import random\n",
        "counter = 0\n",
        "fruit_models = []\n",
        "X_train_fruit = []\n",
        "y_train_fruit = []\n",
        "\n",
        "lime_indexes_train =  [ i for i in range(len(y_train)) if y_train[i] == 4 ]\n",
        "lime_indexes_train_val = [lime_indexes_train[random.randrange(0, len(lime_indexes_train))] for i in range(int(0.2 * len(lime_indexes_train)))]\n",
        "for index in lime_indexes_train:\n",
        "  if index in lime_indexes_train_val:\n",
        "    lime_indexes_train.remove(index)\n",
        "lime_indexes_test =  [ i for i in range(len(y_test)) if y_test[i] == 4 ]\n",
        "\n",
        "# print(lime_indexes_train)\n",
        "# print(lime_indexes_train_val)\n",
        "X_train_fruit = X_train[lime_indexes_train, :, :]\n",
        "y_train_fruit = [0.99] * len(lime_indexes_train)\n",
        "X_train_fruit = np.array(X_train_fruit)\n",
        "y_train_fruit = np.array(y_train_fruit)\n",
        "\n",
        "X_train_val_fruit = X_train[lime_indexes_train_val, :, :]\n",
        "y_train_val_fruit = [0.99] * len(lime_indexes_train_val)\n",
        "X_train_val_fruit = np.array(X_train_val_fruit)\n",
        "y_train_val_fruit = np.array(y_train_val_fruit)\n",
        "\n",
        "X_test_fruit = X_test[lime_indexes_test, :, :]\n",
        "y_test_fruit = [0.99] * len(lime_indexes_test)\n",
        "X_test_fruit = np.array(X_test_fruit)\n",
        "y_test_fruit = np.array(y_test_fruit)\n",
        "# print(X_test_fruit.shape)\n",
        "# print(y_test_fruit.shape)\n",
        "y_test_fruit = np.append(y_test_fruit, y_test[0])\n",
        "y_test_fruit = np.append(y_test_fruit, y_test[1])\n",
        "X_test_fruit = np.append(X_test_fruit, X_test[[0, 1], :, :], axis=0)\n",
        "\n",
        "# print(X_test_fruit.shape)\n",
        "# print(y_test_fruit.shape)\n",
        "# print(train_for_fruits(X_train_fruit, y_train_fruit, X_train_val_fruit, y_train_val_fruit))\n",
        "lime_model = train_for_fruits(X_train_fruit, y_train_fruit, X_train_val_fruit, y_train_val_fruit)\n",
        "print(lime_model.predict(X_test_fruit))\n",
        "print(y_test_fruit)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "12 train sequences\n",
            "2 test sequences\n",
            "X_train shape: (12, 6, 216)\n",
            "y_train shape: (12,)\n",
            "Build model...\n",
            "[0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99]\n",
            "[0.99 0.99]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train...\n",
            "Train on 12 samples, validate on 2 samples\n",
            "Epoch 1/3\n",
            "12/12 [==============================] - 1s 55ms/sample - loss: 0.0000e+00 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 0s 3ms/sample - loss: 0.0000e+00 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
            "X_test shape: (2, 6, 216)\n",
            "y_test shape: (2,)\n",
            "2/2 [==============================] - 0s 1ms/sample - loss: 0.0000e+00 - acc: 0.0000e+00\n",
            "evaluation metrics\n",
            " [0.0, 0.0]\n",
            "2/2 [==============================] - 0s 1ms/sample - loss: 0.0000e+00 - acc: 0.0000e+00\n",
            "Test score: 0.0\n",
            "Test accuracy: 0.0\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "[0.99 0.99 2.   5.  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgpyX7AsfWVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_for_all_fruits(X_train_new, y_train, X_test_new, y_test):\n",
        "  batch_size = 5\n",
        "  hidden_units = 10\n",
        "  nb_classes = 7\n",
        "  # nb_classes = 1\n",
        "  print('Loading data...')\n",
        "  print(len(X_train_new), 'train sequences')\n",
        "  print(len(X_test_new), 'test sequences')\n",
        "  print('X_train shape:', X_train_new.shape)\n",
        "  print('y_train shape:', y_train.shape)\n",
        "  print('Build model...')\n",
        "\n",
        "  print(y_train)\n",
        "  print(y_test)\n",
        "  y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "  y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "  print('y_train categorical is\\n', y_train)\n",
        "  print('y_test categorical is\\n', y_test)\n",
        "  model = Sequential()\n",
        "\n",
        "  # model.add(Embedding(input_dim=(X_train.shape[1], X_train.shape[2]), output_dim=7, input_length=6))\n",
        "  model.add(LSTM(units=hidden_units, input_shape=X_train.shape[1:], activation='tanh', recurrent_activation ='sigmoid', unit_forget_bias=True, kernel_initializer=\"uniform\", recurrent_initializer=\"uniform\"))\n",
        "  # model.add(CuDNNLSTM(units=hidden_units, activation='tanh', recurrent_activation ='sigmoid'))\n",
        "\n",
        "  model.add(Dense(nb_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])\n",
        "  # model.compile(loss='sparse_categorical_crossentropy', optimizer=\"sgd\", metrics=['accuracy'])\n",
        "\n",
        "  print(\"Train...\")\n",
        "  # with tf.device('/gpu:0'):\n",
        "  model.fit(X_train_new, y_train, batch_size=batch_size, epochs=1000, validation_data=(X_test_new, y_test))\n",
        "\n",
        "  print('X_test shape:', X_test_new.shape)\n",
        "  print('y_test shape:', y_test.shape)\n",
        "  print(\"evaluation metrics\\n\", model.evaluate(X_test_new, y_test, batch_size=batch_size))\n",
        "  score, acc = model.evaluate(X_test_new, y_test, batch_size=batch_size)\n",
        "  print('Test score:', score)\n",
        "  print('Test accuracy:', acc)\n",
        "  # print(\"prediction for test set is\", model.predict(X_test_new))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMWeIetoZSYj",
        "colab_type": "code",
        "outputId": "0a2dce50-0b41-43af-d352-eeb90748fabb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "counter = 0\n",
        "fruit_models = []\n",
        "X_all_train_fruit = []\n",
        "y_all_train_fruit = []\n",
        "\n",
        "lime_indexes_train =  [ i for i in range(len(y_train)) ]\n",
        "lime_indexes_train_val = [lime_indexes_train[random.randrange(0, len(lime_indexes_train))] for i in range(int(0.2 * len(lime_indexes_train)))]\n",
        "temp_lime_indexes_train = []\n",
        "for index in lime_indexes_train:\n",
        "  if index in lime_indexes_train_val:\n",
        "    continue\n",
        "  else:\n",
        "    temp_lime_indexes_train.append(index)\n",
        "lime_indexes_train = temp_lime_indexes_train\n",
        "lime_indexes_test =  [ i for i in range(len(y_test)) ]\n",
        "\n",
        "# print(len(lime_indexes_train), lime_indexes_train)\n",
        "# print(len(lime_indexes_train_val), lime_indexes_train_val)\n",
        "# print(len(y_train))\n",
        "X_all_train_fruit = X_train[lime_indexes_train, :, :]\n",
        "y_all_train_fruit = [y_train[index]  for index in lime_indexes_train]\n",
        "X_all_train_fruit = np.array(X_all_train_fruit)\n",
        "y_all_train_fruit = np.array(y_all_train_fruit)\n",
        "\n",
        "X_all_train_val_fruit = X_train[lime_indexes_train_val, :, :]\n",
        "y_all_train_val_fruit = [y_train[index]  for index in lime_indexes_train_val]\n",
        "X_all_train_val_fruit = np.array(X_all_train_val_fruit)\n",
        "y_all_train_val_fruit = np.array(y_all_train_val_fruit)\n",
        "\n",
        "X_test_fruit = X_test\n",
        "y_test_fruit = y_test\n",
        "X_test_fruit = np.array(X_test_fruit)\n",
        "y_test_fruit = np.array(y_test_fruit)\n",
        "# print(y_all_train_fruit)\n",
        "# print(y_all_train_val_fruit)\n",
        "# print(X_all_train_fruit.shape)\n",
        "# print(y_all_train_fruit.shape)\n",
        "# print(X_all_train_val_fruit.shape)\n",
        "# print(y_all_train_val_fruit.shape)\n",
        "# print(X_test_fruit.shape)\n",
        "# print(y_test_fruit.shape)\n",
        "print(train_for_fruits(X_train_fruit, y_train_fruit, X_train_val_fruit, y_train_val_fruit))\n",
        "fruit_model = train_for_all_fruits(X_all_train_fruit, y_all_train_fruit, X_all_train_val_fruit, y_all_train_val_fruit)\n",
        "test_fruit_predictions = fruit_model.predict(X_test_fruit)\n",
        "print(test_fruit_predictions)\n",
        "rnn_predictions = [list(np.where(predictions == max(predictions))[0])[0] for predictions in test_fruit_predictions]\n",
        "print(rnn_predictions)\n",
        "print(y_test_fruit)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "12 train sequences\n",
            "2 test sequences\n",
            "X_train shape: (12, 6, 216)\n",
            "y_train shape: (12,)\n",
            "Build model...\n",
            "[0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99]\n",
            "[0.99 0.99]\n",
            "Train...\n",
            "Train on 12 samples, validate on 2 samples\n",
            "Epoch 1/3\n",
            "12/12 [==============================] - 0s 28ms/sample - loss: 0.0000e+00 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
            "X_test shape: (2, 6, 216)\n",
            "y_test shape: (2,)\n",
            "2/2 [==============================] - 0s 760us/sample - loss: 0.0000e+00 - acc: 0.0000e+00\n",
            "evaluation metrics\n",
            " [0.0, 0.0]\n",
            "2/2 [==============================] - 0s 806us/sample - loss: 0.0000e+00 - acc: 0.0000e+00\n",
            "Test score: 0.0\n",
            "Test accuracy: 0.0\n",
            "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f5770a8b358>\n",
            "Loading data...\n",
            "79 train sequences\n",
            "18 test sequences\n",
            "X_train shape: (79, 6, 216)\n",
            "y_train shape: (79,)\n",
            "Build model...\n",
            "[2. 3. 5. 2. 4. 5. 3. 4. 4. 3. 6. 3. 4. 3. 3. 2. 5. 0. 0. 2. 0. 5. 3. 6.\n",
            " 2. 0. 0. 1. 2. 2. 2. 5. 5. 1. 0. 1. 4. 2. 4. 3. 1. 3. 5. 5. 0. 6. 2. 4.\n",
            " 0. 1. 1. 6. 1. 1. 3. 5. 1. 6. 4. 4. 6. 5. 5. 2. 4. 0. 3. 5. 6. 1. 4. 6.\n",
            " 2. 6. 6. 5. 1. 3. 6.]\n",
            "[1. 0. 4. 1. 4. 0. 6. 0. 6. 6. 2. 3. 0. 1. 1. 1. 0. 1.]\n",
            "y_train categorical is\n",
            " [[0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]]\n",
            "y_test categorical is\n",
            " [[0. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]]\n",
            "Train...\n",
            "Train on 79 samples, validate on 18 samples\n",
            "Epoch 1/1000\n",
            "79/79 [==============================] - 0s 6ms/sample - loss: 1.9499 - acc: 0.1519 - val_loss: 1.9589 - val_acc: 0.0556\n",
            "Epoch 2/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9487 - acc: 0.1266 - val_loss: 1.9665 - val_acc: 0.0556\n",
            "Epoch 3/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9478 - acc: 0.1519 - val_loss: 1.9740 - val_acc: 0.0000e+00\n",
            "Epoch 4/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9471 - acc: 0.1646 - val_loss: 1.9774 - val_acc: 0.0000e+00\n",
            "Epoch 5/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9469 - acc: 0.1519 - val_loss: 1.9827 - val_acc: 0.0000e+00\n",
            "Epoch 6/1000\n",
            "79/79 [==============================] - 0s 974us/sample - loss: 1.9464 - acc: 0.1772 - val_loss: 1.9873 - val_acc: 0.0000e+00\n",
            "Epoch 7/1000\n",
            "79/79 [==============================] - 0s 988us/sample - loss: 1.9455 - acc: 0.1646 - val_loss: 1.9905 - val_acc: 0.0000e+00\n",
            "Epoch 8/1000\n",
            "79/79 [==============================] - 0s 985us/sample - loss: 1.9456 - acc: 0.1646 - val_loss: 1.9941 - val_acc: 0.0000e+00\n",
            "Epoch 9/1000\n",
            "79/79 [==============================] - 0s 956us/sample - loss: 1.9465 - acc: 0.1646 - val_loss: 1.9953 - val_acc: 0.0000e+00\n",
            "Epoch 10/1000\n",
            "79/79 [==============================] - 0s 987us/sample - loss: 1.9455 - acc: 0.1646 - val_loss: 1.9998 - val_acc: 0.0000e+00\n",
            "Epoch 11/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9429 - acc: 0.1646 - val_loss: 2.0008 - val_acc: 0.0000e+00\n",
            "Epoch 12/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9441 - acc: 0.1646 - val_loss: 2.0033 - val_acc: 0.0000e+00\n",
            "Epoch 13/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9431 - acc: 0.1646 - val_loss: 2.0048 - val_acc: 0.0000e+00\n",
            "Epoch 14/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9427 - acc: 0.1646 - val_loss: 2.0055 - val_acc: 0.0000e+00\n",
            "Epoch 15/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9434 - acc: 0.1646 - val_loss: 2.0064 - val_acc: 0.0000e+00\n",
            "Epoch 16/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9435 - acc: 0.1646 - val_loss: 2.0087 - val_acc: 0.0000e+00\n",
            "Epoch 17/1000\n",
            "79/79 [==============================] - 0s 931us/sample - loss: 1.9426 - acc: 0.1646 - val_loss: 2.0108 - val_acc: 0.0000e+00\n",
            "Epoch 18/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9437 - acc: 0.1646 - val_loss: 2.0095 - val_acc: 0.0000e+00\n",
            "Epoch 19/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9417 - acc: 0.1646 - val_loss: 2.0110 - val_acc: 0.0000e+00\n",
            "Epoch 20/1000\n",
            "79/79 [==============================] - 0s 958us/sample - loss: 1.9429 - acc: 0.1646 - val_loss: 2.0125 - val_acc: 0.0000e+00\n",
            "Epoch 21/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9426 - acc: 0.1646 - val_loss: 2.0140 - val_acc: 0.0000e+00\n",
            "Epoch 22/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9422 - acc: 0.1646 - val_loss: 2.0147 - val_acc: 0.0000e+00\n",
            "Epoch 23/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9423 - acc: 0.1646 - val_loss: 2.0155 - val_acc: 0.0000e+00\n",
            "Epoch 24/1000\n",
            "79/79 [==============================] - 0s 941us/sample - loss: 1.9418 - acc: 0.1646 - val_loss: 2.0141 - val_acc: 0.0000e+00\n",
            "Epoch 25/1000\n",
            "79/79 [==============================] - 0s 936us/sample - loss: 1.9413 - acc: 0.1646 - val_loss: 2.0149 - val_acc: 0.0000e+00\n",
            "Epoch 26/1000\n",
            "79/79 [==============================] - 0s 924us/sample - loss: 1.9409 - acc: 0.1646 - val_loss: 2.0171 - val_acc: 0.0000e+00\n",
            "Epoch 27/1000\n",
            "79/79 [==============================] - 0s 963us/sample - loss: 1.9428 - acc: 0.1646 - val_loss: 2.0170 - val_acc: 0.0000e+00\n",
            "Epoch 28/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9418 - acc: 0.1646 - val_loss: 2.0162 - val_acc: 0.0000e+00\n",
            "Epoch 29/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9409 - acc: 0.1646 - val_loss: 2.0155 - val_acc: 0.0000e+00\n",
            "Epoch 30/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9415 - acc: 0.1646 - val_loss: 2.0185 - val_acc: 0.0000e+00\n",
            "Epoch 31/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9409 - acc: 0.1646 - val_loss: 2.0206 - val_acc: 0.0000e+00\n",
            "Epoch 32/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9417 - acc: 0.1646 - val_loss: 2.0179 - val_acc: 0.0000e+00\n",
            "Epoch 33/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9416 - acc: 0.1646 - val_loss: 2.0188 - val_acc: 0.0000e+00\n",
            "Epoch 34/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9412 - acc: 0.1646 - val_loss: 2.0191 - val_acc: 0.0000e+00\n",
            "Epoch 35/1000\n",
            "79/79 [==============================] - 0s 968us/sample - loss: 1.9406 - acc: 0.1646 - val_loss: 2.0191 - val_acc: 0.0000e+00\n",
            "Epoch 36/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9407 - acc: 0.1646 - val_loss: 2.0192 - val_acc: 0.0000e+00\n",
            "Epoch 37/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9397 - acc: 0.1646 - val_loss: 2.0183 - val_acc: 0.0000e+00\n",
            "Epoch 38/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9407 - acc: 0.1646 - val_loss: 2.0203 - val_acc: 0.0000e+00\n",
            "Epoch 39/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9410 - acc: 0.1646 - val_loss: 2.0195 - val_acc: 0.0000e+00\n",
            "Epoch 40/1000\n",
            "79/79 [==============================] - 0s 985us/sample - loss: 1.9420 - acc: 0.1646 - val_loss: 2.0191 - val_acc: 0.0000e+00\n",
            "Epoch 41/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9404 - acc: 0.1646 - val_loss: 2.0171 - val_acc: 0.0000e+00\n",
            "Epoch 42/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9409 - acc: 0.1646 - val_loss: 2.0188 - val_acc: 0.0000e+00\n",
            "Epoch 43/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9404 - acc: 0.1646 - val_loss: 2.0188 - val_acc: 0.0000e+00\n",
            "Epoch 44/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9396 - acc: 0.1646 - val_loss: 2.0176 - val_acc: 0.0000e+00\n",
            "Epoch 45/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9408 - acc: 0.1646 - val_loss: 2.0178 - val_acc: 0.0000e+00\n",
            "Epoch 46/1000\n",
            "79/79 [==============================] - 0s 959us/sample - loss: 1.9398 - acc: 0.1646 - val_loss: 2.0166 - val_acc: 0.0000e+00\n",
            "Epoch 47/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9403 - acc: 0.1646 - val_loss: 2.0153 - val_acc: 0.0000e+00\n",
            "Epoch 48/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9402 - acc: 0.1646 - val_loss: 2.0169 - val_acc: 0.0000e+00\n",
            "Epoch 49/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9392 - acc: 0.1646 - val_loss: 2.0185 - val_acc: 0.0000e+00\n",
            "Epoch 50/1000\n",
            "79/79 [==============================] - 0s 966us/sample - loss: 1.9387 - acc: 0.1646 - val_loss: 2.0181 - val_acc: 0.0000e+00\n",
            "Epoch 51/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9390 - acc: 0.1646 - val_loss: 2.0203 - val_acc: 0.0000e+00\n",
            "Epoch 52/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9404 - acc: 0.1646 - val_loss: 2.0221 - val_acc: 0.0000e+00\n",
            "Epoch 53/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9391 - acc: 0.1646 - val_loss: 2.0209 - val_acc: 0.0000e+00\n",
            "Epoch 54/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9394 - acc: 0.1646 - val_loss: 2.0200 - val_acc: 0.0000e+00\n",
            "Epoch 55/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9387 - acc: 0.1646 - val_loss: 2.0179 - val_acc: 0.0000e+00\n",
            "Epoch 56/1000\n",
            "79/79 [==============================] - 0s 977us/sample - loss: 1.9381 - acc: 0.1646 - val_loss: 2.0168 - val_acc: 0.0000e+00\n",
            "Epoch 57/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9396 - acc: 0.1646 - val_loss: 2.0157 - val_acc: 0.0000e+00\n",
            "Epoch 58/1000\n",
            "79/79 [==============================] - 0s 969us/sample - loss: 1.9380 - acc: 0.1646 - val_loss: 2.0168 - val_acc: 0.0000e+00\n",
            "Epoch 59/1000\n",
            "79/79 [==============================] - 0s 938us/sample - loss: 1.9382 - acc: 0.1646 - val_loss: 2.0165 - val_acc: 0.0000e+00\n",
            "Epoch 60/1000\n",
            "79/79 [==============================] - 0s 974us/sample - loss: 1.9391 - acc: 0.1646 - val_loss: 2.0159 - val_acc: 0.0000e+00\n",
            "Epoch 61/1000\n",
            "79/79 [==============================] - 0s 988us/sample - loss: 1.9374 - acc: 0.1646 - val_loss: 2.0144 - val_acc: 0.0000e+00\n",
            "Epoch 62/1000\n",
            "79/79 [==============================] - 0s 981us/sample - loss: 1.9379 - acc: 0.1646 - val_loss: 2.0142 - val_acc: 0.0000e+00\n",
            "Epoch 63/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9400 - acc: 0.1646 - val_loss: 2.0163 - val_acc: 0.0000e+00\n",
            "Epoch 64/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9376 - acc: 0.1646 - val_loss: 2.0174 - val_acc: 0.0000e+00\n",
            "Epoch 65/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9389 - acc: 0.1646 - val_loss: 2.0141 - val_acc: 0.0000e+00\n",
            "Epoch 66/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9382 - acc: 0.1646 - val_loss: 2.0151 - val_acc: 0.0000e+00\n",
            "Epoch 67/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9371 - acc: 0.1646 - val_loss: 2.0164 - val_acc: 0.0000e+00\n",
            "Epoch 68/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9392 - acc: 0.1646 - val_loss: 2.0157 - val_acc: 0.0000e+00\n",
            "Epoch 69/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9380 - acc: 0.1646 - val_loss: 2.0183 - val_acc: 0.0000e+00\n",
            "Epoch 70/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9391 - acc: 0.1646 - val_loss: 2.0180 - val_acc: 0.0000e+00\n",
            "Epoch 71/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9373 - acc: 0.1646 - val_loss: 2.0180 - val_acc: 0.0000e+00\n",
            "Epoch 72/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9376 - acc: 0.1646 - val_loss: 2.0167 - val_acc: 0.0000e+00\n",
            "Epoch 73/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9379 - acc: 0.1646 - val_loss: 2.0159 - val_acc: 0.0000e+00\n",
            "Epoch 74/1000\n",
            "79/79 [==============================] - 0s 991us/sample - loss: 1.9373 - acc: 0.1646 - val_loss: 2.0170 - val_acc: 0.0000e+00\n",
            "Epoch 75/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9371 - acc: 0.1646 - val_loss: 2.0165 - val_acc: 0.0000e+00\n",
            "Epoch 76/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9374 - acc: 0.1646 - val_loss: 2.0156 - val_acc: 0.0000e+00\n",
            "Epoch 77/1000\n",
            "79/79 [==============================] - 0s 988us/sample - loss: 1.9368 - acc: 0.1646 - val_loss: 2.0185 - val_acc: 0.0000e+00\n",
            "Epoch 78/1000\n",
            "79/79 [==============================] - 0s 986us/sample - loss: 1.9376 - acc: 0.1646 - val_loss: 2.0187 - val_acc: 0.0000e+00\n",
            "Epoch 79/1000\n",
            "79/79 [==============================] - 0s 971us/sample - loss: 1.9368 - acc: 0.1646 - val_loss: 2.0183 - val_acc: 0.0000e+00\n",
            "Epoch 80/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9360 - acc: 0.1646 - val_loss: 2.0191 - val_acc: 0.0000e+00\n",
            "Epoch 81/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9374 - acc: 0.1646 - val_loss: 2.0182 - val_acc: 0.0000e+00\n",
            "Epoch 82/1000\n",
            "79/79 [==============================] - 0s 993us/sample - loss: 1.9363 - acc: 0.1646 - val_loss: 2.0165 - val_acc: 0.0000e+00\n",
            "Epoch 83/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9367 - acc: 0.1646 - val_loss: 2.0159 - val_acc: 0.0000e+00\n",
            "Epoch 84/1000\n",
            "79/79 [==============================] - 0s 950us/sample - loss: 1.9365 - acc: 0.1646 - val_loss: 2.0165 - val_acc: 0.0000e+00\n",
            "Epoch 85/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9372 - acc: 0.1646 - val_loss: 2.0174 - val_acc: 0.0000e+00\n",
            "Epoch 86/1000\n",
            "79/79 [==============================] - 0s 953us/sample - loss: 1.9360 - acc: 0.1646 - val_loss: 2.0151 - val_acc: 0.0000e+00\n",
            "Epoch 87/1000\n",
            "79/79 [==============================] - 0s 955us/sample - loss: 1.9361 - acc: 0.1646 - val_loss: 2.0161 - val_acc: 0.0000e+00\n",
            "Epoch 88/1000\n",
            "79/79 [==============================] - 0s 962us/sample - loss: 1.9372 - acc: 0.1646 - val_loss: 2.0178 - val_acc: 0.0000e+00\n",
            "Epoch 89/1000\n",
            "79/79 [==============================] - 0s 966us/sample - loss: 1.9355 - acc: 0.1646 - val_loss: 2.0183 - val_acc: 0.0000e+00\n",
            "Epoch 90/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9360 - acc: 0.1646 - val_loss: 2.0175 - val_acc: 0.0000e+00\n",
            "Epoch 91/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9354 - acc: 0.1646 - val_loss: 2.0179 - val_acc: 0.0000e+00\n",
            "Epoch 92/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9351 - acc: 0.1646 - val_loss: 2.0195 - val_acc: 0.0000e+00\n",
            "Epoch 93/1000\n",
            "79/79 [==============================] - 0s 979us/sample - loss: 1.9359 - acc: 0.1646 - val_loss: 2.0196 - val_acc: 0.0000e+00\n",
            "Epoch 94/1000\n",
            "79/79 [==============================] - 0s 941us/sample - loss: 1.9346 - acc: 0.1646 - val_loss: 2.0212 - val_acc: 0.0000e+00\n",
            "Epoch 95/1000\n",
            "79/79 [==============================] - 0s 996us/sample - loss: 1.9356 - acc: 0.1646 - val_loss: 2.0208 - val_acc: 0.0000e+00\n",
            "Epoch 96/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9362 - acc: 0.1646 - val_loss: 2.0183 - val_acc: 0.0000e+00\n",
            "Epoch 97/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9367 - acc: 0.1646 - val_loss: 2.0181 - val_acc: 0.0000e+00\n",
            "Epoch 98/1000\n",
            "79/79 [==============================] - 0s 975us/sample - loss: 1.9355 - acc: 0.1646 - val_loss: 2.0185 - val_acc: 0.0000e+00\n",
            "Epoch 99/1000\n",
            "79/79 [==============================] - 0s 953us/sample - loss: 1.9357 - acc: 0.1646 - val_loss: 2.0195 - val_acc: 0.0000e+00\n",
            "Epoch 100/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9343 - acc: 0.1646 - val_loss: 2.0205 - val_acc: 0.0000e+00\n",
            "Epoch 101/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9347 - acc: 0.1646 - val_loss: 2.0201 - val_acc: 0.0000e+00\n",
            "Epoch 102/1000\n",
            "79/79 [==============================] - 0s 989us/sample - loss: 1.9349 - acc: 0.1646 - val_loss: 2.0194 - val_acc: 0.0000e+00\n",
            "Epoch 103/1000\n",
            "79/79 [==============================] - 0s 985us/sample - loss: 1.9347 - acc: 0.1646 - val_loss: 2.0192 - val_acc: 0.0000e+00\n",
            "Epoch 104/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9342 - acc: 0.1646 - val_loss: 2.0169 - val_acc: 0.0000e+00\n",
            "Epoch 105/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9346 - acc: 0.1646 - val_loss: 2.0157 - val_acc: 0.0000e+00\n",
            "Epoch 106/1000\n",
            "79/79 [==============================] - 0s 967us/sample - loss: 1.9336 - acc: 0.1646 - val_loss: 2.0133 - val_acc: 0.0000e+00\n",
            "Epoch 107/1000\n",
            "79/79 [==============================] - 0s 947us/sample - loss: 1.9351 - acc: 0.1646 - val_loss: 2.0150 - val_acc: 0.0000e+00\n",
            "Epoch 108/1000\n",
            "79/79 [==============================] - 0s 906us/sample - loss: 1.9345 - acc: 0.1646 - val_loss: 2.0163 - val_acc: 0.0000e+00\n",
            "Epoch 109/1000\n",
            "79/79 [==============================] - 0s 964us/sample - loss: 1.9348 - acc: 0.1646 - val_loss: 2.0152 - val_acc: 0.0000e+00\n",
            "Epoch 110/1000\n",
            "79/79 [==============================] - 0s 940us/sample - loss: 1.9344 - acc: 0.1646 - val_loss: 2.0143 - val_acc: 0.0000e+00\n",
            "Epoch 111/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9342 - acc: 0.1646 - val_loss: 2.0151 - val_acc: 0.0000e+00\n",
            "Epoch 112/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9351 - acc: 0.1646 - val_loss: 2.0174 - val_acc: 0.0000e+00\n",
            "Epoch 113/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9346 - acc: 0.1646 - val_loss: 2.0147 - val_acc: 0.0000e+00\n",
            "Epoch 114/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9338 - acc: 0.1646 - val_loss: 2.0138 - val_acc: 0.0000e+00\n",
            "Epoch 115/1000\n",
            "79/79 [==============================] - 0s 950us/sample - loss: 1.9340 - acc: 0.1646 - val_loss: 2.0145 - val_acc: 0.0000e+00\n",
            "Epoch 116/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9332 - acc: 0.1646 - val_loss: 2.0137 - val_acc: 0.0000e+00\n",
            "Epoch 117/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9348 - acc: 0.1646 - val_loss: 2.0133 - val_acc: 0.0000e+00\n",
            "Epoch 118/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9332 - acc: 0.1646 - val_loss: 2.0136 - val_acc: 0.0000e+00\n",
            "Epoch 119/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9329 - acc: 0.1646 - val_loss: 2.0137 - val_acc: 0.0000e+00\n",
            "Epoch 120/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9340 - acc: 0.1646 - val_loss: 2.0143 - val_acc: 0.0000e+00\n",
            "Epoch 121/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9328 - acc: 0.1646 - val_loss: 2.0132 - val_acc: 0.0000e+00\n",
            "Epoch 122/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9343 - acc: 0.1646 - val_loss: 2.0130 - val_acc: 0.0000e+00\n",
            "Epoch 123/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9334 - acc: 0.1646 - val_loss: 2.0130 - val_acc: 0.0000e+00\n",
            "Epoch 124/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9331 - acc: 0.1646 - val_loss: 2.0131 - val_acc: 0.0000e+00\n",
            "Epoch 125/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9322 - acc: 0.1646 - val_loss: 2.0141 - val_acc: 0.0000e+00\n",
            "Epoch 126/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9328 - acc: 0.1646 - val_loss: 2.0155 - val_acc: 0.0000e+00\n",
            "Epoch 127/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9320 - acc: 0.1646 - val_loss: 2.0164 - val_acc: 0.0000e+00\n",
            "Epoch 128/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9336 - acc: 0.1646 - val_loss: 2.0143 - val_acc: 0.0000e+00\n",
            "Epoch 129/1000\n",
            "79/79 [==============================] - 0s 968us/sample - loss: 1.9324 - acc: 0.1646 - val_loss: 2.0139 - val_acc: 0.0000e+00\n",
            "Epoch 130/1000\n",
            "79/79 [==============================] - 0s 956us/sample - loss: 1.9322 - acc: 0.1646 - val_loss: 2.0139 - val_acc: 0.0000e+00\n",
            "Epoch 131/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9327 - acc: 0.1646 - val_loss: 2.0132 - val_acc: 0.0000e+00\n",
            "Epoch 132/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9329 - acc: 0.1646 - val_loss: 2.0115 - val_acc: 0.0000e+00\n",
            "Epoch 133/1000\n",
            "79/79 [==============================] - 0s 944us/sample - loss: 1.9317 - acc: 0.1646 - val_loss: 2.0112 - val_acc: 0.0000e+00\n",
            "Epoch 134/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9321 - acc: 0.1646 - val_loss: 2.0097 - val_acc: 0.0000e+00\n",
            "Epoch 135/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9323 - acc: 0.1646 - val_loss: 2.0093 - val_acc: 0.0000e+00\n",
            "Epoch 136/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9313 - acc: 0.1646 - val_loss: 2.0094 - val_acc: 0.0000e+00\n",
            "Epoch 137/1000\n",
            "79/79 [==============================] - 0s 984us/sample - loss: 1.9323 - acc: 0.1646 - val_loss: 2.0087 - val_acc: 0.0000e+00\n",
            "Epoch 138/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9319 - acc: 0.1646 - val_loss: 2.0092 - val_acc: 0.0000e+00\n",
            "Epoch 139/1000\n",
            "79/79 [==============================] - 0s 974us/sample - loss: 1.9317 - acc: 0.1646 - val_loss: 2.0094 - val_acc: 0.0000e+00\n",
            "Epoch 140/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9313 - acc: 0.1646 - val_loss: 2.0110 - val_acc: 0.0000e+00\n",
            "Epoch 141/1000\n",
            "79/79 [==============================] - 0s 995us/sample - loss: 1.9319 - acc: 0.1646 - val_loss: 2.0106 - val_acc: 0.0000e+00\n",
            "Epoch 142/1000\n",
            "79/79 [==============================] - 0s 948us/sample - loss: 1.9310 - acc: 0.1646 - val_loss: 2.0099 - val_acc: 0.0000e+00\n",
            "Epoch 143/1000\n",
            "79/79 [==============================] - 0s 995us/sample - loss: 1.9313 - acc: 0.1646 - val_loss: 2.0090 - val_acc: 0.0000e+00\n",
            "Epoch 144/1000\n",
            "79/79 [==============================] - 0s 991us/sample - loss: 1.9308 - acc: 0.1646 - val_loss: 2.0098 - val_acc: 0.0000e+00\n",
            "Epoch 145/1000\n",
            "79/79 [==============================] - 0s 999us/sample - loss: 1.9312 - acc: 0.1646 - val_loss: 2.0084 - val_acc: 0.0000e+00\n",
            "Epoch 146/1000\n",
            "79/79 [==============================] - 0s 966us/sample - loss: 1.9314 - acc: 0.1646 - val_loss: 2.0076 - val_acc: 0.0000e+00\n",
            "Epoch 147/1000\n",
            "79/79 [==============================] - 0s 927us/sample - loss: 1.9317 - acc: 0.1646 - val_loss: 2.0062 - val_acc: 0.0000e+00\n",
            "Epoch 148/1000\n",
            "79/79 [==============================] - 0s 959us/sample - loss: 1.9298 - acc: 0.1646 - val_loss: 2.0065 - val_acc: 0.0000e+00\n",
            "Epoch 149/1000\n",
            "79/79 [==============================] - 0s 954us/sample - loss: 1.9306 - acc: 0.1646 - val_loss: 2.0070 - val_acc: 0.0000e+00\n",
            "Epoch 150/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9302 - acc: 0.1646 - val_loss: 2.0061 - val_acc: 0.0000e+00\n",
            "Epoch 151/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9296 - acc: 0.1646 - val_loss: 2.0066 - val_acc: 0.0000e+00\n",
            "Epoch 152/1000\n",
            "79/79 [==============================] - 0s 992us/sample - loss: 1.9296 - acc: 0.1646 - val_loss: 2.0072 - val_acc: 0.0000e+00\n",
            "Epoch 153/1000\n",
            "79/79 [==============================] - 0s 960us/sample - loss: 1.9292 - acc: 0.1646 - val_loss: 2.0090 - val_acc: 0.0000e+00\n",
            "Epoch 154/1000\n",
            "79/79 [==============================] - 0s 997us/sample - loss: 1.9293 - acc: 0.1646 - val_loss: 2.0098 - val_acc: 0.0000e+00\n",
            "Epoch 155/1000\n",
            "79/79 [==============================] - 0s 986us/sample - loss: 1.9295 - acc: 0.1646 - val_loss: 2.0102 - val_acc: 0.0000e+00\n",
            "Epoch 156/1000\n",
            "79/79 [==============================] - 0s 978us/sample - loss: 1.9295 - acc: 0.1646 - val_loss: 2.0091 - val_acc: 0.0000e+00\n",
            "Epoch 157/1000\n",
            "79/79 [==============================] - 0s 983us/sample - loss: 1.9294 - acc: 0.1646 - val_loss: 2.0071 - val_acc: 0.0000e+00\n",
            "Epoch 158/1000\n",
            "79/79 [==============================] - 0s 954us/sample - loss: 1.9291 - acc: 0.1646 - val_loss: 2.0082 - val_acc: 0.0000e+00\n",
            "Epoch 159/1000\n",
            "79/79 [==============================] - 0s 917us/sample - loss: 1.9289 - acc: 0.1646 - val_loss: 2.0071 - val_acc: 0.0000e+00\n",
            "Epoch 160/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9296 - acc: 0.1646 - val_loss: 2.0069 - val_acc: 0.0000e+00\n",
            "Epoch 161/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9296 - acc: 0.1646 - val_loss: 2.0084 - val_acc: 0.0000e+00\n",
            "Epoch 162/1000\n",
            "79/79 [==============================] - 0s 924us/sample - loss: 1.9290 - acc: 0.1646 - val_loss: 2.0107 - val_acc: 0.0000e+00\n",
            "Epoch 163/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9292 - acc: 0.1646 - val_loss: 2.0105 - val_acc: 0.0000e+00\n",
            "Epoch 164/1000\n",
            "79/79 [==============================] - 0s 980us/sample - loss: 1.9296 - acc: 0.1646 - val_loss: 2.0096 - val_acc: 0.0000e+00\n",
            "Epoch 165/1000\n",
            "79/79 [==============================] - 0s 987us/sample - loss: 1.9296 - acc: 0.1646 - val_loss: 2.0088 - val_acc: 0.0000e+00\n",
            "Epoch 166/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9283 - acc: 0.1646 - val_loss: 2.0077 - val_acc: 0.0000e+00\n",
            "Epoch 167/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9284 - acc: 0.1646 - val_loss: 2.0080 - val_acc: 0.0000e+00\n",
            "Epoch 168/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9294 - acc: 0.1646 - val_loss: 2.0064 - val_acc: 0.0000e+00\n",
            "Epoch 169/1000\n",
            "79/79 [==============================] - 0s 973us/sample - loss: 1.9288 - acc: 0.1646 - val_loss: 2.0050 - val_acc: 0.0000e+00\n",
            "Epoch 170/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9281 - acc: 0.1646 - val_loss: 2.0064 - val_acc: 0.0000e+00\n",
            "Epoch 171/1000\n",
            "79/79 [==============================] - 0s 941us/sample - loss: 1.9285 - acc: 0.1646 - val_loss: 2.0063 - val_acc: 0.0000e+00\n",
            "Epoch 172/1000\n",
            "79/79 [==============================] - 0s 961us/sample - loss: 1.9276 - acc: 0.1646 - val_loss: 2.0062 - val_acc: 0.0000e+00\n",
            "Epoch 173/1000\n",
            "79/79 [==============================] - 0s 993us/sample - loss: 1.9284 - acc: 0.1646 - val_loss: 2.0058 - val_acc: 0.0000e+00\n",
            "Epoch 174/1000\n",
            "79/79 [==============================] - 0s 921us/sample - loss: 1.9284 - acc: 0.1646 - val_loss: 2.0064 - val_acc: 0.0000e+00\n",
            "Epoch 175/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9275 - acc: 0.1646 - val_loss: 2.0062 - val_acc: 0.0000e+00\n",
            "Epoch 176/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9280 - acc: 0.1646 - val_loss: 2.0041 - val_acc: 0.0000e+00\n",
            "Epoch 177/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9273 - acc: 0.1646 - val_loss: 2.0036 - val_acc: 0.0000e+00\n",
            "Epoch 178/1000\n",
            "79/79 [==============================] - 0s 990us/sample - loss: 1.9275 - acc: 0.1646 - val_loss: 2.0027 - val_acc: 0.0000e+00\n",
            "Epoch 179/1000\n",
            "79/79 [==============================] - 0s 981us/sample - loss: 1.9273 - acc: 0.1646 - val_loss: 2.0027 - val_acc: 0.0000e+00\n",
            "Epoch 180/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9272 - acc: 0.1646 - val_loss: 2.0030 - val_acc: 0.0000e+00\n",
            "Epoch 181/1000\n",
            "79/79 [==============================] - 0s 965us/sample - loss: 1.9273 - acc: 0.1646 - val_loss: 2.0031 - val_acc: 0.0000e+00\n",
            "Epoch 182/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9278 - acc: 0.1646 - val_loss: 2.0023 - val_acc: 0.0000e+00\n",
            "Epoch 183/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9283 - acc: 0.1646 - val_loss: 2.0061 - val_acc: 0.0000e+00\n",
            "Epoch 184/1000\n",
            "79/79 [==============================] - 0s 965us/sample - loss: 1.9268 - acc: 0.1646 - val_loss: 2.0043 - val_acc: 0.0000e+00\n",
            "Epoch 185/1000\n",
            "79/79 [==============================] - 0s 985us/sample - loss: 1.9270 - acc: 0.1646 - val_loss: 2.0026 - val_acc: 0.0000e+00\n",
            "Epoch 186/1000\n",
            "79/79 [==============================] - 0s 972us/sample - loss: 1.9268 - acc: 0.1646 - val_loss: 2.0018 - val_acc: 0.0000e+00\n",
            "Epoch 187/1000\n",
            "79/79 [==============================] - 0s 952us/sample - loss: 1.9265 - acc: 0.1646 - val_loss: 2.0020 - val_acc: 0.0000e+00\n",
            "Epoch 188/1000\n",
            "79/79 [==============================] - 0s 979us/sample - loss: 1.9267 - acc: 0.1646 - val_loss: 2.0024 - val_acc: 0.0000e+00\n",
            "Epoch 189/1000\n",
            "79/79 [==============================] - 0s 940us/sample - loss: 1.9272 - acc: 0.1646 - val_loss: 2.0023 - val_acc: 0.0000e+00\n",
            "Epoch 190/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9258 - acc: 0.1646 - val_loss: 2.0028 - val_acc: 0.0000e+00\n",
            "Epoch 191/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9252 - acc: 0.1646 - val_loss: 2.0027 - val_acc: 0.0000e+00\n",
            "Epoch 192/1000\n",
            "79/79 [==============================] - 0s 957us/sample - loss: 1.9263 - acc: 0.1646 - val_loss: 2.0026 - val_acc: 0.0000e+00\n",
            "Epoch 193/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9262 - acc: 0.1646 - val_loss: 2.0022 - val_acc: 0.0000e+00\n",
            "Epoch 194/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9259 - acc: 0.1646 - val_loss: 2.0044 - val_acc: 0.0000e+00\n",
            "Epoch 195/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9261 - acc: 0.1646 - val_loss: 2.0043 - val_acc: 0.0000e+00\n",
            "Epoch 196/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9269 - acc: 0.1646 - val_loss: 2.0038 - val_acc: 0.0000e+00\n",
            "Epoch 197/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9254 - acc: 0.1646 - val_loss: 2.0027 - val_acc: 0.0000e+00\n",
            "Epoch 198/1000\n",
            "79/79 [==============================] - 0s 911us/sample - loss: 1.9242 - acc: 0.1646 - val_loss: 2.0040 - val_acc: 0.0000e+00\n",
            "Epoch 199/1000\n",
            "79/79 [==============================] - 0s 997us/sample - loss: 1.9260 - acc: 0.1646 - val_loss: 2.0019 - val_acc: 0.0000e+00\n",
            "Epoch 200/1000\n",
            "79/79 [==============================] - 0s 973us/sample - loss: 1.9255 - acc: 0.1646 - val_loss: 2.0006 - val_acc: 0.0000e+00\n",
            "Epoch 201/1000\n",
            "79/79 [==============================] - 0s 997us/sample - loss: 1.9248 - acc: 0.1646 - val_loss: 2.0002 - val_acc: 0.0000e+00\n",
            "Epoch 202/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9244 - acc: 0.1646 - val_loss: 2.0002 - val_acc: 0.0000e+00\n",
            "Epoch 203/1000\n",
            "79/79 [==============================] - 0s 985us/sample - loss: 1.9250 - acc: 0.1646 - val_loss: 2.0010 - val_acc: 0.0000e+00\n",
            "Epoch 204/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9246 - acc: 0.1646 - val_loss: 2.0021 - val_acc: 0.0000e+00\n",
            "Epoch 205/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9247 - acc: 0.1646 - val_loss: 2.0032 - val_acc: 0.0000e+00\n",
            "Epoch 206/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9244 - acc: 0.1646 - val_loss: 2.0029 - val_acc: 0.0000e+00\n",
            "Epoch 207/1000\n",
            "79/79 [==============================] - 0s 982us/sample - loss: 1.9237 - acc: 0.1646 - val_loss: 2.0044 - val_acc: 0.0000e+00\n",
            "Epoch 208/1000\n",
            "79/79 [==============================] - 0s 969us/sample - loss: 1.9255 - acc: 0.1646 - val_loss: 2.0035 - val_acc: 0.0000e+00\n",
            "Epoch 209/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9238 - acc: 0.1646 - val_loss: 2.0043 - val_acc: 0.0000e+00\n",
            "Epoch 210/1000\n",
            "79/79 [==============================] - 0s 951us/sample - loss: 1.9235 - acc: 0.1646 - val_loss: 2.0038 - val_acc: 0.0000e+00\n",
            "Epoch 211/1000\n",
            "79/79 [==============================] - 0s 981us/sample - loss: 1.9235 - acc: 0.1646 - val_loss: 2.0042 - val_acc: 0.0000e+00\n",
            "Epoch 212/1000\n",
            "79/79 [==============================] - 0s 983us/sample - loss: 1.9232 - acc: 0.1646 - val_loss: 2.0022 - val_acc: 0.0000e+00\n",
            "Epoch 213/1000\n",
            "79/79 [==============================] - 0s 950us/sample - loss: 1.9237 - acc: 0.1646 - val_loss: 2.0036 - val_acc: 0.0000e+00\n",
            "Epoch 214/1000\n",
            "79/79 [==============================] - 0s 950us/sample - loss: 1.9227 - acc: 0.1646 - val_loss: 2.0044 - val_acc: 0.0000e+00\n",
            "Epoch 215/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9223 - acc: 0.1646 - val_loss: 2.0035 - val_acc: 0.0000e+00\n",
            "Epoch 216/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9245 - acc: 0.1646 - val_loss: 2.0022 - val_acc: 0.0000e+00\n",
            "Epoch 217/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9216 - acc: 0.1646 - val_loss: 2.0031 - val_acc: 0.0000e+00\n",
            "Epoch 218/1000\n",
            "79/79 [==============================] - 0s 959us/sample - loss: 1.9221 - acc: 0.1646 - val_loss: 2.0034 - val_acc: 0.0000e+00\n",
            "Epoch 219/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9226 - acc: 0.1646 - val_loss: 2.0016 - val_acc: 0.0000e+00\n",
            "Epoch 220/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9225 - acc: 0.1646 - val_loss: 2.0008 - val_acc: 0.0000e+00\n",
            "Epoch 221/1000\n",
            "79/79 [==============================] - 0s 958us/sample - loss: 1.9219 - acc: 0.1646 - val_loss: 2.0004 - val_acc: 0.0000e+00\n",
            "Epoch 222/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9217 - acc: 0.1646 - val_loss: 1.9996 - val_acc: 0.0000e+00\n",
            "Epoch 223/1000\n",
            "79/79 [==============================] - 0s 954us/sample - loss: 1.9218 - acc: 0.1646 - val_loss: 1.9988 - val_acc: 0.0000e+00\n",
            "Epoch 224/1000\n",
            "79/79 [==============================] - 0s 969us/sample - loss: 1.9220 - acc: 0.1646 - val_loss: 1.9989 - val_acc: 0.0000e+00\n",
            "Epoch 225/1000\n",
            "79/79 [==============================] - 0s 976us/sample - loss: 1.9203 - acc: 0.1646 - val_loss: 1.9982 - val_acc: 0.0000e+00\n",
            "Epoch 226/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9220 - acc: 0.1646 - val_loss: 1.9972 - val_acc: 0.0000e+00\n",
            "Epoch 227/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9215 - acc: 0.1646 - val_loss: 1.9980 - val_acc: 0.0000e+00\n",
            "Epoch 228/1000\n",
            "79/79 [==============================] - 0s 998us/sample - loss: 1.9214 - acc: 0.1646 - val_loss: 2.0000 - val_acc: 0.0000e+00\n",
            "Epoch 229/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9216 - acc: 0.1646 - val_loss: 2.0012 - val_acc: 0.0000e+00\n",
            "Epoch 230/1000\n",
            "79/79 [==============================] - 0s 997us/sample - loss: 1.9217 - acc: 0.1646 - val_loss: 2.0031 - val_acc: 0.0000e+00\n",
            "Epoch 231/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9210 - acc: 0.1646 - val_loss: 2.0025 - val_acc: 0.0000e+00\n",
            "Epoch 232/1000\n",
            "79/79 [==============================] - 0s 960us/sample - loss: 1.9223 - acc: 0.1646 - val_loss: 2.0026 - val_acc: 0.0000e+00\n",
            "Epoch 233/1000\n",
            "79/79 [==============================] - 0s 995us/sample - loss: 1.9215 - acc: 0.1646 - val_loss: 2.0035 - val_acc: 0.0000e+00\n",
            "Epoch 234/1000\n",
            "79/79 [==============================] - 0s 986us/sample - loss: 1.9195 - acc: 0.1646 - val_loss: 2.0046 - val_acc: 0.0000e+00\n",
            "Epoch 235/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9209 - acc: 0.1646 - val_loss: 2.0037 - val_acc: 0.0000e+00\n",
            "Epoch 236/1000\n",
            "79/79 [==============================] - 0s 959us/sample - loss: 1.9205 - acc: 0.1646 - val_loss: 2.0034 - val_acc: 0.0000e+00\n",
            "Epoch 237/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9216 - acc: 0.1772 - val_loss: 2.0068 - val_acc: 0.0000e+00\n",
            "Epoch 238/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9208 - acc: 0.1646 - val_loss: 2.0045 - val_acc: 0.0000e+00\n",
            "Epoch 239/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9203 - acc: 0.1646 - val_loss: 2.0015 - val_acc: 0.0000e+00\n",
            "Epoch 240/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9197 - acc: 0.1646 - val_loss: 2.0010 - val_acc: 0.0000e+00\n",
            "Epoch 241/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9205 - acc: 0.1646 - val_loss: 2.0011 - val_acc: 0.0000e+00\n",
            "Epoch 242/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9189 - acc: 0.1646 - val_loss: 1.9997 - val_acc: 0.0000e+00\n",
            "Epoch 243/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9185 - acc: 0.1646 - val_loss: 1.9967 - val_acc: 0.0000e+00\n",
            "Epoch 244/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9186 - acc: 0.1646 - val_loss: 1.9962 - val_acc: 0.0000e+00\n",
            "Epoch 245/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9184 - acc: 0.1646 - val_loss: 1.9977 - val_acc: 0.0000e+00\n",
            "Epoch 246/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9182 - acc: 0.1646 - val_loss: 1.9975 - val_acc: 0.0000e+00\n",
            "Epoch 247/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9183 - acc: 0.1646 - val_loss: 1.9975 - val_acc: 0.0000e+00\n",
            "Epoch 248/1000\n",
            "79/79 [==============================] - 0s 983us/sample - loss: 1.9179 - acc: 0.1646 - val_loss: 1.9975 - val_acc: 0.0000e+00\n",
            "Epoch 249/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9180 - acc: 0.1646 - val_loss: 1.9990 - val_acc: 0.0000e+00\n",
            "Epoch 250/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9185 - acc: 0.1646 - val_loss: 1.9995 - val_acc: 0.0000e+00\n",
            "Epoch 251/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9193 - acc: 0.1646 - val_loss: 2.0022 - val_acc: 0.0000e+00\n",
            "Epoch 252/1000\n",
            "79/79 [==============================] - 0s 997us/sample - loss: 1.9179 - acc: 0.1646 - val_loss: 2.0006 - val_acc: 0.0000e+00\n",
            "Epoch 253/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9173 - acc: 0.1646 - val_loss: 1.9969 - val_acc: 0.0000e+00\n",
            "Epoch 254/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9170 - acc: 0.1646 - val_loss: 1.9981 - val_acc: 0.0000e+00\n",
            "Epoch 255/1000\n",
            "79/79 [==============================] - 0s 941us/sample - loss: 1.9164 - acc: 0.1646 - val_loss: 1.9967 - val_acc: 0.0000e+00\n",
            "Epoch 256/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9167 - acc: 0.1646 - val_loss: 1.9948 - val_acc: 0.0000e+00\n",
            "Epoch 257/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9190 - acc: 0.1646 - val_loss: 1.9955 - val_acc: 0.0000e+00\n",
            "Epoch 258/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9162 - acc: 0.1646 - val_loss: 1.9931 - val_acc: 0.0000e+00\n",
            "Epoch 259/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9162 - acc: 0.1646 - val_loss: 1.9944 - val_acc: 0.0000e+00\n",
            "Epoch 260/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9177 - acc: 0.1646 - val_loss: 1.9970 - val_acc: 0.0000e+00\n",
            "Epoch 261/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9165 - acc: 0.1646 - val_loss: 1.9957 - val_acc: 0.0000e+00\n",
            "Epoch 262/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9154 - acc: 0.1646 - val_loss: 1.9960 - val_acc: 0.0000e+00\n",
            "Epoch 263/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9160 - acc: 0.1646 - val_loss: 1.9951 - val_acc: 0.0000e+00\n",
            "Epoch 264/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9152 - acc: 0.1646 - val_loss: 1.9949 - val_acc: 0.0000e+00\n",
            "Epoch 265/1000\n",
            "79/79 [==============================] - 0s 998us/sample - loss: 1.9149 - acc: 0.1646 - val_loss: 1.9939 - val_acc: 0.0000e+00\n",
            "Epoch 266/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9153 - acc: 0.1646 - val_loss: 1.9931 - val_acc: 0.0000e+00\n",
            "Epoch 267/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9143 - acc: 0.1646 - val_loss: 1.9919 - val_acc: 0.0000e+00\n",
            "Epoch 268/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9148 - acc: 0.1646 - val_loss: 1.9919 - val_acc: 0.0000e+00\n",
            "Epoch 269/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9148 - acc: 0.1646 - val_loss: 1.9916 - val_acc: 0.0000e+00\n",
            "Epoch 270/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9142 - acc: 0.1646 - val_loss: 1.9919 - val_acc: 0.0000e+00\n",
            "Epoch 271/1000\n",
            "79/79 [==============================] - 0s 965us/sample - loss: 1.9143 - acc: 0.1646 - val_loss: 1.9909 - val_acc: 0.0000e+00\n",
            "Epoch 272/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9130 - acc: 0.1646 - val_loss: 1.9907 - val_acc: 0.0000e+00\n",
            "Epoch 273/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9140 - acc: 0.1646 - val_loss: 1.9896 - val_acc: 0.0000e+00\n",
            "Epoch 274/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9136 - acc: 0.1646 - val_loss: 1.9883 - val_acc: 0.0000e+00\n",
            "Epoch 275/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9131 - acc: 0.1646 - val_loss: 1.9885 - val_acc: 0.0000e+00\n",
            "Epoch 276/1000\n",
            "79/79 [==============================] - 0s 968us/sample - loss: 1.9116 - acc: 0.1646 - val_loss: 1.9872 - val_acc: 0.0000e+00\n",
            "Epoch 277/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9141 - acc: 0.1646 - val_loss: 1.9864 - val_acc: 0.0000e+00\n",
            "Epoch 278/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9121 - acc: 0.1646 - val_loss: 1.9873 - val_acc: 0.0000e+00\n",
            "Epoch 279/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9129 - acc: 0.1646 - val_loss: 1.9842 - val_acc: 0.0000e+00\n",
            "Epoch 280/1000\n",
            "79/79 [==============================] - 0s 991us/sample - loss: 1.9129 - acc: 0.1646 - val_loss: 1.9857 - val_acc: 0.0000e+00\n",
            "Epoch 281/1000\n",
            "79/79 [==============================] - 0s 945us/sample - loss: 1.9146 - acc: 0.1646 - val_loss: 1.9837 - val_acc: 0.0000e+00\n",
            "Epoch 282/1000\n",
            "79/79 [==============================] - 0s 983us/sample - loss: 1.9126 - acc: 0.1772 - val_loss: 1.9860 - val_acc: 0.0000e+00\n",
            "Epoch 283/1000\n",
            "79/79 [==============================] - 0s 992us/sample - loss: 1.9123 - acc: 0.1646 - val_loss: 1.9834 - val_acc: 0.0000e+00\n",
            "Epoch 284/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9117 - acc: 0.1646 - val_loss: 1.9844 - val_acc: 0.0000e+00\n",
            "Epoch 285/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9104 - acc: 0.1646 - val_loss: 1.9865 - val_acc: 0.0000e+00\n",
            "Epoch 286/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9106 - acc: 0.1646 - val_loss: 1.9865 - val_acc: 0.0000e+00\n",
            "Epoch 287/1000\n",
            "79/79 [==============================] - 0s 931us/sample - loss: 1.9112 - acc: 0.1646 - val_loss: 1.9882 - val_acc: 0.0000e+00\n",
            "Epoch 288/1000\n",
            "79/79 [==============================] - 0s 935us/sample - loss: 1.9112 - acc: 0.1646 - val_loss: 1.9914 - val_acc: 0.0000e+00\n",
            "Epoch 289/1000\n",
            "79/79 [==============================] - 0s 997us/sample - loss: 1.9101 - acc: 0.1646 - val_loss: 1.9920 - val_acc: 0.0000e+00\n",
            "Epoch 290/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9110 - acc: 0.1646 - val_loss: 1.9897 - val_acc: 0.0000e+00\n",
            "Epoch 291/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9099 - acc: 0.1646 - val_loss: 1.9888 - val_acc: 0.0000e+00\n",
            "Epoch 292/1000\n",
            "79/79 [==============================] - 0s 986us/sample - loss: 1.9101 - acc: 0.1646 - val_loss: 1.9899 - val_acc: 0.0000e+00\n",
            "Epoch 293/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9089 - acc: 0.1646 - val_loss: 1.9893 - val_acc: 0.0000e+00\n",
            "Epoch 294/1000\n",
            "79/79 [==============================] - 0s 955us/sample - loss: 1.9085 - acc: 0.1646 - val_loss: 1.9868 - val_acc: 0.0000e+00\n",
            "Epoch 295/1000\n",
            "79/79 [==============================] - 0s 925us/sample - loss: 1.9092 - acc: 0.1646 - val_loss: 1.9866 - val_acc: 0.0000e+00\n",
            "Epoch 296/1000\n",
            "79/79 [==============================] - 0s 963us/sample - loss: 1.9097 - acc: 0.1646 - val_loss: 1.9855 - val_acc: 0.0000e+00\n",
            "Epoch 297/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9082 - acc: 0.1646 - val_loss: 1.9833 - val_acc: 0.0000e+00\n",
            "Epoch 298/1000\n",
            "79/79 [==============================] - 0s 971us/sample - loss: 1.9083 - acc: 0.2025 - val_loss: 1.9835 - val_acc: 0.0000e+00\n",
            "Epoch 299/1000\n",
            "79/79 [==============================] - 0s 962us/sample - loss: 1.9080 - acc: 0.1646 - val_loss: 1.9830 - val_acc: 0.0000e+00\n",
            "Epoch 300/1000\n",
            "79/79 [==============================] - 0s 921us/sample - loss: 1.9077 - acc: 0.1646 - val_loss: 1.9823 - val_acc: 0.0000e+00\n",
            "Epoch 301/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9082 - acc: 0.1646 - val_loss: 1.9849 - val_acc: 0.0000e+00\n",
            "Epoch 302/1000\n",
            "79/79 [==============================] - 0s 938us/sample - loss: 1.9073 - acc: 0.1646 - val_loss: 1.9827 - val_acc: 0.0000e+00\n",
            "Epoch 303/1000\n",
            "79/79 [==============================] - 0s 991us/sample - loss: 1.9079 - acc: 0.1646 - val_loss: 1.9828 - val_acc: 0.0000e+00\n",
            "Epoch 304/1000\n",
            "79/79 [==============================] - 0s 951us/sample - loss: 1.9064 - acc: 0.1646 - val_loss: 1.9836 - val_acc: 0.0000e+00\n",
            "Epoch 305/1000\n",
            "79/79 [==============================] - 0s 943us/sample - loss: 1.9059 - acc: 0.1899 - val_loss: 1.9851 - val_acc: 0.0000e+00\n",
            "Epoch 306/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9064 - acc: 0.1646 - val_loss: 1.9835 - val_acc: 0.0000e+00\n",
            "Epoch 307/1000\n",
            "79/79 [==============================] - 0s 970us/sample - loss: 1.9057 - acc: 0.1646 - val_loss: 1.9867 - val_acc: 0.0000e+00\n",
            "Epoch 308/1000\n",
            "79/79 [==============================] - 0s 964us/sample - loss: 1.9057 - acc: 0.1646 - val_loss: 1.9865 - val_acc: 0.0000e+00\n",
            "Epoch 309/1000\n",
            "79/79 [==============================] - 0s 974us/sample - loss: 1.9046 - acc: 0.1646 - val_loss: 1.9863 - val_acc: 0.0000e+00\n",
            "Epoch 310/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9069 - acc: 0.1646 - val_loss: 1.9869 - val_acc: 0.0000e+00\n",
            "Epoch 311/1000\n",
            "79/79 [==============================] - 0s 931us/sample - loss: 1.9044 - acc: 0.1646 - val_loss: 1.9855 - val_acc: 0.0000e+00\n",
            "Epoch 312/1000\n",
            "79/79 [==============================] - 0s 966us/sample - loss: 1.9039 - acc: 0.2025 - val_loss: 1.9887 - val_acc: 0.0000e+00\n",
            "Epoch 313/1000\n",
            "79/79 [==============================] - 0s 962us/sample - loss: 1.9049 - acc: 0.1646 - val_loss: 1.9869 - val_acc: 0.0000e+00\n",
            "Epoch 314/1000\n",
            "79/79 [==============================] - 0s 983us/sample - loss: 1.9054 - acc: 0.1646 - val_loss: 1.9875 - val_acc: 0.0000e+00\n",
            "Epoch 315/1000\n",
            "79/79 [==============================] - 0s 990us/sample - loss: 1.9032 - acc: 0.1646 - val_loss: 1.9850 - val_acc: 0.0000e+00\n",
            "Epoch 316/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9030 - acc: 0.1646 - val_loss: 1.9845 - val_acc: 0.0000e+00\n",
            "Epoch 317/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9031 - acc: 0.1646 - val_loss: 1.9839 - val_acc: 0.0000e+00\n",
            "Epoch 318/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9034 - acc: 0.1899 - val_loss: 1.9859 - val_acc: 0.0000e+00\n",
            "Epoch 319/1000\n",
            "79/79 [==============================] - 0s 999us/sample - loss: 1.9030 - acc: 0.1646 - val_loss: 1.9852 - val_acc: 0.0000e+00\n",
            "Epoch 320/1000\n",
            "79/79 [==============================] - 0s 974us/sample - loss: 1.9041 - acc: 0.1646 - val_loss: 1.9844 - val_acc: 0.0000e+00\n",
            "Epoch 321/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9023 - acc: 0.1646 - val_loss: 1.9850 - val_acc: 0.0000e+00\n",
            "Epoch 322/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9026 - acc: 0.1646 - val_loss: 1.9836 - val_acc: 0.0000e+00\n",
            "Epoch 323/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9008 - acc: 0.1646 - val_loss: 1.9819 - val_acc: 0.0000e+00\n",
            "Epoch 324/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9010 - acc: 0.1646 - val_loss: 1.9814 - val_acc: 0.0000e+00\n",
            "Epoch 325/1000\n",
            "79/79 [==============================] - 0s 979us/sample - loss: 1.9001 - acc: 0.1646 - val_loss: 1.9800 - val_acc: 0.0000e+00\n",
            "Epoch 326/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9004 - acc: 0.1646 - val_loss: 1.9792 - val_acc: 0.0000e+00\n",
            "Epoch 327/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.9003 - acc: 0.1772 - val_loss: 1.9780 - val_acc: 0.0000e+00\n",
            "Epoch 328/1000\n",
            "79/79 [==============================] - 0s 972us/sample - loss: 1.8995 - acc: 0.2278 - val_loss: 1.9792 - val_acc: 0.0000e+00\n",
            "Epoch 329/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8997 - acc: 0.1646 - val_loss: 1.9799 - val_acc: 0.0000e+00\n",
            "Epoch 330/1000\n",
            "79/79 [==============================] - 0s 986us/sample - loss: 1.8993 - acc: 0.1646 - val_loss: 1.9762 - val_acc: 0.0000e+00\n",
            "Epoch 331/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8991 - acc: 0.2025 - val_loss: 1.9768 - val_acc: 0.0000e+00\n",
            "Epoch 332/1000\n",
            "79/79 [==============================] - 0s 999us/sample - loss: 1.8992 - acc: 0.1772 - val_loss: 1.9777 - val_acc: 0.0000e+00\n",
            "Epoch 333/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8971 - acc: 0.1646 - val_loss: 1.9773 - val_acc: 0.0000e+00\n",
            "Epoch 334/1000\n",
            "79/79 [==============================] - 0s 975us/sample - loss: 1.8983 - acc: 0.1646 - val_loss: 1.9749 - val_acc: 0.0000e+00\n",
            "Epoch 335/1000\n",
            "79/79 [==============================] - 0s 953us/sample - loss: 1.8970 - acc: 0.1899 - val_loss: 1.9757 - val_acc: 0.0000e+00\n",
            "Epoch 336/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8969 - acc: 0.1646 - val_loss: 1.9752 - val_acc: 0.0000e+00\n",
            "Epoch 337/1000\n",
            "79/79 [==============================] - 0s 971us/sample - loss: 1.8968 - acc: 0.2405 - val_loss: 1.9765 - val_acc: 0.0000e+00\n",
            "Epoch 338/1000\n",
            "79/79 [==============================] - 0s 949us/sample - loss: 1.8963 - acc: 0.2025 - val_loss: 1.9761 - val_acc: 0.0000e+00\n",
            "Epoch 339/1000\n",
            "79/79 [==============================] - 0s 1000us/sample - loss: 1.8969 - acc: 0.1772 - val_loss: 1.9747 - val_acc: 0.0000e+00\n",
            "Epoch 340/1000\n",
            "79/79 [==============================] - 0s 983us/sample - loss: 1.8953 - acc: 0.1772 - val_loss: 1.9721 - val_acc: 0.0000e+00\n",
            "Epoch 341/1000\n",
            "79/79 [==============================] - 0s 956us/sample - loss: 1.8944 - acc: 0.2405 - val_loss: 1.9725 - val_acc: 0.0000e+00\n",
            "Epoch 342/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8965 - acc: 0.1646 - val_loss: 1.9689 - val_acc: 0.0000e+00\n",
            "Epoch 343/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8946 - acc: 0.1772 - val_loss: 1.9685 - val_acc: 0.0000e+00\n",
            "Epoch 344/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8941 - acc: 0.1772 - val_loss: 1.9693 - val_acc: 0.0556\n",
            "Epoch 345/1000\n",
            "79/79 [==============================] - 0s 935us/sample - loss: 1.8930 - acc: 0.1772 - val_loss: 1.9674 - val_acc: 0.0556\n",
            "Epoch 346/1000\n",
            "79/79 [==============================] - 0s 976us/sample - loss: 1.8924 - acc: 0.2658 - val_loss: 1.9688 - val_acc: 0.0556\n",
            "Epoch 347/1000\n",
            "79/79 [==============================] - 0s 948us/sample - loss: 1.8928 - acc: 0.2278 - val_loss: 1.9713 - val_acc: 0.0000e+00\n",
            "Epoch 348/1000\n",
            "79/79 [==============================] - 0s 980us/sample - loss: 1.8920 - acc: 0.1646 - val_loss: 1.9721 - val_acc: 0.0000e+00\n",
            "Epoch 349/1000\n",
            "79/79 [==============================] - 0s 980us/sample - loss: 1.8913 - acc: 0.2658 - val_loss: 1.9711 - val_acc: 0.0000e+00\n",
            "Epoch 350/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8911 - acc: 0.1646 - val_loss: 1.9665 - val_acc: 0.0556\n",
            "Epoch 351/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8908 - acc: 0.2152 - val_loss: 1.9670 - val_acc: 0.0556\n",
            "Epoch 352/1000\n",
            "79/79 [==============================] - 0s 953us/sample - loss: 1.8902 - acc: 0.2278 - val_loss: 1.9678 - val_acc: 0.0556\n",
            "Epoch 353/1000\n",
            "79/79 [==============================] - 0s 968us/sample - loss: 1.8906 - acc: 0.2025 - val_loss: 1.9663 - val_acc: 0.0556\n",
            "Epoch 354/1000\n",
            "79/79 [==============================] - 0s 938us/sample - loss: 1.8892 - acc: 0.2405 - val_loss: 1.9659 - val_acc: 0.0556\n",
            "Epoch 355/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8898 - acc: 0.2025 - val_loss: 1.9667 - val_acc: 0.0556\n",
            "Epoch 356/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8880 - acc: 0.3038 - val_loss: 1.9690 - val_acc: 0.0000e+00\n",
            "Epoch 357/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8881 - acc: 0.2152 - val_loss: 1.9689 - val_acc: 0.0000e+00\n",
            "Epoch 358/1000\n",
            "79/79 [==============================] - 0s 961us/sample - loss: 1.8876 - acc: 0.1772 - val_loss: 1.9664 - val_acc: 0.0556\n",
            "Epoch 359/1000\n",
            "79/79 [==============================] - 0s 941us/sample - loss: 1.8873 - acc: 0.2025 - val_loss: 1.9661 - val_acc: 0.0556\n",
            "Epoch 360/1000\n",
            "79/79 [==============================] - 0s 978us/sample - loss: 1.8866 - acc: 0.2278 - val_loss: 1.9661 - val_acc: 0.0556\n",
            "Epoch 361/1000\n",
            "79/79 [==============================] - 0s 968us/sample - loss: 1.8878 - acc: 0.2152 - val_loss: 1.9652 - val_acc: 0.0556\n",
            "Epoch 362/1000\n",
            "79/79 [==============================] - 0s 935us/sample - loss: 1.8866 - acc: 0.1646 - val_loss: 1.9630 - val_acc: 0.0556\n",
            "Epoch 363/1000\n",
            "79/79 [==============================] - 0s 926us/sample - loss: 1.8846 - acc: 0.1646 - val_loss: 1.9597 - val_acc: 0.1111\n",
            "Epoch 364/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8851 - acc: 0.2025 - val_loss: 1.9597 - val_acc: 0.0556\n",
            "Epoch 365/1000\n",
            "79/79 [==============================] - 0s 964us/sample - loss: 1.8852 - acc: 0.2785 - val_loss: 1.9585 - val_acc: 0.1111\n",
            "Epoch 366/1000\n",
            "79/79 [==============================] - 0s 970us/sample - loss: 1.8827 - acc: 0.3544 - val_loss: 1.9602 - val_acc: 0.0556\n",
            "Epoch 367/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8829 - acc: 0.2405 - val_loss: 1.9594 - val_acc: 0.1111\n",
            "Epoch 368/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8823 - acc: 0.3038 - val_loss: 1.9599 - val_acc: 0.0556\n",
            "Epoch 369/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8811 - acc: 0.2658 - val_loss: 1.9605 - val_acc: 0.0556\n",
            "Epoch 370/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8812 - acc: 0.2658 - val_loss: 1.9591 - val_acc: 0.0556\n",
            "Epoch 371/1000\n",
            "79/79 [==============================] - 0s 990us/sample - loss: 1.8801 - acc: 0.3291 - val_loss: 1.9612 - val_acc: 0.0556\n",
            "Epoch 372/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8809 - acc: 0.2911 - val_loss: 1.9612 - val_acc: 0.0556\n",
            "Epoch 373/1000\n",
            "79/79 [==============================] - 0s 985us/sample - loss: 1.8805 - acc: 0.2911 - val_loss: 1.9594 - val_acc: 0.0556\n",
            "Epoch 374/1000\n",
            "79/79 [==============================] - 0s 997us/sample - loss: 1.8801 - acc: 0.1899 - val_loss: 1.9568 - val_acc: 0.1111\n",
            "Epoch 375/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8789 - acc: 0.2785 - val_loss: 1.9572 - val_acc: 0.1111\n",
            "Epoch 376/1000\n",
            "79/79 [==============================] - 0s 974us/sample - loss: 1.8790 - acc: 0.3418 - val_loss: 1.9552 - val_acc: 0.1667\n",
            "Epoch 377/1000\n",
            "79/79 [==============================] - 0s 936us/sample - loss: 1.8760 - acc: 0.2911 - val_loss: 1.9532 - val_acc: 0.1667\n",
            "Epoch 378/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8767 - acc: 0.3924 - val_loss: 1.9537 - val_acc: 0.1667\n",
            "Epoch 379/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8768 - acc: 0.2278 - val_loss: 1.9481 - val_acc: 0.1667\n",
            "Epoch 380/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8763 - acc: 0.4430 - val_loss: 1.9499 - val_acc: 0.1667\n",
            "Epoch 381/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8734 - acc: 0.2405 - val_loss: 1.9475 - val_acc: 0.1667\n",
            "Epoch 382/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8748 - acc: 0.3038 - val_loss: 1.9453 - val_acc: 0.1667\n",
            "Epoch 383/1000\n",
            "79/79 [==============================] - 0s 986us/sample - loss: 1.8724 - acc: 0.3797 - val_loss: 1.9474 - val_acc: 0.1667\n",
            "Epoch 384/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8713 - acc: 0.3038 - val_loss: 1.9468 - val_acc: 0.1667\n",
            "Epoch 385/1000\n",
            "79/79 [==============================] - 0s 948us/sample - loss: 1.8717 - acc: 0.3038 - val_loss: 1.9485 - val_acc: 0.1667\n",
            "Epoch 386/1000\n",
            "79/79 [==============================] - 0s 987us/sample - loss: 1.8704 - acc: 0.4430 - val_loss: 1.9520 - val_acc: 0.1667\n",
            "Epoch 387/1000\n",
            "79/79 [==============================] - 0s 948us/sample - loss: 1.8702 - acc: 0.3924 - val_loss: 1.9477 - val_acc: 0.1667\n",
            "Epoch 388/1000\n",
            "79/79 [==============================] - 0s 991us/sample - loss: 1.8684 - acc: 0.3165 - val_loss: 1.9459 - val_acc: 0.1667\n",
            "Epoch 389/1000\n",
            "79/79 [==============================] - 0s 957us/sample - loss: 1.8678 - acc: 0.3797 - val_loss: 1.9468 - val_acc: 0.1667\n",
            "Epoch 390/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8679 - acc: 0.4051 - val_loss: 1.9454 - val_acc: 0.1667\n",
            "Epoch 391/1000\n",
            "79/79 [==============================] - 0s 915us/sample - loss: 1.8676 - acc: 0.2532 - val_loss: 1.9408 - val_acc: 0.2222\n",
            "Epoch 392/1000\n",
            "79/79 [==============================] - 0s 917us/sample - loss: 1.8654 - acc: 0.3671 - val_loss: 1.9434 - val_acc: 0.1667\n",
            "Epoch 393/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8652 - acc: 0.3924 - val_loss: 1.9458 - val_acc: 0.1667\n",
            "Epoch 394/1000\n",
            "79/79 [==============================] - 0s 973us/sample - loss: 1.8644 - acc: 0.4304 - val_loss: 1.9439 - val_acc: 0.1667\n",
            "Epoch 395/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8640 - acc: 0.4304 - val_loss: 1.9460 - val_acc: 0.1667\n",
            "Epoch 396/1000\n",
            "79/79 [==============================] - 0s 988us/sample - loss: 1.8629 - acc: 0.3418 - val_loss: 1.9451 - val_acc: 0.1667\n",
            "Epoch 397/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8610 - acc: 0.3544 - val_loss: 1.9388 - val_acc: 0.2222\n",
            "Epoch 398/1000\n",
            "79/79 [==============================] - 0s 933us/sample - loss: 1.8606 - acc: 0.2785 - val_loss: 1.9322 - val_acc: 0.2222\n",
            "Epoch 399/1000\n",
            "79/79 [==============================] - 0s 983us/sample - loss: 1.8605 - acc: 0.5823 - val_loss: 1.9392 - val_acc: 0.2222\n",
            "Epoch 400/1000\n",
            "79/79 [==============================] - 0s 979us/sample - loss: 1.8577 - acc: 0.4177 - val_loss: 1.9383 - val_acc: 0.2222\n",
            "Epoch 401/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8559 - acc: 0.4557 - val_loss: 1.9397 - val_acc: 0.2222\n",
            "Epoch 402/1000\n",
            "79/79 [==============================] - 0s 987us/sample - loss: 1.8542 - acc: 0.4051 - val_loss: 1.9381 - val_acc: 0.2222\n",
            "Epoch 403/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8536 - acc: 0.3418 - val_loss: 1.9342 - val_acc: 0.2222\n",
            "Epoch 404/1000\n",
            "79/79 [==============================] - 0s 952us/sample - loss: 1.8538 - acc: 0.3291 - val_loss: 1.9321 - val_acc: 0.4444\n",
            "Epoch 405/1000\n",
            "79/79 [==============================] - 0s 917us/sample - loss: 1.8527 - acc: 0.4684 - val_loss: 1.9362 - val_acc: 0.4444\n",
            "Epoch 406/1000\n",
            "79/79 [==============================] - 0s 996us/sample - loss: 1.8507 - acc: 0.5316 - val_loss: 1.9336 - val_acc: 0.2222\n",
            "Epoch 407/1000\n",
            "79/79 [==============================] - 0s 985us/sample - loss: 1.8495 - acc: 0.4810 - val_loss: 1.9371 - val_acc: 0.2222\n",
            "Epoch 408/1000\n",
            "79/79 [==============================] - 0s 928us/sample - loss: 1.8497 - acc: 0.4304 - val_loss: 1.9327 - val_acc: 0.2222\n",
            "Epoch 409/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8472 - acc: 0.4304 - val_loss: 1.9322 - val_acc: 0.2222\n",
            "Epoch 410/1000\n",
            "79/79 [==============================] - 0s 935us/sample - loss: 1.8443 - acc: 0.4557 - val_loss: 1.9292 - val_acc: 0.2222\n",
            "Epoch 411/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8453 - acc: 0.5316 - val_loss: 1.9328 - val_acc: 0.2778\n",
            "Epoch 412/1000\n",
            "79/79 [==============================] - 0s 929us/sample - loss: 1.8432 - acc: 0.3671 - val_loss: 1.9274 - val_acc: 0.2778\n",
            "Epoch 413/1000\n",
            "79/79 [==============================] - 0s 969us/sample - loss: 1.8405 - acc: 0.4430 - val_loss: 1.9223 - val_acc: 0.2778\n",
            "Epoch 414/1000\n",
            "79/79 [==============================] - 0s 928us/sample - loss: 1.8391 - acc: 0.4557 - val_loss: 1.9245 - val_acc: 0.4444\n",
            "Epoch 415/1000\n",
            "79/79 [==============================] - 0s 947us/sample - loss: 1.8372 - acc: 0.4177 - val_loss: 1.9224 - val_acc: 0.4444\n",
            "Epoch 416/1000\n",
            "79/79 [==============================] - 0s 965us/sample - loss: 1.8360 - acc: 0.5190 - val_loss: 1.9237 - val_acc: 0.4444\n",
            "Epoch 417/1000\n",
            "79/79 [==============================] - 0s 994us/sample - loss: 1.8349 - acc: 0.4684 - val_loss: 1.9243 - val_acc: 0.4444\n",
            "Epoch 418/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8333 - acc: 0.5949 - val_loss: 1.9230 - val_acc: 0.4444\n",
            "Epoch 419/1000\n",
            "79/79 [==============================] - 0s 933us/sample - loss: 1.8307 - acc: 0.4684 - val_loss: 1.9186 - val_acc: 0.4444\n",
            "Epoch 420/1000\n",
            "79/79 [==============================] - 0s 976us/sample - loss: 1.8287 - acc: 0.6076 - val_loss: 1.9140 - val_acc: 0.2778\n",
            "Epoch 421/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8268 - acc: 0.4937 - val_loss: 1.9151 - val_acc: 0.4444\n",
            "Epoch 422/1000\n",
            "79/79 [==============================] - 0s 966us/sample - loss: 1.8263 - acc: 0.6203 - val_loss: 1.9142 - val_acc: 0.2778\n",
            "Epoch 423/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8225 - acc: 0.5190 - val_loss: 1.9122 - val_acc: 0.2778\n",
            "Epoch 424/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8204 - acc: 0.4684 - val_loss: 1.9073 - val_acc: 0.2778\n",
            "Epoch 425/1000\n",
            "79/79 [==============================] - 0s 952us/sample - loss: 1.8192 - acc: 0.4684 - val_loss: 1.9008 - val_acc: 0.2778\n",
            "Epoch 426/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8163 - acc: 0.5063 - val_loss: 1.9015 - val_acc: 0.2778\n",
            "Epoch 427/1000\n",
            "79/79 [==============================] - 0s 955us/sample - loss: 1.8152 - acc: 0.5696 - val_loss: 1.9028 - val_acc: 0.4444\n",
            "Epoch 428/1000\n",
            "79/79 [==============================] - 0s 943us/sample - loss: 1.8111 - acc: 0.6076 - val_loss: 1.8955 - val_acc: 0.2778\n",
            "Epoch 429/1000\n",
            "79/79 [==============================] - 0s 971us/sample - loss: 1.8076 - acc: 0.4810 - val_loss: 1.8854 - val_acc: 0.4444\n",
            "Epoch 430/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8060 - acc: 0.6203 - val_loss: 1.8833 - val_acc: 0.2778\n",
            "Epoch 431/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.8024 - acc: 0.5570 - val_loss: 1.8829 - val_acc: 0.2778\n",
            "Epoch 432/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.7984 - acc: 0.5570 - val_loss: 1.8880 - val_acc: 0.4444\n",
            "Epoch 433/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.7959 - acc: 0.5696 - val_loss: 1.8920 - val_acc: 0.2778\n",
            "Epoch 434/1000\n",
            "79/79 [==============================] - 0s 952us/sample - loss: 1.7939 - acc: 0.5316 - val_loss: 1.8919 - val_acc: 0.4444\n",
            "Epoch 435/1000\n",
            "79/79 [==============================] - 0s 956us/sample - loss: 1.7906 - acc: 0.5823 - val_loss: 1.8821 - val_acc: 0.2778\n",
            "Epoch 436/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.7856 - acc: 0.6203 - val_loss: 1.8788 - val_acc: 0.2778\n",
            "Epoch 437/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.7822 - acc: 0.5949 - val_loss: 1.8739 - val_acc: 0.2778\n",
            "Epoch 438/1000\n",
            "79/79 [==============================] - 0s 941us/sample - loss: 1.7771 - acc: 0.5823 - val_loss: 1.8766 - val_acc: 0.2778\n",
            "Epoch 439/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.7736 - acc: 0.5949 - val_loss: 1.8719 - val_acc: 0.2778\n",
            "Epoch 440/1000\n",
            "79/79 [==============================] - 0s 997us/sample - loss: 1.7697 - acc: 0.5443 - val_loss: 1.8633 - val_acc: 0.2778\n",
            "Epoch 441/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.7640 - acc: 0.6076 - val_loss: 1.8649 - val_acc: 0.2778\n",
            "Epoch 442/1000\n",
            "79/79 [==============================] - 0s 996us/sample - loss: 1.7594 - acc: 0.5696 - val_loss: 1.8717 - val_acc: 0.2778\n",
            "Epoch 443/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.7542 - acc: 0.5823 - val_loss: 1.8595 - val_acc: 0.2778\n",
            "Epoch 444/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.7482 - acc: 0.5823 - val_loss: 1.8528 - val_acc: 0.2778\n",
            "Epoch 445/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.7451 - acc: 0.6203 - val_loss: 1.8457 - val_acc: 0.2778\n",
            "Epoch 446/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.7370 - acc: 0.5443 - val_loss: 1.8353 - val_acc: 0.2778\n",
            "Epoch 447/1000\n",
            "79/79 [==============================] - 0s 900us/sample - loss: 1.7310 - acc: 0.5949 - val_loss: 1.8261 - val_acc: 0.2778\n",
            "Epoch 448/1000\n",
            "79/79 [==============================] - 0s 969us/sample - loss: 1.7259 - acc: 0.6456 - val_loss: 1.8323 - val_acc: 0.4444\n",
            "Epoch 449/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.7156 - acc: 0.6203 - val_loss: 1.8215 - val_acc: 0.2778\n",
            "Epoch 450/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.7073 - acc: 0.5570 - val_loss: 1.8202 - val_acc: 0.4444\n",
            "Epoch 451/1000\n",
            "79/79 [==============================] - 0s 978us/sample - loss: 1.7003 - acc: 0.6329 - val_loss: 1.8074 - val_acc: 0.2778\n",
            "Epoch 452/1000\n",
            "79/79 [==============================] - 0s 968us/sample - loss: 1.6942 - acc: 0.6203 - val_loss: 1.8081 - val_acc: 0.2778\n",
            "Epoch 453/1000\n",
            "79/79 [==============================] - 0s 992us/sample - loss: 1.6830 - acc: 0.4937 - val_loss: 1.8192 - val_acc: 0.2778\n",
            "Epoch 454/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.6728 - acc: 0.5696 - val_loss: 1.8239 - val_acc: 0.5000\n",
            "Epoch 455/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.6677 - acc: 0.5570 - val_loss: 1.7996 - val_acc: 0.2778\n",
            "Epoch 456/1000\n",
            "79/79 [==============================] - 0s 999us/sample - loss: 1.6582 - acc: 0.6203 - val_loss: 1.8002 - val_acc: 0.2778\n",
            "Epoch 457/1000\n",
            "79/79 [==============================] - 0s 967us/sample - loss: 1.6454 - acc: 0.5949 - val_loss: 1.7887 - val_acc: 0.2778\n",
            "Epoch 458/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.6358 - acc: 0.5823 - val_loss: 1.7889 - val_acc: 0.3889\n",
            "Epoch 459/1000\n",
            "79/79 [==============================] - 0s 920us/sample - loss: 1.6255 - acc: 0.5443 - val_loss: 1.7785 - val_acc: 0.2778\n",
            "Epoch 460/1000\n",
            "79/79 [==============================] - 0s 904us/sample - loss: 1.6170 - acc: 0.5316 - val_loss: 1.7480 - val_acc: 0.2778\n",
            "Epoch 461/1000\n",
            "79/79 [==============================] - 0s 943us/sample - loss: 1.6001 - acc: 0.6076 - val_loss: 1.7520 - val_acc: 0.2778\n",
            "Epoch 462/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.5938 - acc: 0.5570 - val_loss: 1.7232 - val_acc: 0.2778\n",
            "Epoch 463/1000\n",
            "79/79 [==============================] - 0s 985us/sample - loss: 1.5792 - acc: 0.6456 - val_loss: 1.7269 - val_acc: 0.2778\n",
            "Epoch 464/1000\n",
            "79/79 [==============================] - 0s 970us/sample - loss: 1.5672 - acc: 0.5823 - val_loss: 1.7437 - val_acc: 0.2222\n",
            "Epoch 465/1000\n",
            "79/79 [==============================] - 0s 984us/sample - loss: 1.5559 - acc: 0.5696 - val_loss: 1.7238 - val_acc: 0.2778\n",
            "Epoch 466/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.5394 - acc: 0.5316 - val_loss: 1.7045 - val_acc: 0.4444\n",
            "Epoch 467/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.5328 - acc: 0.5570 - val_loss: 1.7102 - val_acc: 0.4444\n",
            "Epoch 468/1000\n",
            "79/79 [==============================] - 0s 998us/sample - loss: 1.5216 - acc: 0.5823 - val_loss: 1.7020 - val_acc: 0.3889\n",
            "Epoch 469/1000\n",
            "79/79 [==============================] - 0s 980us/sample - loss: 1.5061 - acc: 0.5823 - val_loss: 1.7002 - val_acc: 0.4444\n",
            "Epoch 470/1000\n",
            "79/79 [==============================] - 0s 996us/sample - loss: 1.5003 - acc: 0.5570 - val_loss: 1.6794 - val_acc: 0.2222\n",
            "Epoch 471/1000\n",
            "79/79 [==============================] - 0s 961us/sample - loss: 1.4834 - acc: 0.6329 - val_loss: 1.6723 - val_acc: 0.2222\n",
            "Epoch 472/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.4790 - acc: 0.5316 - val_loss: 1.6635 - val_acc: 0.3889\n",
            "Epoch 473/1000\n",
            "79/79 [==============================] - 0s 940us/sample - loss: 1.4614 - acc: 0.6203 - val_loss: 1.6517 - val_acc: 0.2222\n",
            "Epoch 474/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.4546 - acc: 0.5696 - val_loss: 1.6253 - val_acc: 0.2778\n",
            "Epoch 475/1000\n",
            "79/79 [==============================] - 0s 997us/sample - loss: 1.4434 - acc: 0.6076 - val_loss: 1.6134 - val_acc: 0.2778\n",
            "Epoch 476/1000\n",
            "79/79 [==============================] - 0s 960us/sample - loss: 1.4318 - acc: 0.6076 - val_loss: 1.6141 - val_acc: 0.4444\n",
            "Epoch 477/1000\n",
            "79/79 [==============================] - 0s 953us/sample - loss: 1.4192 - acc: 0.6076 - val_loss: 1.6077 - val_acc: 0.4444\n",
            "Epoch 478/1000\n",
            "79/79 [==============================] - 0s 983us/sample - loss: 1.4103 - acc: 0.6456 - val_loss: 1.6081 - val_acc: 0.5000\n",
            "Epoch 479/1000\n",
            "79/79 [==============================] - 0s 951us/sample - loss: 1.4002 - acc: 0.6456 - val_loss: 1.5882 - val_acc: 0.2778\n",
            "Epoch 480/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.3967 - acc: 0.6329 - val_loss: 1.5832 - val_acc: 0.2778\n",
            "Epoch 481/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.3883 - acc: 0.5823 - val_loss: 1.5721 - val_acc: 0.4444\n",
            "Epoch 482/1000\n",
            "79/79 [==============================] - 0s 930us/sample - loss: 1.3710 - acc: 0.7089 - val_loss: 1.5979 - val_acc: 0.3889\n",
            "Epoch 483/1000\n",
            "79/79 [==============================] - 0s 974us/sample - loss: 1.3682 - acc: 0.5823 - val_loss: 1.5761 - val_acc: 0.6111\n",
            "Epoch 484/1000\n",
            "79/79 [==============================] - 0s 946us/sample - loss: 1.3534 - acc: 0.6835 - val_loss: 1.5558 - val_acc: 0.4444\n",
            "Epoch 485/1000\n",
            "79/79 [==============================] - 0s 994us/sample - loss: 1.3448 - acc: 0.7215 - val_loss: 1.5363 - val_acc: 0.5000\n",
            "Epoch 486/1000\n",
            "79/79 [==============================] - 0s 1000us/sample - loss: 1.3398 - acc: 0.6962 - val_loss: 1.5508 - val_acc: 0.5000\n",
            "Epoch 487/1000\n",
            "79/79 [==============================] - 0s 945us/sample - loss: 1.3280 - acc: 0.7215 - val_loss: 1.5253 - val_acc: 0.5000\n",
            "Epoch 488/1000\n",
            "79/79 [==============================] - 0s 958us/sample - loss: 1.3248 - acc: 0.6456 - val_loss: 1.5001 - val_acc: 0.5000\n",
            "Epoch 489/1000\n",
            "79/79 [==============================] - 0s 979us/sample - loss: 1.3102 - acc: 0.6962 - val_loss: 1.5446 - val_acc: 0.2778\n",
            "Epoch 490/1000\n",
            "79/79 [==============================] - 0s 976us/sample - loss: 1.3087 - acc: 0.7342 - val_loss: 1.5192 - val_acc: 0.5000\n",
            "Epoch 491/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.2943 - acc: 0.7215 - val_loss: 1.4743 - val_acc: 0.5000\n",
            "Epoch 492/1000\n",
            "79/79 [==============================] - 0s 970us/sample - loss: 1.2900 - acc: 0.7468 - val_loss: 1.5020 - val_acc: 0.6111\n",
            "Epoch 493/1000\n",
            "79/79 [==============================] - 0s 914us/sample - loss: 1.2786 - acc: 0.7089 - val_loss: 1.5221 - val_acc: 0.6111\n",
            "Epoch 494/1000\n",
            "79/79 [==============================] - 0s 999us/sample - loss: 1.2728 - acc: 0.7215 - val_loss: 1.4733 - val_acc: 0.6667\n",
            "Epoch 495/1000\n",
            "79/79 [==============================] - 0s 963us/sample - loss: 1.2588 - acc: 0.7468 - val_loss: 1.4552 - val_acc: 0.6111\n",
            "Epoch 496/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.2518 - acc: 0.7595 - val_loss: 1.4566 - val_acc: 0.6667\n",
            "Epoch 497/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.2424 - acc: 0.7595 - val_loss: 1.4140 - val_acc: 0.5000\n",
            "Epoch 498/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.2354 - acc: 0.7342 - val_loss: 1.4270 - val_acc: 0.5556\n",
            "Epoch 499/1000\n",
            "79/79 [==============================] - 0s 975us/sample - loss: 1.2260 - acc: 0.6582 - val_loss: 1.3868 - val_acc: 0.6667\n",
            "Epoch 500/1000\n",
            "79/79 [==============================] - 0s 964us/sample - loss: 1.2166 - acc: 0.7468 - val_loss: 1.3958 - val_acc: 0.6111\n",
            "Epoch 501/1000\n",
            "79/79 [==============================] - 0s 997us/sample - loss: 1.2119 - acc: 0.7722 - val_loss: 1.3874 - val_acc: 0.7222\n",
            "Epoch 502/1000\n",
            "79/79 [==============================] - 0s 945us/sample - loss: 1.1982 - acc: 0.7975 - val_loss: 1.3930 - val_acc: 0.6667\n",
            "Epoch 503/1000\n",
            "79/79 [==============================] - 0s 939us/sample - loss: 1.1881 - acc: 0.7595 - val_loss: 1.3531 - val_acc: 0.6111\n",
            "Epoch 504/1000\n",
            "79/79 [==============================] - 0s 999us/sample - loss: 1.1853 - acc: 0.7468 - val_loss: 1.3613 - val_acc: 0.7222\n",
            "Epoch 505/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.1741 - acc: 0.7595 - val_loss: 1.3280 - val_acc: 0.6667\n",
            "Epoch 506/1000\n",
            "79/79 [==============================] - 0s 993us/sample - loss: 1.1605 - acc: 0.7468 - val_loss: 1.3843 - val_acc: 0.5556\n",
            "Epoch 507/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.1590 - acc: 0.7595 - val_loss: 1.3307 - val_acc: 0.7222\n",
            "Epoch 508/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.1356 - acc: 0.7848 - val_loss: 1.3439 - val_acc: 0.7222\n",
            "Epoch 509/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.1351 - acc: 0.7468 - val_loss: 1.3025 - val_acc: 0.7222\n",
            "Epoch 510/1000\n",
            "79/79 [==============================] - 0s 971us/sample - loss: 1.1270 - acc: 0.8101 - val_loss: 1.2996 - val_acc: 0.7222\n",
            "Epoch 511/1000\n",
            "79/79 [==============================] - 0s 993us/sample - loss: 1.1081 - acc: 0.7848 - val_loss: 1.2928 - val_acc: 0.7222\n",
            "Epoch 512/1000\n",
            "79/79 [==============================] - 0s 994us/sample - loss: 1.1035 - acc: 0.8101 - val_loss: 1.2572 - val_acc: 0.7222\n",
            "Epoch 513/1000\n",
            "79/79 [==============================] - 0s 947us/sample - loss: 1.0937 - acc: 0.7848 - val_loss: 1.2573 - val_acc: 0.7222\n",
            "Epoch 514/1000\n",
            "79/79 [==============================] - 0s 954us/sample - loss: 1.0858 - acc: 0.7848 - val_loss: 1.2520 - val_acc: 0.7222\n",
            "Epoch 515/1000\n",
            "79/79 [==============================] - 0s 975us/sample - loss: 1.0722 - acc: 0.8228 - val_loss: 1.2313 - val_acc: 0.7222\n",
            "Epoch 516/1000\n",
            "79/79 [==============================] - 0s 962us/sample - loss: 1.0658 - acc: 0.7848 - val_loss: 1.2514 - val_acc: 0.7222\n",
            "Epoch 517/1000\n",
            "79/79 [==============================] - 0s 950us/sample - loss: 1.0569 - acc: 0.8228 - val_loss: 1.2109 - val_acc: 0.6667\n",
            "Epoch 518/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.0392 - acc: 0.8354 - val_loss: 1.2059 - val_acc: 0.6667\n",
            "Epoch 519/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.0269 - acc: 0.7975 - val_loss: 1.2037 - val_acc: 0.7222\n",
            "Epoch 520/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 1.0214 - acc: 0.7848 - val_loss: 1.1882 - val_acc: 0.7222\n",
            "Epoch 521/1000\n",
            "79/79 [==============================] - 0s 960us/sample - loss: 1.0057 - acc: 0.8481 - val_loss: 1.1798 - val_acc: 0.7222\n",
            "Epoch 522/1000\n",
            "79/79 [==============================] - 0s 951us/sample - loss: 0.9990 - acc: 0.8481 - val_loss: 1.1823 - val_acc: 0.7222\n",
            "Epoch 523/1000\n",
            "79/79 [==============================] - 0s 980us/sample - loss: 0.9830 - acc: 0.8608 - val_loss: 1.1435 - val_acc: 0.7222\n",
            "Epoch 524/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.9724 - acc: 0.8354 - val_loss: 1.1706 - val_acc: 0.6667\n",
            "Epoch 525/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.9556 - acc: 0.8734 - val_loss: 1.1412 - val_acc: 0.7222\n",
            "Epoch 526/1000\n",
            "79/79 [==============================] - 0s 973us/sample - loss: 0.9571 - acc: 0.8101 - val_loss: 1.0995 - val_acc: 0.7222\n",
            "Epoch 527/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.9399 - acc: 0.8608 - val_loss: 1.0853 - val_acc: 0.7222\n",
            "Epoch 528/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.9305 - acc: 0.8608 - val_loss: 1.0781 - val_acc: 0.7222\n",
            "Epoch 529/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.9116 - acc: 0.8481 - val_loss: 1.0641 - val_acc: 0.7222\n",
            "Epoch 530/1000\n",
            "79/79 [==============================] - 0s 961us/sample - loss: 0.8948 - acc: 0.8734 - val_loss: 1.0499 - val_acc: 0.7222\n",
            "Epoch 531/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.8843 - acc: 0.8608 - val_loss: 1.0494 - val_acc: 0.7222\n",
            "Epoch 532/1000\n",
            "79/79 [==============================] - 0s 944us/sample - loss: 0.8680 - acc: 0.8734 - val_loss: 1.0305 - val_acc: 0.7222\n",
            "Epoch 533/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.8570 - acc: 0.8734 - val_loss: 1.0171 - val_acc: 0.7222\n",
            "Epoch 534/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.8508 - acc: 0.8608 - val_loss: 1.0010 - val_acc: 0.7222\n",
            "Epoch 535/1000\n",
            "79/79 [==============================] - 0s 975us/sample - loss: 0.8386 - acc: 0.8608 - val_loss: 0.9977 - val_acc: 0.7222\n",
            "Epoch 536/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.8243 - acc: 0.8734 - val_loss: 0.9954 - val_acc: 0.7222\n",
            "Epoch 537/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.8136 - acc: 0.8608 - val_loss: 0.9671 - val_acc: 0.7222\n",
            "Epoch 538/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.8049 - acc: 0.8608 - val_loss: 0.9415 - val_acc: 0.7222\n",
            "Epoch 539/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.7900 - acc: 0.8608 - val_loss: 0.9411 - val_acc: 0.7222\n",
            "Epoch 540/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.7755 - acc: 0.8608 - val_loss: 0.9527 - val_acc: 0.6667\n",
            "Epoch 541/1000\n",
            "79/79 [==============================] - 0s 949us/sample - loss: 0.7688 - acc: 0.8861 - val_loss: 0.9243 - val_acc: 0.7222\n",
            "Epoch 542/1000\n",
            "79/79 [==============================] - 0s 994us/sample - loss: 0.7604 - acc: 0.8608 - val_loss: 0.8956 - val_acc: 0.7222\n",
            "Epoch 543/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.7450 - acc: 0.8608 - val_loss: 0.8903 - val_acc: 0.7222\n",
            "Epoch 544/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.7324 - acc: 0.8481 - val_loss: 0.8825 - val_acc: 0.6667\n",
            "Epoch 545/1000\n",
            "79/79 [==============================] - 0s 952us/sample - loss: 0.7227 - acc: 0.8608 - val_loss: 0.8636 - val_acc: 0.7222\n",
            "Epoch 546/1000\n",
            "79/79 [==============================] - 0s 986us/sample - loss: 0.7150 - acc: 0.8608 - val_loss: 0.8622 - val_acc: 0.7222\n",
            "Epoch 547/1000\n",
            "79/79 [==============================] - 0s 972us/sample - loss: 0.7047 - acc: 0.8734 - val_loss: 0.8586 - val_acc: 0.6667\n",
            "Epoch 548/1000\n",
            "79/79 [==============================] - 0s 955us/sample - loss: 0.6988 - acc: 0.8861 - val_loss: 0.8528 - val_acc: 0.6667\n",
            "Epoch 549/1000\n",
            "79/79 [==============================] - 0s 979us/sample - loss: 0.6821 - acc: 0.8734 - val_loss: 0.8337 - val_acc: 0.7222\n",
            "Epoch 550/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.6741 - acc: 0.8608 - val_loss: 0.8481 - val_acc: 0.6667\n",
            "Epoch 551/1000\n",
            "79/79 [==============================] - 0s 945us/sample - loss: 0.6655 - acc: 0.8861 - val_loss: 0.8101 - val_acc: 0.7222\n",
            "Epoch 552/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.6522 - acc: 0.8734 - val_loss: 0.8162 - val_acc: 0.6667\n",
            "Epoch 553/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.6461 - acc: 0.8861 - val_loss: 0.7940 - val_acc: 0.7222\n",
            "Epoch 554/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.6388 - acc: 0.8734 - val_loss: 0.7882 - val_acc: 0.6667\n",
            "Epoch 555/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.6298 - acc: 0.8734 - val_loss: 0.7753 - val_acc: 0.7222\n",
            "Epoch 556/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.6127 - acc: 0.8861 - val_loss: 0.7634 - val_acc: 0.7222\n",
            "Epoch 557/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.6137 - acc: 0.8734 - val_loss: 0.7802 - val_acc: 0.6667\n",
            "Epoch 558/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.6017 - acc: 0.8861 - val_loss: 0.7453 - val_acc: 0.7222\n",
            "Epoch 559/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.5938 - acc: 0.8734 - val_loss: 0.7656 - val_acc: 0.6667\n",
            "Epoch 560/1000\n",
            "79/79 [==============================] - 0s 999us/sample - loss: 0.5841 - acc: 0.8861 - val_loss: 0.7268 - val_acc: 0.7222\n",
            "Epoch 561/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.5769 - acc: 0.8608 - val_loss: 0.7428 - val_acc: 0.7222\n",
            "Epoch 562/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.5654 - acc: 0.8987 - val_loss: 0.7410 - val_acc: 0.7222\n",
            "Epoch 563/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.5489 - acc: 0.8734 - val_loss: 0.7236 - val_acc: 0.6667\n",
            "Epoch 564/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.5483 - acc: 0.8861 - val_loss: 0.7162 - val_acc: 0.6667\n",
            "Epoch 565/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.5390 - acc: 0.8861 - val_loss: 0.7107 - val_acc: 0.7222\n",
            "Epoch 566/1000\n",
            "79/79 [==============================] - 0s 967us/sample - loss: 0.5412 - acc: 0.9114 - val_loss: 0.7239 - val_acc: 0.6667\n",
            "Epoch 567/1000\n",
            "79/79 [==============================] - 0s 991us/sample - loss: 0.5290 - acc: 0.8861 - val_loss: 0.6892 - val_acc: 0.6667\n",
            "Epoch 568/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.5188 - acc: 0.8987 - val_loss: 0.6840 - val_acc: 0.7222\n",
            "Epoch 569/1000\n",
            "79/79 [==============================] - 0s 1000us/sample - loss: 0.5088 - acc: 0.8987 - val_loss: 0.6831 - val_acc: 0.6667\n",
            "Epoch 570/1000\n",
            "79/79 [==============================] - 0s 998us/sample - loss: 0.5020 - acc: 0.9114 - val_loss: 0.6606 - val_acc: 0.7222\n",
            "Epoch 571/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.4902 - acc: 0.8987 - val_loss: 0.6679 - val_acc: 0.7222\n",
            "Epoch 572/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.4903 - acc: 0.8987 - val_loss: 0.6552 - val_acc: 0.6667\n",
            "Epoch 573/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.4818 - acc: 0.8987 - val_loss: 0.6542 - val_acc: 0.6667\n",
            "Epoch 574/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.4757 - acc: 0.9114 - val_loss: 0.7088 - val_acc: 0.6667\n",
            "Epoch 575/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.4618 - acc: 0.9114 - val_loss: 0.6582 - val_acc: 0.6667\n",
            "Epoch 576/1000\n",
            "79/79 [==============================] - 0s 945us/sample - loss: 0.4542 - acc: 0.8987 - val_loss: 0.6124 - val_acc: 0.7222\n",
            "Epoch 577/1000\n",
            "79/79 [==============================] - 0s 939us/sample - loss: 0.4447 - acc: 0.8987 - val_loss: 0.6125 - val_acc: 0.7778\n",
            "Epoch 578/1000\n",
            "79/79 [==============================] - 0s 945us/sample - loss: 0.4573 - acc: 0.8861 - val_loss: 0.6200 - val_acc: 0.7778\n",
            "Epoch 579/1000\n",
            "79/79 [==============================] - 0s 956us/sample - loss: 0.4309 - acc: 0.9241 - val_loss: 0.6508 - val_acc: 0.6667\n",
            "Epoch 580/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.4313 - acc: 0.9114 - val_loss: 0.6078 - val_acc: 0.8333\n",
            "Epoch 581/1000\n",
            "79/79 [==============================] - 0s 977us/sample - loss: 0.4232 - acc: 0.9367 - val_loss: 0.6167 - val_acc: 0.6667\n",
            "Epoch 582/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.4399 - acc: 0.8987 - val_loss: 0.5860 - val_acc: 0.7778\n",
            "Epoch 583/1000\n",
            "79/79 [==============================] - 0s 961us/sample - loss: 0.4087 - acc: 0.9367 - val_loss: 0.6215 - val_acc: 0.6667\n",
            "Epoch 584/1000\n",
            "79/79 [==============================] - 0s 935us/sample - loss: 0.4034 - acc: 0.9114 - val_loss: 0.6476 - val_acc: 0.6667\n",
            "Epoch 585/1000\n",
            "79/79 [==============================] - 0s 953us/sample - loss: 0.3927 - acc: 0.9114 - val_loss: 0.6503 - val_acc: 0.6667\n",
            "Epoch 586/1000\n",
            "79/79 [==============================] - 0s 951us/sample - loss: 0.3938 - acc: 0.9241 - val_loss: 0.6221 - val_acc: 0.6667\n",
            "Epoch 587/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.4012 - acc: 0.9367 - val_loss: 0.5936 - val_acc: 0.6667\n",
            "Epoch 588/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.3810 - acc: 0.9494 - val_loss: 0.5790 - val_acc: 0.7222\n",
            "Epoch 589/1000\n",
            "79/79 [==============================] - 0s 959us/sample - loss: 0.3740 - acc: 0.9367 - val_loss: 0.5731 - val_acc: 0.6667\n",
            "Epoch 590/1000\n",
            "79/79 [==============================] - 0s 977us/sample - loss: 0.3665 - acc: 0.9494 - val_loss: 0.5456 - val_acc: 0.7778\n",
            "Epoch 591/1000\n",
            "79/79 [==============================] - 0s 967us/sample - loss: 0.3787 - acc: 0.9367 - val_loss: 0.5502 - val_acc: 0.7222\n",
            "Epoch 592/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.3589 - acc: 0.9494 - val_loss: 0.5498 - val_acc: 0.7222\n",
            "Epoch 593/1000\n",
            "79/79 [==============================] - 0s 920us/sample - loss: 0.3497 - acc: 0.9241 - val_loss: 0.5729 - val_acc: 0.7222\n",
            "Epoch 594/1000\n",
            "79/79 [==============================] - 0s 911us/sample - loss: 0.3462 - acc: 0.9367 - val_loss: 0.5290 - val_acc: 0.7778\n",
            "Epoch 595/1000\n",
            "79/79 [==============================] - 0s 997us/sample - loss: 0.3480 - acc: 0.9620 - val_loss: 0.5154 - val_acc: 0.8333\n",
            "Epoch 596/1000\n",
            "79/79 [==============================] - 0s 905us/sample - loss: 0.3391 - acc: 0.9367 - val_loss: 0.4898 - val_acc: 0.8333\n",
            "Epoch 597/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.3403 - acc: 0.9367 - val_loss: 0.5019 - val_acc: 0.8889\n",
            "Epoch 598/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.3369 - acc: 0.9367 - val_loss: 0.5565 - val_acc: 0.7222\n",
            "Epoch 599/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.3293 - acc: 0.9367 - val_loss: 0.5448 - val_acc: 0.7778\n",
            "Epoch 600/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.3229 - acc: 0.9494 - val_loss: 0.4638 - val_acc: 0.8889\n",
            "Epoch 601/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.3295 - acc: 0.9620 - val_loss: 0.5186 - val_acc: 0.7222\n",
            "Epoch 602/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.3111 - acc: 0.9620 - val_loss: 0.4965 - val_acc: 0.7778\n",
            "Epoch 603/1000\n",
            "79/79 [==============================] - 0s 983us/sample - loss: 0.3071 - acc: 0.9620 - val_loss: 0.5261 - val_acc: 0.7222\n",
            "Epoch 604/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.3037 - acc: 0.9494 - val_loss: 0.4867 - val_acc: 0.8889\n",
            "Epoch 605/1000\n",
            "79/79 [==============================] - 0s 963us/sample - loss: 0.3029 - acc: 0.9620 - val_loss: 0.4669 - val_acc: 0.9444\n",
            "Epoch 606/1000\n",
            "79/79 [==============================] - 0s 998us/sample - loss: 0.2999 - acc: 0.9747 - val_loss: 0.5602 - val_acc: 0.6667\n",
            "Epoch 607/1000\n",
            "79/79 [==============================] - 0s 945us/sample - loss: 0.2882 - acc: 0.9620 - val_loss: 0.4391 - val_acc: 0.8889\n",
            "Epoch 608/1000\n",
            "79/79 [==============================] - 0s 961us/sample - loss: 0.2884 - acc: 0.9873 - val_loss: 0.5133 - val_acc: 0.7778\n",
            "Epoch 609/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.2816 - acc: 0.9747 - val_loss: 0.5096 - val_acc: 0.7778\n",
            "Epoch 610/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.2880 - acc: 0.9747 - val_loss: 0.4302 - val_acc: 0.8889\n",
            "Epoch 611/1000\n",
            "79/79 [==============================] - 0s 981us/sample - loss: 0.2773 - acc: 0.9620 - val_loss: 0.5518 - val_acc: 0.7222\n",
            "Epoch 612/1000\n",
            "79/79 [==============================] - 0s 995us/sample - loss: 0.2970 - acc: 0.9747 - val_loss: 0.4317 - val_acc: 0.8889\n",
            "Epoch 613/1000\n",
            "79/79 [==============================] - 0s 915us/sample - loss: 0.2633 - acc: 0.9873 - val_loss: 0.4538 - val_acc: 0.8889\n",
            "Epoch 614/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.2644 - acc: 0.9873 - val_loss: 0.4053 - val_acc: 0.8889\n",
            "Epoch 615/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.2847 - acc: 0.9747 - val_loss: 0.4414 - val_acc: 0.8889\n",
            "Epoch 616/1000\n",
            "79/79 [==============================] - 0s 932us/sample - loss: 0.2720 - acc: 0.9620 - val_loss: 0.6189 - val_acc: 0.6667\n",
            "Epoch 617/1000\n",
            "79/79 [==============================] - 0s 952us/sample - loss: 0.2512 - acc: 0.9873 - val_loss: 0.4776 - val_acc: 0.7222\n",
            "Epoch 618/1000\n",
            "79/79 [==============================] - 0s 903us/sample - loss: 0.2513 - acc: 0.9873 - val_loss: 0.5001 - val_acc: 0.7778\n",
            "Epoch 619/1000\n",
            "79/79 [==============================] - 0s 999us/sample - loss: 0.2528 - acc: 0.9620 - val_loss: 0.5040 - val_acc: 0.7778\n",
            "Epoch 620/1000\n",
            "79/79 [==============================] - 0s 970us/sample - loss: 0.2415 - acc: 1.0000 - val_loss: 0.4328 - val_acc: 0.8889\n",
            "Epoch 621/1000\n",
            "79/79 [==============================] - 0s 956us/sample - loss: 0.2409 - acc: 0.9873 - val_loss: 0.4429 - val_acc: 0.8889\n",
            "Epoch 622/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.2443 - acc: 0.9873 - val_loss: 0.4069 - val_acc: 0.8889\n",
            "Epoch 623/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.2300 - acc: 0.9873 - val_loss: 0.3936 - val_acc: 0.9444\n",
            "Epoch 624/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.2345 - acc: 1.0000 - val_loss: 0.3959 - val_acc: 0.8889\n",
            "Epoch 625/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.2233 - acc: 1.0000 - val_loss: 0.3879 - val_acc: 0.8889\n",
            "Epoch 626/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.2185 - acc: 1.0000 - val_loss: 0.3720 - val_acc: 0.8889\n",
            "Epoch 627/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.2198 - acc: 1.0000 - val_loss: 0.4323 - val_acc: 0.8333\n",
            "Epoch 628/1000\n",
            "79/79 [==============================] - 0s 904us/sample - loss: 0.2149 - acc: 1.0000 - val_loss: 0.4139 - val_acc: 0.8889\n",
            "Epoch 629/1000\n",
            "79/79 [==============================] - 0s 905us/sample - loss: 0.2148 - acc: 1.0000 - val_loss: 0.4554 - val_acc: 0.7778\n",
            "Epoch 630/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.2079 - acc: 1.0000 - val_loss: 0.4834 - val_acc: 0.7778\n",
            "Epoch 631/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.2078 - acc: 1.0000 - val_loss: 0.3852 - val_acc: 0.8889\n",
            "Epoch 632/1000\n",
            "79/79 [==============================] - 0s 962us/sample - loss: 0.2097 - acc: 1.0000 - val_loss: 0.3477 - val_acc: 0.8889\n",
            "Epoch 633/1000\n",
            "79/79 [==============================] - 0s 979us/sample - loss: 0.2479 - acc: 0.9494 - val_loss: 1.1621 - val_acc: 0.5000\n",
            "Epoch 634/1000\n",
            "79/79 [==============================] - 0s 954us/sample - loss: 0.4914 - acc: 0.8481 - val_loss: 0.3548 - val_acc: 0.8889\n",
            "Epoch 635/1000\n",
            "79/79 [==============================] - 0s 960us/sample - loss: 0.2031 - acc: 1.0000 - val_loss: 0.3492 - val_acc: 0.8889\n",
            "Epoch 636/1000\n",
            "79/79 [==============================] - 0s 972us/sample - loss: 0.2034 - acc: 0.9873 - val_loss: 0.3198 - val_acc: 0.8889\n",
            "Epoch 637/1000\n",
            "79/79 [==============================] - 0s 989us/sample - loss: 0.1923 - acc: 1.0000 - val_loss: 0.4218 - val_acc: 0.8333\n",
            "Epoch 638/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1926 - acc: 1.0000 - val_loss: 0.3436 - val_acc: 0.9444\n",
            "Epoch 639/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1900 - acc: 1.0000 - val_loss: 0.3494 - val_acc: 0.8889\n",
            "Epoch 640/1000\n",
            "79/79 [==============================] - 0s 955us/sample - loss: 0.1893 - acc: 1.0000 - val_loss: 0.3483 - val_acc: 0.8889\n",
            "Epoch 641/1000\n",
            "79/79 [==============================] - 0s 935us/sample - loss: 0.1968 - acc: 1.0000 - val_loss: 1.1408 - val_acc: 0.6111\n",
            "Epoch 642/1000\n",
            "79/79 [==============================] - 0s 912us/sample - loss: 0.2235 - acc: 0.9747 - val_loss: 0.3420 - val_acc: 0.9444\n",
            "Epoch 643/1000\n",
            "79/79 [==============================] - 0s 927us/sample - loss: 0.1858 - acc: 1.0000 - val_loss: 0.3568 - val_acc: 0.8333\n",
            "Epoch 644/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1775 - acc: 1.0000 - val_loss: 0.3394 - val_acc: 0.8889\n",
            "Epoch 645/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1751 - acc: 1.0000 - val_loss: 0.3147 - val_acc: 0.9444\n",
            "Epoch 646/1000\n",
            "79/79 [==============================] - 0s 991us/sample - loss: 0.1704 - acc: 1.0000 - val_loss: 0.3384 - val_acc: 0.8889\n",
            "Epoch 647/1000\n",
            "79/79 [==============================] - 0s 954us/sample - loss: 0.1710 - acc: 1.0000 - val_loss: 0.3132 - val_acc: 0.8889\n",
            "Epoch 648/1000\n",
            "79/79 [==============================] - 0s 949us/sample - loss: 0.1670 - acc: 1.0000 - val_loss: 0.2901 - val_acc: 0.9444\n",
            "Epoch 649/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1644 - acc: 1.0000 - val_loss: 0.2945 - val_acc: 0.9444\n",
            "Epoch 650/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1610 - acc: 1.0000 - val_loss: 0.2875 - val_acc: 0.9444\n",
            "Epoch 651/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1599 - acc: 1.0000 - val_loss: 0.3558 - val_acc: 0.8889\n",
            "Epoch 652/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1627 - acc: 1.0000 - val_loss: 0.3131 - val_acc: 0.8889\n",
            "Epoch 653/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1597 - acc: 1.0000 - val_loss: 0.2927 - val_acc: 0.9444\n",
            "Epoch 654/1000\n",
            "79/79 [==============================] - 0s 959us/sample - loss: 0.1559 - acc: 1.0000 - val_loss: 0.3001 - val_acc: 0.8889\n",
            "Epoch 655/1000\n",
            "79/79 [==============================] - 0s 946us/sample - loss: 0.1536 - acc: 1.0000 - val_loss: 0.2819 - val_acc: 0.9444\n",
            "Epoch 656/1000\n",
            "79/79 [==============================] - 0s 914us/sample - loss: 0.1529 - acc: 1.0000 - val_loss: 0.2786 - val_acc: 0.9444\n",
            "Epoch 657/1000\n",
            "79/79 [==============================] - 0s 933us/sample - loss: 0.1543 - acc: 1.0000 - val_loss: 0.2919 - val_acc: 0.8889\n",
            "Epoch 658/1000\n",
            "79/79 [==============================] - 0s 999us/sample - loss: 0.1485 - acc: 1.0000 - val_loss: 0.3053 - val_acc: 0.8889\n",
            "Epoch 659/1000\n",
            "79/79 [==============================] - 0s 978us/sample - loss: 0.1491 - acc: 1.0000 - val_loss: 0.2763 - val_acc: 0.9444\n",
            "Epoch 660/1000\n",
            "79/79 [==============================] - 0s 965us/sample - loss: 0.1451 - acc: 1.0000 - val_loss: 0.2774 - val_acc: 0.9444\n",
            "Epoch 661/1000\n",
            "79/79 [==============================] - 0s 942us/sample - loss: 0.1445 - acc: 1.0000 - val_loss: 0.3190 - val_acc: 0.8889\n",
            "Epoch 662/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1432 - acc: 1.0000 - val_loss: 0.3289 - val_acc: 0.8889\n",
            "Epoch 663/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1382 - acc: 1.0000 - val_loss: 0.3327 - val_acc: 0.8333\n",
            "Epoch 664/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1392 - acc: 1.0000 - val_loss: 0.2919 - val_acc: 0.8889\n",
            "Epoch 665/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1382 - acc: 1.0000 - val_loss: 0.2632 - val_acc: 0.9444\n",
            "Epoch 666/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1327 - acc: 1.0000 - val_loss: 0.3093 - val_acc: 0.8889\n",
            "Epoch 667/1000\n",
            "79/79 [==============================] - 0s 954us/sample - loss: 0.1345 - acc: 1.0000 - val_loss: 0.3279 - val_acc: 0.8333\n",
            "Epoch 668/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1306 - acc: 1.0000 - val_loss: 0.2616 - val_acc: 0.9444\n",
            "Epoch 669/1000\n",
            "79/79 [==============================] - 0s 936us/sample - loss: 0.1312 - acc: 1.0000 - val_loss: 0.2605 - val_acc: 0.9444\n",
            "Epoch 670/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1290 - acc: 1.0000 - val_loss: 0.2896 - val_acc: 0.9444\n",
            "Epoch 671/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1367 - acc: 1.0000 - val_loss: 0.2782 - val_acc: 0.9444\n",
            "Epoch 672/1000\n",
            "79/79 [==============================] - 0s 994us/sample - loss: 0.1265 - acc: 1.0000 - val_loss: 0.2384 - val_acc: 0.9444\n",
            "Epoch 673/1000\n",
            "79/79 [==============================] - 0s 999us/sample - loss: 0.1250 - acc: 1.0000 - val_loss: 0.2416 - val_acc: 0.9444\n",
            "Epoch 674/1000\n",
            "79/79 [==============================] - 0s 961us/sample - loss: 0.1252 - acc: 1.0000 - val_loss: 0.2702 - val_acc: 0.8889\n",
            "Epoch 675/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1227 - acc: 1.0000 - val_loss: 0.2559 - val_acc: 0.8889\n",
            "Epoch 676/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1194 - acc: 1.0000 - val_loss: 0.2409 - val_acc: 0.9444\n",
            "Epoch 677/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1182 - acc: 1.0000 - val_loss: 0.2927 - val_acc: 0.8889\n",
            "Epoch 678/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1188 - acc: 1.0000 - val_loss: 0.2581 - val_acc: 0.8889\n",
            "Epoch 679/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1162 - acc: 1.0000 - val_loss: 0.2403 - val_acc: 0.8889\n",
            "Epoch 680/1000\n",
            "79/79 [==============================] - 0s 967us/sample - loss: 0.1156 - acc: 1.0000 - val_loss: 0.2341 - val_acc: 0.8889\n",
            "Epoch 681/1000\n",
            "79/79 [==============================] - 0s 981us/sample - loss: 0.1124 - acc: 1.0000 - val_loss: 0.2646 - val_acc: 0.8889\n",
            "Epoch 682/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1140 - acc: 1.0000 - val_loss: 0.2619 - val_acc: 0.8889\n",
            "Epoch 683/1000\n",
            "79/79 [==============================] - 0s 966us/sample - loss: 0.1150 - acc: 1.0000 - val_loss: 0.2320 - val_acc: 0.8889\n",
            "Epoch 684/1000\n",
            "79/79 [==============================] - 0s 954us/sample - loss: 0.1113 - acc: 1.0000 - val_loss: 0.2572 - val_acc: 0.8889\n",
            "Epoch 685/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1089 - acc: 1.0000 - val_loss: 0.3327 - val_acc: 0.8889\n",
            "Epoch 686/1000\n",
            "79/79 [==============================] - 0s 946us/sample - loss: 0.1138 - acc: 1.0000 - val_loss: 0.2563 - val_acc: 0.8889\n",
            "Epoch 687/1000\n",
            "79/79 [==============================] - 0s 952us/sample - loss: 0.1072 - acc: 1.0000 - val_loss: 0.2761 - val_acc: 0.8889\n",
            "Epoch 688/1000\n",
            "79/79 [==============================] - 0s 987us/sample - loss: 0.1067 - acc: 1.0000 - val_loss: 0.2204 - val_acc: 0.9444\n",
            "Epoch 689/1000\n",
            "79/79 [==============================] - 0s 915us/sample - loss: 0.1074 - acc: 1.0000 - val_loss: 0.2398 - val_acc: 0.8889\n",
            "Epoch 690/1000\n",
            "79/79 [==============================] - 0s 986us/sample - loss: 0.1029 - acc: 1.0000 - val_loss: 0.2532 - val_acc: 0.9444\n",
            "Epoch 691/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1035 - acc: 1.0000 - val_loss: 0.2826 - val_acc: 0.8889\n",
            "Epoch 692/1000\n",
            "79/79 [==============================] - 0s 993us/sample - loss: 0.1022 - acc: 1.0000 - val_loss: 0.2323 - val_acc: 0.8889\n",
            "Epoch 693/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.1006 - acc: 1.0000 - val_loss: 0.2344 - val_acc: 0.8889\n",
            "Epoch 694/1000\n",
            "79/79 [==============================] - 0s 958us/sample - loss: 0.1041 - acc: 1.0000 - val_loss: 0.2415 - val_acc: 0.8889\n",
            "Epoch 695/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0989 - acc: 1.0000 - val_loss: 0.2550 - val_acc: 0.8889\n",
            "Epoch 696/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0983 - acc: 1.0000 - val_loss: 0.2558 - val_acc: 0.9444\n",
            "Epoch 697/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0979 - acc: 1.0000 - val_loss: 0.2397 - val_acc: 0.8889\n",
            "Epoch 698/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0966 - acc: 1.0000 - val_loss: 0.2406 - val_acc: 0.8889\n",
            "Epoch 699/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0976 - acc: 1.0000 - val_loss: 0.2946 - val_acc: 0.8889\n",
            "Epoch 700/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0957 - acc: 1.0000 - val_loss: 0.2460 - val_acc: 0.9444\n",
            "Epoch 701/1000\n",
            "79/79 [==============================] - 0s 958us/sample - loss: 0.0952 - acc: 1.0000 - val_loss: 0.2550 - val_acc: 0.9444\n",
            "Epoch 702/1000\n",
            "79/79 [==============================] - 0s 962us/sample - loss: 0.0941 - acc: 1.0000 - val_loss: 0.2496 - val_acc: 0.8889\n",
            "Epoch 703/1000\n",
            "79/79 [==============================] - 0s 963us/sample - loss: 0.0926 - acc: 1.0000 - val_loss: 0.2166 - val_acc: 0.8889\n",
            "Epoch 704/1000\n",
            "79/79 [==============================] - 0s 927us/sample - loss: 0.0914 - acc: 1.0000 - val_loss: 0.2405 - val_acc: 0.8889\n",
            "Epoch 705/1000\n",
            "79/79 [==============================] - 0s 992us/sample - loss: 0.0920 - acc: 1.0000 - val_loss: 0.2275 - val_acc: 0.8889\n",
            "Epoch 706/1000\n",
            "79/79 [==============================] - 0s 907us/sample - loss: 0.0901 - acc: 1.0000 - val_loss: 0.2365 - val_acc: 0.8889\n",
            "Epoch 707/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0895 - acc: 1.0000 - val_loss: 0.2237 - val_acc: 0.8889\n",
            "Epoch 708/1000\n",
            "79/79 [==============================] - 0s 943us/sample - loss: 0.0885 - acc: 1.0000 - val_loss: 0.2110 - val_acc: 0.8889\n",
            "Epoch 709/1000\n",
            "79/79 [==============================] - 0s 970us/sample - loss: 0.0878 - acc: 1.0000 - val_loss: 0.2543 - val_acc: 0.8889\n",
            "Epoch 710/1000\n",
            "79/79 [==============================] - 0s 975us/sample - loss: 0.0869 - acc: 1.0000 - val_loss: 0.2568 - val_acc: 0.9444\n",
            "Epoch 711/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0862 - acc: 1.0000 - val_loss: 0.2429 - val_acc: 0.9444\n",
            "Epoch 712/1000\n",
            "79/79 [==============================] - 0s 965us/sample - loss: 0.0855 - acc: 1.0000 - val_loss: 0.2238 - val_acc: 0.8889\n",
            "Epoch 713/1000\n",
            "79/79 [==============================] - 0s 983us/sample - loss: 0.0849 - acc: 1.0000 - val_loss: 0.2058 - val_acc: 0.8889\n",
            "Epoch 714/1000\n",
            "79/79 [==============================] - 0s 967us/sample - loss: 0.0843 - acc: 1.0000 - val_loss: 0.2244 - val_acc: 0.8889\n",
            "Epoch 715/1000\n",
            "79/79 [==============================] - 0s 956us/sample - loss: 0.0832 - acc: 1.0000 - val_loss: 0.2343 - val_acc: 0.8889\n",
            "Epoch 716/1000\n",
            "79/79 [==============================] - 0s 922us/sample - loss: 0.0829 - acc: 1.0000 - val_loss: 0.2274 - val_acc: 0.8889\n",
            "Epoch 717/1000\n",
            "79/79 [==============================] - 0s 934us/sample - loss: 0.0829 - acc: 1.0000 - val_loss: 0.2502 - val_acc: 0.8889\n",
            "Epoch 718/1000\n",
            "79/79 [==============================] - 0s 993us/sample - loss: 0.0819 - acc: 1.0000 - val_loss: 0.1984 - val_acc: 0.9444\n",
            "Epoch 719/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0816 - acc: 1.0000 - val_loss: 0.2507 - val_acc: 0.8889\n",
            "Epoch 720/1000\n",
            "79/79 [==============================] - 0s 984us/sample - loss: 0.0809 - acc: 1.0000 - val_loss: 0.2148 - val_acc: 0.8889\n",
            "Epoch 721/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0802 - acc: 1.0000 - val_loss: 0.2184 - val_acc: 0.8889\n",
            "Epoch 722/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0795 - acc: 1.0000 - val_loss: 0.1840 - val_acc: 0.9444\n",
            "Epoch 723/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0788 - acc: 1.0000 - val_loss: 0.2119 - val_acc: 0.8889\n",
            "Epoch 724/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0775 - acc: 1.0000 - val_loss: 0.2054 - val_acc: 0.8889\n",
            "Epoch 725/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0772 - acc: 1.0000 - val_loss: 0.2265 - val_acc: 0.8889\n",
            "Epoch 726/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0766 - acc: 1.0000 - val_loss: 0.2144 - val_acc: 0.8889\n",
            "Epoch 727/1000\n",
            "79/79 [==============================] - 0s 985us/sample - loss: 0.0760 - acc: 1.0000 - val_loss: 0.2239 - val_acc: 0.8889\n",
            "Epoch 728/1000\n",
            "79/79 [==============================] - 0s 940us/sample - loss: 0.0754 - acc: 1.0000 - val_loss: 0.2158 - val_acc: 0.8889\n",
            "Epoch 729/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0757 - acc: 1.0000 - val_loss: 0.2088 - val_acc: 0.8889\n",
            "Epoch 730/1000\n",
            "79/79 [==============================] - 0s 979us/sample - loss: 0.0748 - acc: 1.0000 - val_loss: 0.2165 - val_acc: 0.8889\n",
            "Epoch 731/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0740 - acc: 1.0000 - val_loss: 0.2147 - val_acc: 0.8889\n",
            "Epoch 732/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0734 - acc: 1.0000 - val_loss: 0.2381 - val_acc: 0.9444\n",
            "Epoch 733/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0732 - acc: 1.0000 - val_loss: 0.2302 - val_acc: 0.8889\n",
            "Epoch 734/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0732 - acc: 1.0000 - val_loss: 0.2113 - val_acc: 0.8889\n",
            "Epoch 735/1000\n",
            "79/79 [==============================] - 0s 964us/sample - loss: 0.0718 - acc: 1.0000 - val_loss: 0.2340 - val_acc: 0.8889\n",
            "Epoch 736/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0714 - acc: 1.0000 - val_loss: 0.2376 - val_acc: 0.8889\n",
            "Epoch 737/1000\n",
            "79/79 [==============================] - 0s 962us/sample - loss: 0.0711 - acc: 1.0000 - val_loss: 0.2186 - val_acc: 0.8889\n",
            "Epoch 738/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0703 - acc: 1.0000 - val_loss: 0.2272 - val_acc: 0.8889\n",
            "Epoch 739/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0703 - acc: 1.0000 - val_loss: 0.2108 - val_acc: 0.8889\n",
            "Epoch 740/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0693 - acc: 1.0000 - val_loss: 0.2224 - val_acc: 0.8889\n",
            "Epoch 741/1000\n",
            "79/79 [==============================] - 0s 963us/sample - loss: 0.0690 - acc: 1.0000 - val_loss: 0.2001 - val_acc: 0.8889\n",
            "Epoch 742/1000\n",
            "79/79 [==============================] - 0s 972us/sample - loss: 0.0688 - acc: 1.0000 - val_loss: 0.1974 - val_acc: 0.8889\n",
            "Epoch 743/1000\n",
            "79/79 [==============================] - 0s 979us/sample - loss: 0.0684 - acc: 1.0000 - val_loss: 0.2283 - val_acc: 0.8889\n",
            "Epoch 744/1000\n",
            "79/79 [==============================] - 0s 946us/sample - loss: 0.0674 - acc: 1.0000 - val_loss: 0.2218 - val_acc: 0.8889\n",
            "Epoch 745/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0674 - acc: 1.0000 - val_loss: 0.2106 - val_acc: 0.8889\n",
            "Epoch 746/1000\n",
            "79/79 [==============================] - 0s 912us/sample - loss: 0.0668 - acc: 1.0000 - val_loss: 0.2131 - val_acc: 0.8889\n",
            "Epoch 747/1000\n",
            "79/79 [==============================] - 0s 946us/sample - loss: 0.0664 - acc: 1.0000 - val_loss: 0.2150 - val_acc: 0.8889\n",
            "Epoch 748/1000\n",
            "79/79 [==============================] - 0s 923us/sample - loss: 0.0657 - acc: 1.0000 - val_loss: 0.2080 - val_acc: 0.8889\n",
            "Epoch 749/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0653 - acc: 1.0000 - val_loss: 0.2032 - val_acc: 0.8889\n",
            "Epoch 750/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0650 - acc: 1.0000 - val_loss: 0.2069 - val_acc: 0.8889\n",
            "Epoch 751/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0644 - acc: 1.0000 - val_loss: 0.2156 - val_acc: 0.8889\n",
            "Epoch 752/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0640 - acc: 1.0000 - val_loss: 0.1843 - val_acc: 0.8889\n",
            "Epoch 753/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0640 - acc: 1.0000 - val_loss: 0.1916 - val_acc: 0.8889\n",
            "Epoch 754/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0633 - acc: 1.0000 - val_loss: 0.2075 - val_acc: 0.8889\n",
            "Epoch 755/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0629 - acc: 1.0000 - val_loss: 0.1961 - val_acc: 0.8889\n",
            "Epoch 756/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0625 - acc: 1.0000 - val_loss: 0.2027 - val_acc: 0.8889\n",
            "Epoch 757/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0620 - acc: 1.0000 - val_loss: 0.1991 - val_acc: 0.8889\n",
            "Epoch 758/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0617 - acc: 1.0000 - val_loss: 0.2015 - val_acc: 0.8889\n",
            "Epoch 759/1000\n",
            "79/79 [==============================] - 0s 945us/sample - loss: 0.0612 - acc: 1.0000 - val_loss: 0.2279 - val_acc: 0.8889\n",
            "Epoch 760/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0609 - acc: 1.0000 - val_loss: 0.2053 - val_acc: 0.8889\n",
            "Epoch 761/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0606 - acc: 1.0000 - val_loss: 0.2103 - val_acc: 0.8889\n",
            "Epoch 762/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0602 - acc: 1.0000 - val_loss: 0.1939 - val_acc: 0.8889\n",
            "Epoch 763/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0601 - acc: 1.0000 - val_loss: 0.2113 - val_acc: 0.8889\n",
            "Epoch 764/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0595 - acc: 1.0000 - val_loss: 0.2165 - val_acc: 0.8889\n",
            "Epoch 765/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0592 - acc: 1.0000 - val_loss: 0.2011 - val_acc: 0.8889\n",
            "Epoch 766/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0588 - acc: 1.0000 - val_loss: 0.2028 - val_acc: 0.8889\n",
            "Epoch 767/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0584 - acc: 1.0000 - val_loss: 0.1992 - val_acc: 0.8889\n",
            "Epoch 768/1000\n",
            "79/79 [==============================] - 0s 989us/sample - loss: 0.0580 - acc: 1.0000 - val_loss: 0.1871 - val_acc: 0.8889\n",
            "Epoch 769/1000\n",
            "79/79 [==============================] - 0s 965us/sample - loss: 0.0579 - acc: 1.0000 - val_loss: 0.1940 - val_acc: 0.8889\n",
            "Epoch 770/1000\n",
            "79/79 [==============================] - 0s 998us/sample - loss: 0.0573 - acc: 1.0000 - val_loss: 0.2190 - val_acc: 0.8889\n",
            "Epoch 771/1000\n",
            "79/79 [==============================] - 0s 909us/sample - loss: 0.0572 - acc: 1.0000 - val_loss: 0.2090 - val_acc: 0.8889\n",
            "Epoch 772/1000\n",
            "79/79 [==============================] - 0s 955us/sample - loss: 0.0567 - acc: 1.0000 - val_loss: 0.2114 - val_acc: 0.8889\n",
            "Epoch 773/1000\n",
            "79/79 [==============================] - 0s 969us/sample - loss: 0.0565 - acc: 1.0000 - val_loss: 0.2027 - val_acc: 0.8889\n",
            "Epoch 774/1000\n",
            "79/79 [==============================] - 0s 974us/sample - loss: 0.0562 - acc: 1.0000 - val_loss: 0.2121 - val_acc: 0.8889\n",
            "Epoch 775/1000\n",
            "79/79 [==============================] - 0s 915us/sample - loss: 0.0558 - acc: 1.0000 - val_loss: 0.2040 - val_acc: 0.8889\n",
            "Epoch 776/1000\n",
            "79/79 [==============================] - 0s 924us/sample - loss: 0.0555 - acc: 1.0000 - val_loss: 0.2074 - val_acc: 0.8889\n",
            "Epoch 777/1000\n",
            "79/79 [==============================] - 0s 975us/sample - loss: 0.0550 - acc: 1.0000 - val_loss: 0.1860 - val_acc: 0.8889\n",
            "Epoch 778/1000\n",
            "79/79 [==============================] - 0s 945us/sample - loss: 0.0548 - acc: 1.0000 - val_loss: 0.2043 - val_acc: 0.8889\n",
            "Epoch 779/1000\n",
            "79/79 [==============================] - 0s 946us/sample - loss: 0.0545 - acc: 1.0000 - val_loss: 0.2118 - val_acc: 0.8889\n",
            "Epoch 780/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0542 - acc: 1.0000 - val_loss: 0.2068 - val_acc: 0.8889\n",
            "Epoch 781/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0539 - acc: 1.0000 - val_loss: 0.2132 - val_acc: 0.8889\n",
            "Epoch 782/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0536 - acc: 1.0000 - val_loss: 0.2036 - val_acc: 0.8889\n",
            "Epoch 783/1000\n",
            "79/79 [==============================] - 0s 914us/sample - loss: 0.0535 - acc: 1.0000 - val_loss: 0.2012 - val_acc: 0.8889\n",
            "Epoch 784/1000\n",
            "79/79 [==============================] - 0s 934us/sample - loss: 0.0532 - acc: 1.0000 - val_loss: 0.2020 - val_acc: 0.8889\n",
            "Epoch 785/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0529 - acc: 1.0000 - val_loss: 0.1952 - val_acc: 0.8889\n",
            "Epoch 786/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0524 - acc: 1.0000 - val_loss: 0.1963 - val_acc: 0.8889\n",
            "Epoch 787/1000\n",
            "79/79 [==============================] - 0s 994us/sample - loss: 0.0524 - acc: 1.0000 - val_loss: 0.2078 - val_acc: 0.8889\n",
            "Epoch 788/1000\n",
            "79/79 [==============================] - 0s 986us/sample - loss: 0.0519 - acc: 1.0000 - val_loss: 0.1956 - val_acc: 0.8889\n",
            "Epoch 789/1000\n",
            "79/79 [==============================] - 0s 953us/sample - loss: 0.0516 - acc: 1.0000 - val_loss: 0.1989 - val_acc: 0.8889\n",
            "Epoch 790/1000\n",
            "79/79 [==============================] - 0s 951us/sample - loss: 0.0513 - acc: 1.0000 - val_loss: 0.1985 - val_acc: 0.8889\n",
            "Epoch 791/1000\n",
            "79/79 [==============================] - 0s 996us/sample - loss: 0.0510 - acc: 1.0000 - val_loss: 0.2073 - val_acc: 0.8889\n",
            "Epoch 792/1000\n",
            "79/79 [==============================] - 0s 966us/sample - loss: 0.0508 - acc: 1.0000 - val_loss: 0.2005 - val_acc: 0.8889\n",
            "Epoch 793/1000\n",
            "79/79 [==============================] - 0s 958us/sample - loss: 0.0506 - acc: 1.0000 - val_loss: 0.1979 - val_acc: 0.8889\n",
            "Epoch 794/1000\n",
            "79/79 [==============================] - 0s 969us/sample - loss: 0.0503 - acc: 1.0000 - val_loss: 0.1962 - val_acc: 0.8889\n",
            "Epoch 795/1000\n",
            "79/79 [==============================] - 0s 956us/sample - loss: 0.0500 - acc: 1.0000 - val_loss: 0.2002 - val_acc: 0.8889\n",
            "Epoch 796/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0498 - acc: 1.0000 - val_loss: 0.1979 - val_acc: 0.8889\n",
            "Epoch 797/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0495 - acc: 1.0000 - val_loss: 0.2136 - val_acc: 0.8889\n",
            "Epoch 798/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0493 - acc: 1.0000 - val_loss: 0.1904 - val_acc: 0.8889\n",
            "Epoch 799/1000\n",
            "79/79 [==============================] - 0s 920us/sample - loss: 0.0490 - acc: 1.0000 - val_loss: 0.1980 - val_acc: 0.8889\n",
            "Epoch 800/1000\n",
            "79/79 [==============================] - 0s 940us/sample - loss: 0.0487 - acc: 1.0000 - val_loss: 0.1933 - val_acc: 0.8889\n",
            "Epoch 801/1000\n",
            "79/79 [==============================] - 0s 975us/sample - loss: 0.0485 - acc: 1.0000 - val_loss: 0.1907 - val_acc: 0.8889\n",
            "Epoch 802/1000\n",
            "79/79 [==============================] - 0s 954us/sample - loss: 0.0483 - acc: 1.0000 - val_loss: 0.1911 - val_acc: 0.8889\n",
            "Epoch 803/1000\n",
            "79/79 [==============================] - 0s 994us/sample - loss: 0.0481 - acc: 1.0000 - val_loss: 0.1927 - val_acc: 0.8889\n",
            "Epoch 804/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0479 - acc: 1.0000 - val_loss: 0.2013 - val_acc: 0.8889\n",
            "Epoch 805/1000\n",
            "79/79 [==============================] - 0s 953us/sample - loss: 0.0475 - acc: 1.0000 - val_loss: 0.1994 - val_acc: 0.8889\n",
            "Epoch 806/1000\n",
            "79/79 [==============================] - 0s 977us/sample - loss: 0.0473 - acc: 1.0000 - val_loss: 0.2053 - val_acc: 0.8889\n",
            "Epoch 807/1000\n",
            "79/79 [==============================] - 0s 942us/sample - loss: 0.0471 - acc: 1.0000 - val_loss: 0.2051 - val_acc: 0.8889\n",
            "Epoch 808/1000\n",
            "79/79 [==============================] - 0s 932us/sample - loss: 0.0468 - acc: 1.0000 - val_loss: 0.2066 - val_acc: 0.8889\n",
            "Epoch 809/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0467 - acc: 1.0000 - val_loss: 0.2013 - val_acc: 0.8889\n",
            "Epoch 810/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0464 - acc: 1.0000 - val_loss: 0.2118 - val_acc: 0.8889\n",
            "Epoch 811/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0462 - acc: 1.0000 - val_loss: 0.2172 - val_acc: 0.8889\n",
            "Epoch 812/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0460 - acc: 1.0000 - val_loss: 0.2096 - val_acc: 0.8889\n",
            "Epoch 813/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0457 - acc: 1.0000 - val_loss: 0.2071 - val_acc: 0.8889\n",
            "Epoch 814/1000\n",
            "79/79 [==============================] - 0s 994us/sample - loss: 0.0455 - acc: 1.0000 - val_loss: 0.2112 - val_acc: 0.8889\n",
            "Epoch 815/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0454 - acc: 1.0000 - val_loss: 0.2024 - val_acc: 0.8889\n",
            "Epoch 816/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0452 - acc: 1.0000 - val_loss: 0.2023 - val_acc: 0.8889\n",
            "Epoch 817/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0448 - acc: 1.0000 - val_loss: 0.2034 - val_acc: 0.8889\n",
            "Epoch 818/1000\n",
            "79/79 [==============================] - 0s 943us/sample - loss: 0.0446 - acc: 1.0000 - val_loss: 0.2093 - val_acc: 0.8889\n",
            "Epoch 819/1000\n",
            "79/79 [==============================] - 0s 978us/sample - loss: 0.0445 - acc: 1.0000 - val_loss: 0.1935 - val_acc: 0.8889\n",
            "Epoch 820/1000\n",
            "79/79 [==============================] - 0s 999us/sample - loss: 0.0442 - acc: 1.0000 - val_loss: 0.1932 - val_acc: 0.8889\n",
            "Epoch 821/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0440 - acc: 1.0000 - val_loss: 0.1838 - val_acc: 0.8889\n",
            "Epoch 822/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0439 - acc: 1.0000 - val_loss: 0.2022 - val_acc: 0.8889\n",
            "Epoch 823/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0436 - acc: 1.0000 - val_loss: 0.2034 - val_acc: 0.8889\n",
            "Epoch 824/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0434 - acc: 1.0000 - val_loss: 0.1982 - val_acc: 0.8889\n",
            "Epoch 825/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0432 - acc: 1.0000 - val_loss: 0.2009 - val_acc: 0.8889\n",
            "Epoch 826/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0430 - acc: 1.0000 - val_loss: 0.1886 - val_acc: 0.8889\n",
            "Epoch 827/1000\n",
            "79/79 [==============================] - 0s 950us/sample - loss: 0.0429 - acc: 1.0000 - val_loss: 0.1990 - val_acc: 0.8889\n",
            "Epoch 828/1000\n",
            "79/79 [==============================] - 0s 977us/sample - loss: 0.0426 - acc: 1.0000 - val_loss: 0.2055 - val_acc: 0.8889\n",
            "Epoch 829/1000\n",
            "79/79 [==============================] - 0s 940us/sample - loss: 0.0425 - acc: 1.0000 - val_loss: 0.2056 - val_acc: 0.8889\n",
            "Epoch 830/1000\n",
            "79/79 [==============================] - 0s 915us/sample - loss: 0.0423 - acc: 1.0000 - val_loss: 0.2088 - val_acc: 0.8889\n",
            "Epoch 831/1000\n",
            "79/79 [==============================] - 0s 992us/sample - loss: 0.0421 - acc: 1.0000 - val_loss: 0.1911 - val_acc: 0.8889\n",
            "Epoch 832/1000\n",
            "79/79 [==============================] - 0s 933us/sample - loss: 0.0419 - acc: 1.0000 - val_loss: 0.1912 - val_acc: 0.8889\n",
            "Epoch 833/1000\n",
            "79/79 [==============================] - 0s 993us/sample - loss: 0.0417 - acc: 1.0000 - val_loss: 0.2056 - val_acc: 0.8889\n",
            "Epoch 834/1000\n",
            "79/79 [==============================] - 0s 940us/sample - loss: 0.0415 - acc: 1.0000 - val_loss: 0.2073 - val_acc: 0.8889\n",
            "Epoch 835/1000\n",
            "79/79 [==============================] - 0s 952us/sample - loss: 0.0414 - acc: 1.0000 - val_loss: 0.2117 - val_acc: 0.8889\n",
            "Epoch 836/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0411 - acc: 1.0000 - val_loss: 0.2022 - val_acc: 0.8889\n",
            "Epoch 837/1000\n",
            "79/79 [==============================] - 0s 998us/sample - loss: 0.0410 - acc: 1.0000 - val_loss: 0.2027 - val_acc: 0.8889\n",
            "Epoch 838/1000\n",
            "79/79 [==============================] - 0s 968us/sample - loss: 0.0408 - acc: 1.0000 - val_loss: 0.2067 - val_acc: 0.8889\n",
            "Epoch 839/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0407 - acc: 1.0000 - val_loss: 0.1972 - val_acc: 0.8889\n",
            "Epoch 840/1000\n",
            "79/79 [==============================] - 0s 908us/sample - loss: 0.0404 - acc: 1.0000 - val_loss: 0.1956 - val_acc: 0.8889\n",
            "Epoch 841/1000\n",
            "79/79 [==============================] - 0s 986us/sample - loss: 0.0403 - acc: 1.0000 - val_loss: 0.1876 - val_acc: 0.8889\n",
            "Epoch 842/1000\n",
            "79/79 [==============================] - 0s 953us/sample - loss: 0.0401 - acc: 1.0000 - val_loss: 0.1964 - val_acc: 0.8889\n",
            "Epoch 843/1000\n",
            "79/79 [==============================] - 0s 953us/sample - loss: 0.0399 - acc: 1.0000 - val_loss: 0.2064 - val_acc: 0.8889\n",
            "Epoch 844/1000\n",
            "79/79 [==============================] - 0s 949us/sample - loss: 0.0398 - acc: 1.0000 - val_loss: 0.1883 - val_acc: 0.8889\n",
            "Epoch 845/1000\n",
            "79/79 [==============================] - 0s 990us/sample - loss: 0.0396 - acc: 1.0000 - val_loss: 0.2055 - val_acc: 0.8889\n",
            "Epoch 846/1000\n",
            "79/79 [==============================] - 0s 964us/sample - loss: 0.0395 - acc: 1.0000 - val_loss: 0.1999 - val_acc: 0.8889\n",
            "Epoch 847/1000\n",
            "79/79 [==============================] - 0s 933us/sample - loss: 0.0392 - acc: 1.0000 - val_loss: 0.2098 - val_acc: 0.8889\n",
            "Epoch 848/1000\n",
            "79/79 [==============================] - 0s 974us/sample - loss: 0.0391 - acc: 1.0000 - val_loss: 0.1968 - val_acc: 0.8889\n",
            "Epoch 849/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0389 - acc: 1.0000 - val_loss: 0.1947 - val_acc: 0.8889\n",
            "Epoch 850/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0388 - acc: 1.0000 - val_loss: 0.2064 - val_acc: 0.8889\n",
            "Epoch 851/1000\n",
            "79/79 [==============================] - 0s 937us/sample - loss: 0.0386 - acc: 1.0000 - val_loss: 0.2067 - val_acc: 0.8889\n",
            "Epoch 852/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0384 - acc: 1.0000 - val_loss: 0.2042 - val_acc: 0.8889\n",
            "Epoch 853/1000\n",
            "79/79 [==============================] - 0s 947us/sample - loss: 0.0383 - acc: 1.0000 - val_loss: 0.2036 - val_acc: 0.8889\n",
            "Epoch 854/1000\n",
            "79/79 [==============================] - 0s 961us/sample - loss: 0.0381 - acc: 1.0000 - val_loss: 0.2040 - val_acc: 0.8889\n",
            "Epoch 855/1000\n",
            "79/79 [==============================] - 0s 988us/sample - loss: 0.0380 - acc: 1.0000 - val_loss: 0.2031 - val_acc: 0.8889\n",
            "Epoch 856/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0378 - acc: 1.0000 - val_loss: 0.2076 - val_acc: 0.8889\n",
            "Epoch 857/1000\n",
            "79/79 [==============================] - 0s 964us/sample - loss: 0.0377 - acc: 1.0000 - val_loss: 0.2056 - val_acc: 0.8889\n",
            "Epoch 858/1000\n",
            "79/79 [==============================] - 0s 979us/sample - loss: 0.0375 - acc: 1.0000 - val_loss: 0.1983 - val_acc: 0.8889\n",
            "Epoch 859/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0374 - acc: 1.0000 - val_loss: 0.2005 - val_acc: 0.8889\n",
            "Epoch 860/1000\n",
            "79/79 [==============================] - 0s 927us/sample - loss: 0.0372 - acc: 1.0000 - val_loss: 0.2043 - val_acc: 0.8889\n",
            "Epoch 861/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0370 - acc: 1.0000 - val_loss: 0.1995 - val_acc: 0.8889\n",
            "Epoch 862/1000\n",
            "79/79 [==============================] - 0s 900us/sample - loss: 0.0369 - acc: 1.0000 - val_loss: 0.2046 - val_acc: 0.8889\n",
            "Epoch 863/1000\n",
            "79/79 [==============================] - 0s 926us/sample - loss: 0.0367 - acc: 1.0000 - val_loss: 0.1999 - val_acc: 0.8889\n",
            "Epoch 864/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0366 - acc: 1.0000 - val_loss: 0.1851 - val_acc: 0.8889\n",
            "Epoch 865/1000\n",
            "79/79 [==============================] - 0s 970us/sample - loss: 0.0365 - acc: 1.0000 - val_loss: 0.1942 - val_acc: 0.8889\n",
            "Epoch 866/1000\n",
            "79/79 [==============================] - 0s 930us/sample - loss: 0.0364 - acc: 1.0000 - val_loss: 0.1996 - val_acc: 0.8889\n",
            "Epoch 867/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0362 - acc: 1.0000 - val_loss: 0.1930 - val_acc: 0.8889\n",
            "Epoch 868/1000\n",
            "79/79 [==============================] - 0s 941us/sample - loss: 0.0361 - acc: 1.0000 - val_loss: 0.2028 - val_acc: 0.8889\n",
            "Epoch 869/1000\n",
            "79/79 [==============================] - 0s 969us/sample - loss: 0.0359 - acc: 1.0000 - val_loss: 0.2046 - val_acc: 0.8889\n",
            "Epoch 870/1000\n",
            "79/79 [==============================] - 0s 998us/sample - loss: 0.0358 - acc: 1.0000 - val_loss: 0.2035 - val_acc: 0.8889\n",
            "Epoch 871/1000\n",
            "79/79 [==============================] - 0s 949us/sample - loss: 0.0357 - acc: 1.0000 - val_loss: 0.1981 - val_acc: 0.8889\n",
            "Epoch 872/1000\n",
            "79/79 [==============================] - 0s 977us/sample - loss: 0.0355 - acc: 1.0000 - val_loss: 0.1951 - val_acc: 0.8889\n",
            "Epoch 873/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0354 - acc: 1.0000 - val_loss: 0.1933 - val_acc: 0.8889\n",
            "Epoch 874/1000\n",
            "79/79 [==============================] - 0s 999us/sample - loss: 0.0352 - acc: 1.0000 - val_loss: 0.1978 - val_acc: 0.8889\n",
            "Epoch 875/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0351 - acc: 1.0000 - val_loss: 0.2045 - val_acc: 0.8889\n",
            "Epoch 876/1000\n",
            "79/79 [==============================] - 0s 963us/sample - loss: 0.0349 - acc: 1.0000 - val_loss: 0.2040 - val_acc: 0.8889\n",
            "Epoch 877/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0348 - acc: 1.0000 - val_loss: 0.2069 - val_acc: 0.8889\n",
            "Epoch 878/1000\n",
            "79/79 [==============================] - 0s 973us/sample - loss: 0.0347 - acc: 1.0000 - val_loss: 0.2074 - val_acc: 0.8889\n",
            "Epoch 879/1000\n",
            "79/79 [==============================] - 0s 956us/sample - loss: 0.0346 - acc: 1.0000 - val_loss: 0.2105 - val_acc: 0.8889\n",
            "Epoch 880/1000\n",
            "79/79 [==============================] - 0s 954us/sample - loss: 0.0344 - acc: 1.0000 - val_loss: 0.2020 - val_acc: 0.8889\n",
            "Epoch 881/1000\n",
            "79/79 [==============================] - 0s 973us/sample - loss: 0.0343 - acc: 1.0000 - val_loss: 0.2085 - val_acc: 0.8889\n",
            "Epoch 882/1000\n",
            "79/79 [==============================] - 0s 933us/sample - loss: 0.0342 - acc: 1.0000 - val_loss: 0.2005 - val_acc: 0.8889\n",
            "Epoch 883/1000\n",
            "79/79 [==============================] - 0s 968us/sample - loss: 0.0340 - acc: 1.0000 - val_loss: 0.1992 - val_acc: 0.8889\n",
            "Epoch 884/1000\n",
            "79/79 [==============================] - 0s 929us/sample - loss: 0.0339 - acc: 1.0000 - val_loss: 0.1923 - val_acc: 0.8889\n",
            "Epoch 885/1000\n",
            "79/79 [==============================] - 0s 940us/sample - loss: 0.0338 - acc: 1.0000 - val_loss: 0.1934 - val_acc: 0.8889\n",
            "Epoch 886/1000\n",
            "79/79 [==============================] - 0s 961us/sample - loss: 0.0337 - acc: 1.0000 - val_loss: 0.1980 - val_acc: 0.8889\n",
            "Epoch 887/1000\n",
            "79/79 [==============================] - 0s 975us/sample - loss: 0.0336 - acc: 1.0000 - val_loss: 0.1965 - val_acc: 0.8889\n",
            "Epoch 888/1000\n",
            "79/79 [==============================] - 0s 941us/sample - loss: 0.0334 - acc: 1.0000 - val_loss: 0.2042 - val_acc: 0.8889\n",
            "Epoch 889/1000\n",
            "79/79 [==============================] - 0s 963us/sample - loss: 0.0333 - acc: 1.0000 - val_loss: 0.1993 - val_acc: 0.8889\n",
            "Epoch 890/1000\n",
            "79/79 [==============================] - 0s 996us/sample - loss: 0.0332 - acc: 1.0000 - val_loss: 0.1988 - val_acc: 0.8889\n",
            "Epoch 891/1000\n",
            "79/79 [==============================] - 0s 956us/sample - loss: 0.0331 - acc: 1.0000 - val_loss: 0.2046 - val_acc: 0.8889\n",
            "Epoch 892/1000\n",
            "79/79 [==============================] - 0s 930us/sample - loss: 0.0330 - acc: 1.0000 - val_loss: 0.2090 - val_acc: 0.8889\n",
            "Epoch 893/1000\n",
            "79/79 [==============================] - 0s 961us/sample - loss: 0.0328 - acc: 1.0000 - val_loss: 0.2101 - val_acc: 0.8889\n",
            "Epoch 894/1000\n",
            "79/79 [==============================] - 0s 934us/sample - loss: 0.0327 - acc: 1.0000 - val_loss: 0.2090 - val_acc: 0.8889\n",
            "Epoch 895/1000\n",
            "79/79 [==============================] - 0s 933us/sample - loss: 0.0326 - acc: 1.0000 - val_loss: 0.2058 - val_acc: 0.8889\n",
            "Epoch 896/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0325 - acc: 1.0000 - val_loss: 0.2123 - val_acc: 0.8889\n",
            "Epoch 897/1000\n",
            "79/79 [==============================] - 0s 954us/sample - loss: 0.0324 - acc: 1.0000 - val_loss: 0.2050 - val_acc: 0.8889\n",
            "Epoch 898/1000\n",
            "79/79 [==============================] - 0s 990us/sample - loss: 0.0322 - acc: 1.0000 - val_loss: 0.2018 - val_acc: 0.8889\n",
            "Epoch 899/1000\n",
            "79/79 [==============================] - 0s 948us/sample - loss: 0.0321 - acc: 1.0000 - val_loss: 0.2070 - val_acc: 0.8889\n",
            "Epoch 900/1000\n",
            "79/79 [==============================] - 0s 935us/sample - loss: 0.0320 - acc: 1.0000 - val_loss: 0.2086 - val_acc: 0.8889\n",
            "Epoch 901/1000\n",
            "79/79 [==============================] - 0s 941us/sample - loss: 0.0319 - acc: 1.0000 - val_loss: 0.2106 - val_acc: 0.8889\n",
            "Epoch 902/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0318 - acc: 1.0000 - val_loss: 0.2028 - val_acc: 0.8889\n",
            "Epoch 903/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0317 - acc: 1.0000 - val_loss: 0.2027 - val_acc: 0.8889\n",
            "Epoch 904/1000\n",
            "79/79 [==============================] - 0s 990us/sample - loss: 0.0316 - acc: 1.0000 - val_loss: 0.2026 - val_acc: 0.8889\n",
            "Epoch 905/1000\n",
            "79/79 [==============================] - 0s 990us/sample - loss: 0.0315 - acc: 1.0000 - val_loss: 0.2110 - val_acc: 0.8889\n",
            "Epoch 906/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0314 - acc: 1.0000 - val_loss: 0.2125 - val_acc: 0.8889\n",
            "Epoch 907/1000\n",
            "79/79 [==============================] - 0s 951us/sample - loss: 0.0313 - acc: 1.0000 - val_loss: 0.2041 - val_acc: 0.8889\n",
            "Epoch 908/1000\n",
            "79/79 [==============================] - 0s 976us/sample - loss: 0.0311 - acc: 1.0000 - val_loss: 0.2070 - val_acc: 0.8889\n",
            "Epoch 909/1000\n",
            "79/79 [==============================] - 0s 942us/sample - loss: 0.0310 - acc: 1.0000 - val_loss: 0.2109 - val_acc: 0.8889\n",
            "Epoch 910/1000\n",
            "79/79 [==============================] - 0s 930us/sample - loss: 0.0310 - acc: 1.0000 - val_loss: 0.2098 - val_acc: 0.8889\n",
            "Epoch 911/1000\n",
            "79/79 [==============================] - 0s 992us/sample - loss: 0.0308 - acc: 1.0000 - val_loss: 0.2061 - val_acc: 0.8889\n",
            "Epoch 912/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0307 - acc: 1.0000 - val_loss: 0.2116 - val_acc: 0.8889\n",
            "Epoch 913/1000\n",
            "79/79 [==============================] - 0s 967us/sample - loss: 0.0307 - acc: 1.0000 - val_loss: 0.2122 - val_acc: 0.8889\n",
            "Epoch 914/1000\n",
            "79/79 [==============================] - 0s 953us/sample - loss: 0.0306 - acc: 1.0000 - val_loss: 0.2168 - val_acc: 0.8889\n",
            "Epoch 915/1000\n",
            "79/79 [==============================] - 0s 951us/sample - loss: 0.0304 - acc: 1.0000 - val_loss: 0.2085 - val_acc: 0.8889\n",
            "Epoch 916/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0303 - acc: 1.0000 - val_loss: 0.2130 - val_acc: 0.8889\n",
            "Epoch 917/1000\n",
            "79/79 [==============================] - 0s 974us/sample - loss: 0.0302 - acc: 1.0000 - val_loss: 0.2086 - val_acc: 0.8889\n",
            "Epoch 918/1000\n",
            "79/79 [==============================] - 0s 995us/sample - loss: 0.0301 - acc: 1.0000 - val_loss: 0.2041 - val_acc: 0.8889\n",
            "Epoch 919/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0300 - acc: 1.0000 - val_loss: 0.2099 - val_acc: 0.8889\n",
            "Epoch 920/1000\n",
            "79/79 [==============================] - 0s 937us/sample - loss: 0.0299 - acc: 1.0000 - val_loss: 0.2100 - val_acc: 0.8889\n",
            "Epoch 921/1000\n",
            "79/79 [==============================] - 0s 926us/sample - loss: 0.0298 - acc: 1.0000 - val_loss: 0.2079 - val_acc: 0.8889\n",
            "Epoch 922/1000\n",
            "79/79 [==============================] - 0s 925us/sample - loss: 0.0297 - acc: 1.0000 - val_loss: 0.2063 - val_acc: 0.8889\n",
            "Epoch 923/1000\n",
            "79/79 [==============================] - 0s 997us/sample - loss: 0.0296 - acc: 1.0000 - val_loss: 0.2031 - val_acc: 0.8889\n",
            "Epoch 924/1000\n",
            "79/79 [==============================] - 0s 933us/sample - loss: 0.0295 - acc: 1.0000 - val_loss: 0.2071 - val_acc: 0.8889\n",
            "Epoch 925/1000\n",
            "79/79 [==============================] - 0s 986us/sample - loss: 0.0294 - acc: 1.0000 - val_loss: 0.2026 - val_acc: 0.8889\n",
            "Epoch 926/1000\n",
            "79/79 [==============================] - 0s 947us/sample - loss: 0.0293 - acc: 1.0000 - val_loss: 0.2066 - val_acc: 0.8889\n",
            "Epoch 927/1000\n",
            "79/79 [==============================] - 0s 968us/sample - loss: 0.0292 - acc: 1.0000 - val_loss: 0.2061 - val_acc: 0.8889\n",
            "Epoch 928/1000\n",
            "79/79 [==============================] - 0s 991us/sample - loss: 0.0292 - acc: 1.0000 - val_loss: 0.2106 - val_acc: 0.8889\n",
            "Epoch 929/1000\n",
            "79/79 [==============================] - 0s 939us/sample - loss: 0.0291 - acc: 1.0000 - val_loss: 0.2103 - val_acc: 0.8889\n",
            "Epoch 930/1000\n",
            "79/79 [==============================] - 0s 950us/sample - loss: 0.0289 - acc: 1.0000 - val_loss: 0.2106 - val_acc: 0.8889\n",
            "Epoch 931/1000\n",
            "79/79 [==============================] - 0s 966us/sample - loss: 0.0288 - acc: 1.0000 - val_loss: 0.2031 - val_acc: 0.8889\n",
            "Epoch 932/1000\n",
            "79/79 [==============================] - 0s 993us/sample - loss: 0.0288 - acc: 1.0000 - val_loss: 0.2079 - val_acc: 0.8889\n",
            "Epoch 933/1000\n",
            "79/79 [==============================] - 0s 933us/sample - loss: 0.0287 - acc: 1.0000 - val_loss: 0.2099 - val_acc: 0.8889\n",
            "Epoch 934/1000\n",
            "79/79 [==============================] - 0s 975us/sample - loss: 0.0286 - acc: 1.0000 - val_loss: 0.2030 - val_acc: 0.8889\n",
            "Epoch 935/1000\n",
            "79/79 [==============================] - 0s 945us/sample - loss: 0.0285 - acc: 1.0000 - val_loss: 0.2109 - val_acc: 0.8889\n",
            "Epoch 936/1000\n",
            "79/79 [==============================] - 0s 953us/sample - loss: 0.0284 - acc: 1.0000 - val_loss: 0.2064 - val_acc: 0.8889\n",
            "Epoch 937/1000\n",
            "79/79 [==============================] - 0s 961us/sample - loss: 0.0283 - acc: 1.0000 - val_loss: 0.2043 - val_acc: 0.8889\n",
            "Epoch 938/1000\n",
            "79/79 [==============================] - 0s 989us/sample - loss: 0.0282 - acc: 1.0000 - val_loss: 0.2166 - val_acc: 0.8889\n",
            "Epoch 939/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0282 - acc: 1.0000 - val_loss: 0.2092 - val_acc: 0.8889\n",
            "Epoch 940/1000\n",
            "79/79 [==============================] - 0s 993us/sample - loss: 0.0280 - acc: 1.0000 - val_loss: 0.2145 - val_acc: 0.8889\n",
            "Epoch 941/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0280 - acc: 1.0000 - val_loss: 0.2042 - val_acc: 0.8889\n",
            "Epoch 942/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0279 - acc: 1.0000 - val_loss: 0.2076 - val_acc: 0.8889\n",
            "Epoch 943/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0278 - acc: 1.0000 - val_loss: 0.2109 - val_acc: 0.8889\n",
            "Epoch 944/1000\n",
            "79/79 [==============================] - 0s 999us/sample - loss: 0.0277 - acc: 1.0000 - val_loss: 0.2072 - val_acc: 0.8889\n",
            "Epoch 945/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0276 - acc: 1.0000 - val_loss: 0.2108 - val_acc: 0.8889\n",
            "Epoch 946/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0275 - acc: 1.0000 - val_loss: 0.2064 - val_acc: 0.8889\n",
            "Epoch 947/1000\n",
            "79/79 [==============================] - 0s 925us/sample - loss: 0.0274 - acc: 1.0000 - val_loss: 0.2050 - val_acc: 0.8889\n",
            "Epoch 948/1000\n",
            "79/79 [==============================] - 0s 971us/sample - loss: 0.0274 - acc: 1.0000 - val_loss: 0.2103 - val_acc: 0.8889\n",
            "Epoch 949/1000\n",
            "79/79 [==============================] - 0s 951us/sample - loss: 0.0273 - acc: 1.0000 - val_loss: 0.2043 - val_acc: 0.8889\n",
            "Epoch 950/1000\n",
            "79/79 [==============================] - 0s 927us/sample - loss: 0.0272 - acc: 1.0000 - val_loss: 0.2087 - val_acc: 0.8889\n",
            "Epoch 951/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0271 - acc: 1.0000 - val_loss: 0.2055 - val_acc: 0.8889\n",
            "Epoch 952/1000\n",
            "79/79 [==============================] - 0s 975us/sample - loss: 0.0270 - acc: 1.0000 - val_loss: 0.2118 - val_acc: 0.8889\n",
            "Epoch 953/1000\n",
            "79/79 [==============================] - 0s 971us/sample - loss: 0.0270 - acc: 1.0000 - val_loss: 0.2076 - val_acc: 0.8889\n",
            "Epoch 954/1000\n",
            "79/79 [==============================] - 0s 990us/sample - loss: 0.0269 - acc: 1.0000 - val_loss: 0.2079 - val_acc: 0.8889\n",
            "Epoch 955/1000\n",
            "79/79 [==============================] - 0s 947us/sample - loss: 0.0268 - acc: 1.0000 - val_loss: 0.2065 - val_acc: 0.8889\n",
            "Epoch 956/1000\n",
            "79/79 [==============================] - 0s 911us/sample - loss: 0.0267 - acc: 1.0000 - val_loss: 0.2103 - val_acc: 0.8889\n",
            "Epoch 957/1000\n",
            "79/79 [==============================] - 0s 913us/sample - loss: 0.0266 - acc: 1.0000 - val_loss: 0.2054 - val_acc: 0.8889\n",
            "Epoch 958/1000\n",
            "79/79 [==============================] - 0s 945us/sample - loss: 0.0265 - acc: 1.0000 - val_loss: 0.2085 - val_acc: 0.8889\n",
            "Epoch 959/1000\n",
            "79/79 [==============================] - 0s 920us/sample - loss: 0.0265 - acc: 1.0000 - val_loss: 0.2107 - val_acc: 0.8889\n",
            "Epoch 960/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0264 - acc: 1.0000 - val_loss: 0.2105 - val_acc: 0.8889\n",
            "Epoch 961/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0263 - acc: 1.0000 - val_loss: 0.2173 - val_acc: 0.8889\n",
            "Epoch 962/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0262 - acc: 1.0000 - val_loss: 0.2088 - val_acc: 0.8889\n",
            "Epoch 963/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0262 - acc: 1.0000 - val_loss: 0.2084 - val_acc: 0.8889\n",
            "Epoch 964/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0261 - acc: 1.0000 - val_loss: 0.2069 - val_acc: 0.8889\n",
            "Epoch 965/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0260 - acc: 1.0000 - val_loss: 0.2091 - val_acc: 0.8889\n",
            "Epoch 966/1000\n",
            "79/79 [==============================] - 0s 926us/sample - loss: 0.0259 - acc: 1.0000 - val_loss: 0.2187 - val_acc: 0.8889\n",
            "Epoch 967/1000\n",
            "79/79 [==============================] - 0s 934us/sample - loss: 0.0259 - acc: 1.0000 - val_loss: 0.2118 - val_acc: 0.8889\n",
            "Epoch 968/1000\n",
            "79/79 [==============================] - 0s 958us/sample - loss: 0.0258 - acc: 1.0000 - val_loss: 0.2152 - val_acc: 0.8889\n",
            "Epoch 969/1000\n",
            "79/79 [==============================] - 0s 939us/sample - loss: 0.0257 - acc: 1.0000 - val_loss: 0.2128 - val_acc: 0.8889\n",
            "Epoch 970/1000\n",
            "79/79 [==============================] - 0s 917us/sample - loss: 0.0256 - acc: 1.0000 - val_loss: 0.2116 - val_acc: 0.8889\n",
            "Epoch 971/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0256 - acc: 1.0000 - val_loss: 0.2154 - val_acc: 0.8889\n",
            "Epoch 972/1000\n",
            "79/79 [==============================] - 0s 977us/sample - loss: 0.0255 - acc: 1.0000 - val_loss: 0.2097 - val_acc: 0.8889\n",
            "Epoch 973/1000\n",
            "79/79 [==============================] - 0s 988us/sample - loss: 0.0254 - acc: 1.0000 - val_loss: 0.2103 - val_acc: 0.8889\n",
            "Epoch 974/1000\n",
            "79/79 [==============================] - 0s 993us/sample - loss: 0.0253 - acc: 1.0000 - val_loss: 0.2160 - val_acc: 0.8889\n",
            "Epoch 975/1000\n",
            "79/79 [==============================] - 0s 931us/sample - loss: 0.0253 - acc: 1.0000 - val_loss: 0.2119 - val_acc: 0.8889\n",
            "Epoch 976/1000\n",
            "79/79 [==============================] - 0s 938us/sample - loss: 0.0252 - acc: 1.0000 - val_loss: 0.2127 - val_acc: 0.8889\n",
            "Epoch 977/1000\n",
            "79/79 [==============================] - 0s 939us/sample - loss: 0.0251 - acc: 1.0000 - val_loss: 0.2079 - val_acc: 0.8889\n",
            "Epoch 978/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0251 - acc: 1.0000 - val_loss: 0.2098 - val_acc: 0.8889\n",
            "Epoch 979/1000\n",
            "79/79 [==============================] - 0s 971us/sample - loss: 0.0250 - acc: 1.0000 - val_loss: 0.2146 - val_acc: 0.8889\n",
            "Epoch 980/1000\n",
            "79/79 [==============================] - 0s 925us/sample - loss: 0.0249 - acc: 1.0000 - val_loss: 0.2184 - val_acc: 0.8889\n",
            "Epoch 981/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0249 - acc: 1.0000 - val_loss: 0.2139 - val_acc: 0.8889\n",
            "Epoch 982/1000\n",
            "79/79 [==============================] - 0s 956us/sample - loss: 0.0248 - acc: 1.0000 - val_loss: 0.2101 - val_acc: 0.8889\n",
            "Epoch 983/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0247 - acc: 1.0000 - val_loss: 0.2062 - val_acc: 0.8889\n",
            "Epoch 984/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0246 - acc: 1.0000 - val_loss: 0.2146 - val_acc: 0.8889\n",
            "Epoch 985/1000\n",
            "79/79 [==============================] - 0s 991us/sample - loss: 0.0246 - acc: 1.0000 - val_loss: 0.2167 - val_acc: 0.8889\n",
            "Epoch 986/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0245 - acc: 1.0000 - val_loss: 0.2129 - val_acc: 0.8889\n",
            "Epoch 987/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0244 - acc: 1.0000 - val_loss: 0.2099 - val_acc: 0.8889\n",
            "Epoch 988/1000\n",
            "79/79 [==============================] - 0s 953us/sample - loss: 0.0244 - acc: 1.0000 - val_loss: 0.2080 - val_acc: 0.8889\n",
            "Epoch 989/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0243 - acc: 1.0000 - val_loss: 0.2162 - val_acc: 0.8889\n",
            "Epoch 990/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0242 - acc: 1.0000 - val_loss: 0.2159 - val_acc: 0.8889\n",
            "Epoch 991/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0242 - acc: 1.0000 - val_loss: 0.2130 - val_acc: 0.8889\n",
            "Epoch 992/1000\n",
            "79/79 [==============================] - 0s 926us/sample - loss: 0.0241 - acc: 1.0000 - val_loss: 0.2174 - val_acc: 0.8889\n",
            "Epoch 993/1000\n",
            "79/79 [==============================] - 0s 968us/sample - loss: 0.0240 - acc: 1.0000 - val_loss: 0.2121 - val_acc: 0.8889\n",
            "Epoch 994/1000\n",
            "79/79 [==============================] - 0s 1ms/sample - loss: 0.0240 - acc: 1.0000 - val_loss: 0.2149 - val_acc: 0.8889\n",
            "Epoch 995/1000\n",
            "79/79 [==============================] - 0s 947us/sample - loss: 0.0239 - acc: 1.0000 - val_loss: 0.2145 - val_acc: 0.8889\n",
            "Epoch 996/1000\n",
            "79/79 [==============================] - 0s 994us/sample - loss: 0.0238 - acc: 1.0000 - val_loss: 0.2173 - val_acc: 0.8889\n",
            "Epoch 997/1000\n",
            "79/79 [==============================] - 0s 994us/sample - loss: 0.0238 - acc: 1.0000 - val_loss: 0.2140 - val_acc: 0.8889\n",
            "Epoch 998/1000\n",
            "79/79 [==============================] - 0s 963us/sample - loss: 0.0237 - acc: 1.0000 - val_loss: 0.2124 - val_acc: 0.8889\n",
            "Epoch 999/1000\n",
            "79/79 [==============================] - 0s 918us/sample - loss: 0.0236 - acc: 1.0000 - val_loss: 0.2101 - val_acc: 0.8889\n",
            "Epoch 1000/1000\n",
            "79/79 [==============================] - 0s 975us/sample - loss: 0.0236 - acc: 1.0000 - val_loss: 0.2041 - val_acc: 0.8889\n",
            "X_test shape: (18, 6, 216)\n",
            "y_test shape: (18, 7)\n",
            "18/18 [==============================] - 0s 592us/sample - loss: 0.2041 - acc: 0.8889\n",
            "evaluation metrics\n",
            " [0.20414086017343733, 0.8888889]\n",
            "18/18 [==============================] - 0s 656us/sample - loss: 0.2041 - acc: 0.8889\n",
            "Test score: 0.20414086017343733\n",
            "Test accuracy: 0.8888889\n",
            "[[5.97961247e-04 1.03455292e-04 9.80237067e-01 1.10236453e-02\n",
            "  6.93917833e-03 1.09763863e-03 1.09870041e-06]\n",
            " [1.55889211e-04 8.03237890e-06 2.19012232e-04 5.67797724e-06\n",
            "  1.08818775e-02 9.84142601e-01 4.58693877e-03]\n",
            " [8.48913938e-03 9.75365639e-01 4.85687669e-05 6.06817054e-03\n",
            "  5.63010617e-05 6.67290951e-05 9.90535971e-03]\n",
            " [1.68782324e-02 7.81969065e-05 9.03870072e-03 2.60443285e-05\n",
            "  9.67439175e-01 6.41747750e-03 1.22133279e-04]\n",
            " [1.22453785e-02 5.79671796e-05 7.89519120e-03 1.93432370e-05\n",
            "  9.73335564e-01 6.32192008e-03 1.24584854e-04]\n",
            " [9.72286612e-03 4.63662902e-03 8.24719667e-03 9.77316916e-01\n",
            "  6.36040786e-05 6.15419458e-06 6.60005708e-06]\n",
            " [1.92227145e-03 7.53591489e-03 5.35706567e-06 2.34746622e-04\n",
            "  1.15145653e-04 7.61731062e-03 9.82569218e-01]\n",
            " [8.67909007e-03 4.30482952e-03 7.76938628e-03 9.79182541e-01\n",
            "  5.24948482e-05 5.57209614e-06 5.95360143e-06]\n",
            " [1.05329473e-02 3.51645099e-03 2.35780831e-02 9.62170660e-01\n",
            "  1.79431532e-04 1.51796194e-05 7.32933859e-06]\n",
            " [1.81952142e-04 1.36906647e-05 1.98096110e-04 7.30806914e-06\n",
            "  8.28562677e-03 9.84092712e-01 7.22062076e-03]\n",
            " [4.60357219e-01 1.51858166e-01 2.77954910e-04 8.80422629e-03\n",
            "  9.55087878e-03 6.41184486e-03 3.62739772e-01]]\n",
            "[2, 5, 1, 4, 4, 3, 6, 3, 3, 5, 0]\n",
            "[2. 5. 1. 4. 4. 3. 6. 3. 2. 5. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZS1eYBipP7c",
        "colab_type": "code",
        "outputId": "5d24e39c-d178-40f9-d086-a96562106f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "rnn_predictions = [list(np.where(predictions == max(predictions))[0])[0] for predictions in test_fruit_predictions]\n",
        "rnn_predictions = np.array(rnn_predictions)\n",
        "print(rnn_predictions)\n",
        "print(y_test_fruit)\n",
        "missed = (rnn_predictions != y_test_fruit)\n",
        "print(np.mean(np.array(missed)))\n",
        "print('Test accuracy: %.2f percent' % (100 * (1 - np.mean(missed))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 5 1 4 4 3 6 3 3 5 0]\n",
            "[2. 5. 1. 4. 4. 3. 6. 3. 2. 5. 0.]\n",
            "0.09090909090909091\n",
            "Test accuracy: 90.91 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlItj9RkjeDi",
        "colab_type": "code",
        "outputId": "0f7839b9-47c8-42c1-9467-66ecd43776c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!pip install hmmlearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hmmlearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/c5/91b43156b193d180ed94069269bcf88d3c7c6e54514a8482050fa9995e10/hmmlearn-0.2.2.tar.gz (146kB)\n",
            "\r\u001b[K     |██▎                             | 10kB 21.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 61kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 112kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 122kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 133kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 143kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from hmmlearn) (1.17.4)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.6/dist-packages (from hmmlearn) (0.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.16->hmmlearn) (0.14.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.16->hmmlearn) (1.3.2)\n",
            "Building wheels for collected packages: hmmlearn\n",
            "  Building wheel for hmmlearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hmmlearn: filename=hmmlearn-0.2.2-cp36-cp36m-linux_x86_64.whl size=325111 sha256=6ce2c643ef906f5ff38d1bdb0d0c46b7cf68d5e3ec22654dbb993b20df4fdd3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/b6/0e/63a865a30e21e01d04f417d8995fbfb793d6bd464707efc546\n",
            "Successfully built hmmlearn\n",
            "Installing collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbqWPHTmj0z6",
        "colab_type": "code",
        "outputId": "5490c4c4-c7c3-4715-feed-97ffb732f438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# print(set(y_train))\n",
        "from hmmlearn import hmm\n",
        "models = [hmm.GaussianHMM(n_components=6) for y in set(y_train)]\n",
        "for current_fruit in set(y_train):\n",
        "  for i in range(len(y_train)):\n",
        "    if y_train[i] == current_fruit:\n",
        "      models[int(current_fruit)].fit(X_train[i, :, :])\n",
        "\n",
        "# logprob = np.array([[models.score(i) for i in X_test.mean(axis=2)] for m in models])\n",
        "logprob = []\n",
        "for m in models:\n",
        "  new_scores = []\n",
        "  for i in range(len(y_test)):\n",
        "    new_scores.append(m.score(X_test[i, :, :]))\n",
        "  logprob.append(new_scores)\n",
        "\n",
        "logprob = np.array(logprob)\n",
        "\n",
        "y_predicted = np.argmax(logprob, axis=0)\n",
        "print(y_predicted)\n",
        "missed = (y_predicted != y_test)\n",
        "print(y_test)\n",
        "print(missed)\n",
        "print(np.mean(missed))\n",
        "print('Test accuracy: %.2f percent' % (100 * (1 - np.mean(missed))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 0 1 0 0 3 6 3 2 0 0]\n",
            "[2. 5. 1. 4. 4. 3. 6. 3. 2. 5. 0.]\n",
            "[False  True False  True  True False False False False  True False]\n",
            "0.36363636363636365\n",
            "Test accuracy: 63.64 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZcnZZrokLgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for m in models:\n",
        "  print(m.decode)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}